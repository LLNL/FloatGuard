diff --git a/CUDA/BT/Makefile b/CUDA/BT/Makefile
index 825e4a0..eef07e8 100644
--- a/CUDA/BT/Makefile
+++ b/CUDA/BT/Makefile
@@ -13,8 +13,8 @@ ${PROGRAM}: config ${OBJS}
 	${CLINK} ${CLINKFLAGS} -o ${PROGRAM} ${OBJS} ${C_LIB}
 
 bt.o:             bt.cu npbparams.hpp
-	${CCOMPILE} -maxrregcount 50 bt.cu
+	${CCOMPILE} bt.cu
 
 clean:
-	- rm -f *.o *~ mputil*
+	- rm -f *.o *.bc seq.txt *~ mputil*
 	- rm -f  npbparams.hpp core
diff --git a/CUDA/BT/Makefile.all b/CUDA/BT/Makefile.all
new file mode 100644
index 0000000..1fb284c
--- /dev/null
+++ b/CUDA/BT/Makefile.all
@@ -0,0 +1,15 @@
+SHELL=/bin/sh
+BENCHMARK=bt
+BENCHMARKU=BT
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+clang:
\ No newline at end of file
diff --git a/CUDA/BT/bt.cu b/CUDA/BT/bt.cu
index 3bec6d9..779047e 100644
--- a/CUDA/BT/bt.cu
+++ b/CUDA/BT/bt.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -59,7 +60,7 @@
  * ------------------------------------------------------------------------------
  */
 
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -103,6 +104,36 @@
 #define PROFILING_INITIALIZE (26)
 #define PROFILING_RHS_NORM_1 (27)
 #define PROFILING_RHS_NORM_2 (28)
+/* new */
+#define THREADS_PER_BLOCK_ON_ADD (32)
+#define THREADS_PER_BLOCK_ON_RHS_1 (32)
+#define THREADS_PER_BLOCK_ON_RHS_2 (32)
+#define THREADS_PER_BLOCK_ON_RHS_3 (32)
+#define THREADS_PER_BLOCK_ON_RHS_4 (32)
+#define THREADS_PER_BLOCK_ON_RHS_5 (32)
+#define THREADS_PER_BLOCK_ON_RHS_6 (32)
+#define THREADS_PER_BLOCK_ON_RHS_7 (32)
+#define THREADS_PER_BLOCK_ON_RHS_8 (32)
+#define THREADS_PER_BLOCK_ON_RHS_9 (32)
+#define THREADS_PER_BLOCK_ON_X_SOLVE_1 (32)
+#define THREADS_PER_BLOCK_ON_X_SOLVE_2 (32)
+#define THREADS_PER_BLOCK_ON_X_SOLVE_3 (32)
+#define THREADS_PER_BLOCK_ON_Y_SOLVE_1 (32)
+#define THREADS_PER_BLOCK_ON_Y_SOLVE_2 (32)
+#define THREADS_PER_BLOCK_ON_Y_SOLVE_3 (32)
+#define THREADS_PER_BLOCK_ON_Z_SOLVE_1 (32)
+#define THREADS_PER_BLOCK_ON_Z_SOLVE_2 (32)
+#define THREADS_PER_BLOCK_ON_Z_SOLVE_3 (32)
+/* old */
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_1 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_2 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_3 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_4 (32)
+#define THREADS_PER_BLOCK_ON_ERROR_NORM_1 (32)
+#define THREADS_PER_BLOCK_ON_ERROR_NORM_2 (32)
+#define THREADS_PER_BLOCK_ON_INITIALIZE (32)
+#define THREADS_PER_BLOCK_ON_RHS_NORM_1 (32)
+#define THREADS_PER_BLOCK_ON_RHS_NORM_2 (32)
 
 /* global variables */
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
@@ -162,39 +193,9 @@ static size_t size_rhs;
 static size_t size_qs;
 static size_t size_square;
 static size_t size_rho_i;
-/* new */
-static int THREADS_PER_BLOCK_ON_ADD;
-static int THREADS_PER_BLOCK_ON_RHS_1;
-static int THREADS_PER_BLOCK_ON_RHS_2;
-static int THREADS_PER_BLOCK_ON_RHS_3;
-static int THREADS_PER_BLOCK_ON_RHS_4;
-static int THREADS_PER_BLOCK_ON_RHS_5;
-static int THREADS_PER_BLOCK_ON_RHS_6;
-static int THREADS_PER_BLOCK_ON_RHS_7;
-static int THREADS_PER_BLOCK_ON_RHS_8;
-static int THREADS_PER_BLOCK_ON_RHS_9;
-static int THREADS_PER_BLOCK_ON_X_SOLVE_1;
-static int THREADS_PER_BLOCK_ON_X_SOLVE_2;
-static int THREADS_PER_BLOCK_ON_X_SOLVE_3;
-static int THREADS_PER_BLOCK_ON_Y_SOLVE_1;
-static int THREADS_PER_BLOCK_ON_Y_SOLVE_2;
-static int THREADS_PER_BLOCK_ON_Y_SOLVE_3;
-static int THREADS_PER_BLOCK_ON_Z_SOLVE_1;
-static int THREADS_PER_BLOCK_ON_Z_SOLVE_2;
-static int THREADS_PER_BLOCK_ON_Z_SOLVE_3;
-/* old */
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_1;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_2;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_3;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_4;
-static int THREADS_PER_BLOCK_ON_ERROR_NORM_1;
-static int THREADS_PER_BLOCK_ON_ERROR_NORM_2;
-static int THREADS_PER_BLOCK_ON_INITIALIZE;
-static int THREADS_PER_BLOCK_ON_RHS_NORM_1;
-static int THREADS_PER_BLOCK_ON_RHS_NORM_2;
 int gpu_device_id;
 int total_devices;
-cudaDeviceProp gpu_device_properties;
+hipDeviceProp_t gpu_device_properties;
 extern __shared__ double extern_share_data[];
 
 /* constants */
@@ -449,36 +450,36 @@ int main(int argc, char* argv[]){
 	 * do one time step to touch all code, and reinitialize_gpu
 	 * ---------------------------------------------------------------------
 	 */
-	cudaMemcpy(u_device, u, size_u, cudaMemcpyHostToDevice);
-	cudaMemcpy(forcing_device, forcing, size_forcing, cudaMemcpyHostToDevice);
+	hipMemcpy(u_device, u, size_u, hipMemcpyHostToDevice);
+	hipMemcpy(forcing_device, forcing, size_forcing, hipMemcpyHostToDevice);
 
 	adi_gpu();
 
-	cudaMemcpy(qs, qs_device, size_qs, cudaMemcpyDeviceToHost);
-	cudaMemcpy(square, square_device, size_square, cudaMemcpyDeviceToHost);
-	cudaMemcpy(rho_i, rho_i_device, size_rho_i, cudaMemcpyDeviceToHost);
-	cudaMemcpy(rhs, rhs_device, size_rhs, cudaMemcpyDeviceToHost);
-	cudaMemcpy(u, u_device, size_u, cudaMemcpyDeviceToHost);
+	hipMemcpy(qs, qs_device, size_qs, hipMemcpyDeviceToHost);
+	hipMemcpy(square, square_device, size_square, hipMemcpyDeviceToHost);
+	hipMemcpy(rho_i, rho_i_device, size_rho_i, hipMemcpyDeviceToHost);
+	hipMemcpy(rhs, rhs_device, size_rhs, hipMemcpyDeviceToHost);
+	hipMemcpy(u, u_device, size_u, hipMemcpyDeviceToHost);
 
 	initialize();
 
-	cudaMemcpy(u_device, u, size_u, cudaMemcpyHostToDevice);
-	cudaMemcpy(forcing_device, forcing, size_forcing, cudaMemcpyHostToDevice);
+	hipMemcpy(u_device, u, size_u, hipMemcpyHostToDevice);
+	hipMemcpy(forcing_device, forcing, size_forcing, hipMemcpyHostToDevice);
 
-	timer_clear();
+	timer_clear(0);
 
-	timer_start();
+	timer_start(0);
 
 	for(step=1; step<=niter; step++){
 		if((step%20)==0||step==1){printf(" Time step %4d\n",step);}
 		adi_gpu();
 	}
 
-	timer_stop();
-	tmax = timer_read();
+	timer_stop(0);
+	tmax = timer_read(0);
 
-	cudaMemcpy(rhs, rhs_device, size_rhs, cudaMemcpyDeviceToHost);
-	cudaMemcpy(u, u_device, size_u, cudaMemcpyDeviceToHost);
+	hipMemcpy(rhs, rhs_device, size_rhs, hipMemcpyDeviceToHost);
+	hipMemcpy(u, u_device, size_u, hipMemcpyDeviceToHost);
 
 	verify(niter, &class_npb, &verified);
 
@@ -625,11 +626,6 @@ int main(int argc, char* argv[]){
 			verified,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
@@ -2059,18 +2055,18 @@ void initialize(){
 }
 
 static void release_gpu(){
-	cudaFree(forcing_device);
-	cudaFree(u_device);
-	cudaFree(rhs_device);
-	cudaFree(us_device);
-	cudaFree(vs_device);
-	cudaFree(ws_device);
-	cudaFree(qs_device);
-	cudaFree(rho_i_device);
-	cudaFree(square_device);
-	cudaFree(lhsA_device);
-	cudaFree(lhsB_device);
-	cudaFree(lhsC_device);
+	hipFree(forcing_device);
+	hipFree(u_device);
+	hipFree(rhs_device);
+	hipFree(us_device);
+	hipFree(vs_device);
+	hipFree(ws_device);
+	hipFree(qs_device);
+	hipFree(rho_i_device);
+	hipFree(square_device);
+	hipFree(lhsA_device);
+	hipFree(lhsB_device);
+	hipFree(lhsC_device);
 }
 
 void rhs_norm(double rms[5]){
@@ -2292,106 +2288,106 @@ void set_constants(){
 	zzcon4 = c3c4tz3*con16*tz3;
 	zzcon5 = c3c4tz3*c1c5*tz3;
 
-	cudaMemcpyToSymbol(constants_device::ce, &ce, 13*5*sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dt, &dt, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1, &c1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2, &c2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3, &c3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c4, &c4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c5, &c5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dIMAXm1, &dIMAXm1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dJMAXm1, &dJMAXm1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dKMAXm1, &dKMAXm1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1c2, &c1c2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1c5, &c1c5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4, &c3c4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1345, &c1345, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::coKMAX1, &coKMAX1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx1, &tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx2, &tx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx3, &tx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty1, &ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty2, &ty2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty3, &ty3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz1, &tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz2, &tz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz3, &tz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx1, &dx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx2, &dx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx3, &dx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx4, &dx4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx5, &dx5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy1, &dy1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy2, &dy2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy3, &dy3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy4, &dy4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy5, &dy5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz1, &dz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz2, &dz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz3, &dz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz4, &dz4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz5, &dz5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dxmax, &dxmax, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dymax, &dymax, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dzmax, &dzmax, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dssp, &dssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c4dssp, &c4dssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c5dssp, &c5dssp, sizeof(double));	
-	cudaMemcpyToSymbol(constants_device::dttx1, &dttx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dttx2, &dttx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dtty1, &dtty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dtty2, &dtty2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dttz1, &dttz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dttz2, &dttz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2dttx1, &c2dttx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2dtty1, &c2dtty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2dttz1, &c2dttz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dtdssp, &dtdssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz1, &comz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz4, &comz4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz5, &comz5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz6, &comz6, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4tx3, &c3c4tx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4ty3, &c3c4ty3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4tz3, &c3c4tz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx1tx1, &dx1tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx2tx1, &dx2tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx3tx1, &dx3tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx4tx1, &dx4tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx5tx1, &dx5tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy1ty1, &dy1ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy2ty1, &dy2ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy3ty1, &dy3ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy4ty1, &dy4ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy5ty1, &dy5ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz1tz1, &dz1tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz2tz1, &dz2tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz3tz1, &dz3tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz4tz1, &dz4tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz5tz1, &dz5tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2iv, &c2iv, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::con43, &con43, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::con16, &con16, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon1, &xxcon1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon2, &xxcon2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon3, &xxcon3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon4, &xxcon4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon5, &xxcon5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon1, &yycon1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon2, &yycon2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon3, &yycon3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon4, &yycon4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon5, &yycon5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon1, &zzcon1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon2, &zzcon2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon3, &zzcon3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon4, &zzcon4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon5, &zzcon5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ce), &ce, 13*5*sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dt), &dt, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1), &c1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2), &c2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3), &c3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c4), &c4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c5), &c5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dIMAXm1), &dIMAXm1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dJMAXm1), &dJMAXm1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dKMAXm1), &dKMAXm1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1c2), &c1c2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1c5), &c1c5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4), &c3c4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1345), &c1345, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::coKMAX1), &coKMAX1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx1), &tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx2), &tx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx3), &tx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty1), &ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty2), &ty2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty3), &ty3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz1), &tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz2), &tz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz3), &tz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx1), &dx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx2), &dx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx3), &dx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx4), &dx4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx5), &dx5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy1), &dy1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy2), &dy2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy3), &dy3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy4), &dy4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy5), &dy5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz1), &dz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz2), &dz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz3), &dz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz4), &dz4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz5), &dz5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dxmax), &dxmax, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dymax), &dymax, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dzmax), &dzmax, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dssp), &dssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c4dssp), &c4dssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c5dssp), &c5dssp, sizeof(double));	
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttx1), &dttx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttx2), &dttx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dtty1), &dtty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dtty2), &dtty2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttz1), &dttz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttz2), &dttz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2dttx1), &c2dttx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2dtty1), &c2dtty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2dttz1), &c2dttz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dtdssp), &dtdssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz1), &comz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz4), &comz4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz5), &comz5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz6), &comz6, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4tx3), &c3c4tx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4ty3), &c3c4ty3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4tz3), &c3c4tz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx1tx1), &dx1tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx2tx1), &dx2tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx3tx1), &dx3tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx4tx1), &dx4tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx5tx1), &dx5tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy1ty1), &dy1ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy2ty1), &dy2ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy3ty1), &dy3ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy4ty1), &dy4ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy5ty1), &dy5ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz1tz1), &dz1tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz2tz1), &dz2tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz3tz1), &dz3tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz4tz1), &dz4tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz5tz1), &dz5tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2iv), &c2iv, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::con43), &con43, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::con16), &con16, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon1), &xxcon1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon2), &xxcon2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon3), &xxcon3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon4), &xxcon4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon5), &xxcon5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon1), &yycon1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon2), &yycon2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon3), &yycon3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon4), &yycon4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon5), &yycon5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon1), &zzcon1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon2), &zzcon2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon3), &zzcon3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon4), &zzcon4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon5), &zzcon5, sizeof(double));
 }
 
 static void setup_gpu(){
 	/*
-	 * struct cudaDeviceProp{
+	 * struct hipDeviceProp_t{
 	 *  char name[256];
 	 *  size_t totalGlobalMem;
 	 *  size_t sharedMemPerBlock;
@@ -2420,222 +2416,18 @@ static void setup_gpu(){
 	 * }
 	 */
 	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);
+	hipGetDeviceCount(&total_devices);
 
 	/* define gpu_device */
 	if(total_devices==0){
 		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
 		exit(-1);
 	}
-	else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
-	}
 	else{
 		gpu_device_id = 0;
 	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	/* new */
-	if((BT_THREADS_PER_BLOCK_ON_ADD>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_ADD<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ADD = BT_THREADS_PER_BLOCK_ON_ADD;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ADD = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_1 = BT_THREADS_PER_BLOCK_ON_RHS_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_1 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_2 = BT_THREADS_PER_BLOCK_ON_RHS_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_2 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_3>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_3 = BT_THREADS_PER_BLOCK_ON_RHS_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_3 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_4>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_4<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_4 = BT_THREADS_PER_BLOCK_ON_RHS_4;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_4 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_5>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_5<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_5 = BT_THREADS_PER_BLOCK_ON_RHS_5;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_5 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_6>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_6<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_6 = BT_THREADS_PER_BLOCK_ON_RHS_6;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_6 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_7>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_7<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_7 = BT_THREADS_PER_BLOCK_ON_RHS_7;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_7 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_8>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_8<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_8 = BT_THREADS_PER_BLOCK_ON_RHS_8;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_8 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_9>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_9<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_9 = BT_THREADS_PER_BLOCK_ON_RHS_9;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_9 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_X_SOLVE_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_X_SOLVE_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_X_SOLVE_1 = BT_THREADS_PER_BLOCK_ON_X_SOLVE_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_X_SOLVE_1 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_X_SOLVE_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_X_SOLVE_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_X_SOLVE_2 = BT_THREADS_PER_BLOCK_ON_X_SOLVE_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_X_SOLVE_2 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_X_SOLVE_3>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_X_SOLVE_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_X_SOLVE_3 = BT_THREADS_PER_BLOCK_ON_X_SOLVE_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_X_SOLVE_3 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_Y_SOLVE_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_Y_SOLVE_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Y_SOLVE_1 = BT_THREADS_PER_BLOCK_ON_Y_SOLVE_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Y_SOLVE_1 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_Y_SOLVE_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_Y_SOLVE_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Y_SOLVE_2 = BT_THREADS_PER_BLOCK_ON_Y_SOLVE_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Y_SOLVE_2 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_Y_SOLVE_3>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_Y_SOLVE_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Y_SOLVE_3 = BT_THREADS_PER_BLOCK_ON_Y_SOLVE_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Y_SOLVE_3 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_Z_SOLVE_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_Z_SOLVE_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Z_SOLVE_1 = BT_THREADS_PER_BLOCK_ON_Z_SOLVE_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Z_SOLVE_1 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_Z_SOLVE_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_Z_SOLVE_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Z_SOLVE_2 = BT_THREADS_PER_BLOCK_ON_Z_SOLVE_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Z_SOLVE_2 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_Z_SOLVE_3>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_Z_SOLVE_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Z_SOLVE_3 = BT_THREADS_PER_BLOCK_ON_Z_SOLVE_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Z_SOLVE_3 = gpu_device_properties.warpSize;
-	}
-	/* old */
-	if((BT_THREADS_PER_BLOCK_ON_EXACT_RHS_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_EXACT_RHS_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_1 = BT_THREADS_PER_BLOCK_ON_EXACT_RHS_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_1 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_EXACT_RHS_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_EXACT_RHS_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_2 = BT_THREADS_PER_BLOCK_ON_EXACT_RHS_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_2 = gpu_device_properties.warpSize;
-	}	
-	if((BT_THREADS_PER_BLOCK_ON_EXACT_RHS_3>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_EXACT_RHS_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_3 = BT_THREADS_PER_BLOCK_ON_EXACT_RHS_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_3 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_EXACT_RHS_4>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_EXACT_RHS_4<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_4 = BT_THREADS_PER_BLOCK_ON_EXACT_RHS_4;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_4 = gpu_device_properties.warpSize;
-	}	
-	if((BT_THREADS_PER_BLOCK_ON_ERROR_NORM_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_ERROR_NORM_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERROR_NORM_1 = BT_THREADS_PER_BLOCK_ON_ERROR_NORM_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERROR_NORM_1 = gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_ERROR_NORM_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_ERROR_NORM_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERROR_NORM_2 = BT_THREADS_PER_BLOCK_ON_ERROR_NORM_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERROR_NORM_2=gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_INITIALIZE>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_INITIALIZE<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_INITIALIZE = BT_THREADS_PER_BLOCK_ON_INITIALIZE;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_INITIALIZE=gpu_device_properties.warpSize;
-	}
-	if((BT_THREADS_PER_BLOCK_ON_RHS_NORM_1>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_NORM_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_NORM_1 = BT_THREADS_PER_BLOCK_ON_RHS_NORM_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_NORM_1 = gpu_device_properties.warpSize;
-	}	
-	if((BT_THREADS_PER_BLOCK_ON_RHS_NORM_2>=1)&&
-			(BT_THREADS_PER_BLOCK_ON_RHS_NORM_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_NORM_2 = BT_THREADS_PER_BLOCK_ON_RHS_NORM_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_NORM_2 = gpu_device_properties.warpSize;
-	}	
+	hipSetDevice(gpu_device_id);	
+	hipGetDeviceProperties(&gpu_device_properties, gpu_device_id);
 
 	size_u = sizeof(double)*KMAX*(JMAXP+1)*(IMAXP+1)*5;
 	size_forcing = sizeof(double)*KMAX*(JMAXP+1)*(IMAXP+1)*5;
@@ -2709,18 +2501,18 @@ static void setup_gpu(){
 	lhsB_buf_size *= (JMAXP+1);
 	lhsC_buf_size *= (JMAXP+1);
 
-	cudaMalloc(&forcing_device, forcing_buf_size);
-	cudaMalloc(&u_device, u_buf_size);
-	cudaMalloc(&rhs_device, rhs_buf_size);
-	cudaMalloc(&us_device, us_buf_size);
-	cudaMalloc(&vs_device, vs_buf_size);
-	cudaMalloc(&ws_device, ws_buf_size);
-	cudaMalloc(&qs_device, qs_buf_size);
-	cudaMalloc(&rho_i_device, rho_i_buf_size);
-	cudaMalloc(&square_device, square_buf_size);
-	cudaMalloc(&lhsA_device, lhsA_buf_size);
-	cudaMalloc(&lhsB_device, lhsB_buf_size);
-	cudaMalloc(&lhsC_device, lhsC_buf_size);
+	hipMalloc(&forcing_device, forcing_buf_size);
+	hipMalloc(&u_device, u_buf_size);
+	hipMalloc(&rhs_device, rhs_buf_size);
+	hipMalloc(&us_device, us_buf_size);
+	hipMalloc(&vs_device, vs_buf_size);
+	hipMalloc(&ws_device, ws_buf_size);
+	hipMalloc(&qs_device, qs_buf_size);
+	hipMalloc(&rho_i_device, rho_i_buf_size);
+	hipMalloc(&square_device, square_buf_size);
+	hipMalloc(&lhsA_device, lhsA_buf_size);
+	hipMalloc(&lhsB_device, lhsB_buf_size);
+	hipMalloc(&lhsC_device, lhsC_buf_size);
 }
 
 /*
@@ -2747,12 +2539,12 @@ void verify(int no_time_steps, char *class_npb, boolean *verified){
 	 */
 	error_norm(xce);
 
-	cudaMemcpy(u_device, u, sizeof(double) * KMAX * (JMAXP+1) * (IMAXP+1) * 5, cudaMemcpyHostToDevice);
-	cudaMemcpy(forcing_device, forcing, sizeof(double) * KMAX * (JMAXP+1) * (IMAXP+1) * 5, cudaMemcpyHostToDevice);
+	hipMemcpy(u_device, u, sizeof(double) * KMAX * (JMAXP+1) * (IMAXP+1) * 5, hipMemcpyHostToDevice);
+	hipMemcpy(forcing_device, forcing, sizeof(double) * KMAX * (JMAXP+1) * (IMAXP+1) * 5, hipMemcpyHostToDevice);
 
 	compute_rhs_gpu();
 
-	cudaMemcpy(rhs, rhs_device, sizeof(double) * KMAX * (JMAXP+1) * (IMAXP+1) * 5, cudaMemcpyDeviceToHost);
+	hipMemcpy(rhs, rhs_device, sizeof(double) * KMAX * (JMAXP+1) * (IMAXP+1) * 5, hipMemcpyDeviceToHost);
 
 	rhs_norm(xcr);
 
diff --git a/CUDA/BT/setup.ini b/CUDA/BT/setup.ini
new file mode 100644
index 0000000..2811617
--- /dev/null
+++ b/CUDA/BT/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/bt.S;../bin/bt.W;../bin/bt.A;../bin/bt.B
+use_clang_plugin = true
+clang_convert = make CLASS=A INJECT_CODE_CLANG=1
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/CG/Makefile b/CUDA/CG/Makefile
index 632370c..674ed3a 100644
--- a/CUDA/CG/Makefile
+++ b/CUDA/CG/Makefile
@@ -16,5 +16,5 @@ cg.o:             cg.cu npbparams.hpp
 	${CCOMPILE} cg.cu
 
 clean:
-	- rm -f *.o *~ 
+	- rm -f *.o *.bc seq.txt *~ 
 	- rm -f npbparams.hpp core
diff --git a/CUDA/CG/Makefile.all b/CUDA/CG/Makefile.all
new file mode 100644
index 0000000..0deb4d2
--- /dev/null
+++ b/CUDA/CG/Makefile.all
@@ -0,0 +1,23 @@
+SHELL=/bin/sh
+BENCHMARK=cg
+BENCHMARKU=CG
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+llvm:
+	$(MAKE) CLASS=S INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=W INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=A INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=B INJECT_CODE_LLVM=1
+	$(MAKE) clean 
diff --git a/CUDA/CG/cg.cu b/CUDA/CG/cg.cu
index 715974b..9fb6dde 100644
--- a/CUDA/CG/cg.cu
+++ b/CUDA/CG/cg.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -57,7 +58,7 @@
  * ------------------------------------------------------------------------------
  */
 
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -80,18 +81,34 @@
  */
 #define NZ (NA*(NONZER+1)*(NONZER+1))
 #define NAZ (NA*(NONZER+1))
-#define PROFILING_TOTAL_TIME (0)
-#define PROFILING_KERNEL_ONE (1)
-#define PROFILING_KERNEL_TWO (2)
-#define PROFILING_KERNEL_THREE (3)
-#define PROFILING_KERNEL_FOUR (4)
-#define PROFILING_KERNEL_FIVE (5)
-#define PROFILING_KERNEL_SIX (6)
-#define PROFILING_KERNEL_SEVEN (7)
-#define PROFILING_KERNEL_EIGHT (8)
-#define PROFILING_KERNEL_NINE (9)
-#define PROFILING_KERNEL_TEN (10)
-#define PROFILING_KERNEL_ELEVEN (11)
+#define T_INIT (0)
+#define T_BENCH (1)
+#define T_CONJ_GRAD (2)
+#define T_LAST (3)
+#define PROFILING_KERNEL_ONE (21)
+#define PROFILING_KERNEL_TWO (22)
+#define PROFILING_KERNEL_THREE (23)
+#define PROFILING_KERNEL_FOUR (24)
+#define PROFILING_KERNEL_FIVE (25)
+#define PROFILING_KERNEL_SIX (26)
+#define PROFILING_KERNEL_SEVEN (27)
+#define PROFILING_KERNEL_EIGHT (28)
+#define PROFILING_KERNEL_NINE (29)
+#define PROFILING_KERNEL_TEN (30)
+#define PROFILING_KERNEL_ELEVEN (31)
+#define PROFILING_KERNEL_FIVE_MERGED_KERNEL_SIX (32)
+#define MINIMUM_THREADS_PER_BLOCK (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_ONE (128)
+#define THREADS_PER_BLOCK_ON_KERNEL_TWO (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_THREE (32)
+#define THREADS_PER_BLOCK_ON_KERNEL_FOUR (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_FIVE (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_SIX (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_SEVEN (128)
+#define THREADS_PER_BLOCK_ON_KERNEL_EIGHT (32)
+#define THREADS_PER_BLOCK_ON_KERNEL_NINE (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_TEN (64)
+#define THREADS_PER_BLOCK_ON_KERNEL_ELEVEN (128)
 
 /* global variables */
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
@@ -129,6 +146,7 @@ static int firstcol;
 static int lastcol;
 static double amult;
 static double tran;
+static boolean timeron;
 /* gpu variables */
 int* colidx_device;
 int* rowstr_device;
@@ -172,53 +190,39 @@ size_t size_beta_device;
 size_t size_sum_device;
 size_t size_norm_temp1_device;
 size_t size_norm_temp2_device;
-int blocks_per_grid_on_kernel_one;
-int blocks_per_grid_on_kernel_two;
-int blocks_per_grid_on_kernel_three;
-int blocks_per_grid_on_kernel_four;
-int blocks_per_grid_on_kernel_five;
-int blocks_per_grid_on_kernel_six;
-int blocks_per_grid_on_kernel_seven;
-int blocks_per_grid_on_kernel_eight;
-int blocks_per_grid_on_kernel_nine;
-int blocks_per_grid_on_kernel_ten;
-int blocks_per_grid_on_kernel_eleven;
-int threads_per_block_on_kernel_one;
-int threads_per_block_on_kernel_two;
-int threads_per_block_on_kernel_three;
-int threads_per_block_on_kernel_four;
-int threads_per_block_on_kernel_five;
-int threads_per_block_on_kernel_six;
-int threads_per_block_on_kernel_seven;
-int threads_per_block_on_kernel_eight;
-int threads_per_block_on_kernel_nine;
-int threads_per_block_on_kernel_ten;
-int threads_per_block_on_kernel_eleven;
-size_t size_shared_data_on_kernel_one;
-size_t size_shared_data_on_kernel_two;
-size_t size_shared_data_on_kernel_three;
-size_t size_shared_data_on_kernel_four;
-size_t size_shared_data_on_kernel_five;
-size_t size_shared_data_on_kernel_six;
-size_t size_shared_data_on_kernel_seven;
-size_t size_shared_data_on_kernel_eight;
-size_t size_shared_data_on_kernel_nine;
-size_t size_shared_data_on_kernel_ten;
-size_t size_shared_data_on_kernel_eleven;
-size_t size_reduce_memory_on_kernel_one;
-size_t size_reduce_memory_on_kernel_two;
-size_t size_reduce_memory_on_kernel_three;
-size_t size_reduce_memory_on_kernel_four;
-size_t size_reduce_memory_on_kernel_five;
-size_t size_reduce_memory_on_kernel_six;
-size_t size_reduce_memory_on_kernel_seven;
-size_t size_reduce_memory_on_kernel_eight;
-size_t size_reduce_memory_on_kernel_nine;
-size_t size_reduce_memory_on_kernel_ten;
-size_t size_reduce_memory_on_kernel_eleven;
-int gpu_device_id;
-int total_devices;
-cudaDeviceProp gpu_device_properties;
+size_t kernel_one_blocks_per_grid;
+size_t kernel_two_blocks_per_grid;
+size_t kernel_three_blocks_per_grid;
+size_t kernel_four_blocks_per_grid;
+size_t kernel_five_blocks_per_grid;
+size_t kernel_six_blocks_per_grid;
+size_t kernel_seven_blocks_per_grid;
+size_t kernel_eight_blocks_per_grid;
+size_t kernel_nine_blocks_per_grid;
+size_t kernel_ten_blocks_per_grid;
+size_t kernel_eleven_blocks_per_grid;
+size_t amount_of_share_data_on_kernel_one;
+size_t amount_of_share_data_on_kernel_two;
+size_t amount_of_share_data_on_kernel_three;
+size_t amount_of_share_data_on_kernel_four;
+size_t amount_of_share_data_on_kernel_five;
+size_t amount_of_share_data_on_kernel_six;
+size_t amount_of_share_data_on_kernel_seven;
+size_t amount_of_share_data_on_kernel_eight;
+size_t amount_of_share_data_on_kernel_nine;
+size_t amount_of_share_data_on_kernel_ten;
+size_t amount_of_share_data_on_kernel_eleven;
+size_t reduce_memory_on_kernel_one;
+size_t reduce_memory_on_kernel_two;
+size_t reduce_memory_on_kernel_three;
+size_t reduce_memory_on_kernel_four;
+size_t reduce_memory_on_kernel_five;
+size_t reduce_memory_on_kernel_six;
+size_t reduce_memory_on_kernel_seven;
+size_t reduce_memory_on_kernel_eight;
+size_t reduce_memory_on_kernel_nine;
+size_t reduce_memory_on_kernel_ten;
+size_t reduce_memory_on_kernel_eleven;
 extern __shared__ double extern_share_data[];
 
 /* function prototypes */
@@ -260,6 +264,14 @@ __global__ void gpu_kernel_five_1(double alpha,
 __global__ void gpu_kernel_five_2(double alpha, 
 		double* q, 
 		double* r);
+static void gpu_kernel_five_merged_kernel_six(double alpha_host, 
+		double* rho_host);
+__global__ void gpu_kernel_five_merged_kernel_six(double alpha, 
+		double* p, 
+		double* q, 
+		double* r, 
+		double* z,
+		double global_data[]);
 static void gpu_kernel_six(double* rho_host);
 __global__ void gpu_kernel_six(double r[],
 		double global_data[]);
@@ -280,6 +292,10 @@ __global__ void gpu_kernel_nine(double r[],
 		double global_data[]);
 static void gpu_kernel_ten(double* norm_temp1, 
 		double* norm_temp2);
+__global__ void gpu_kernel_ten(double* norm_temp1, 
+		double* norm_temp2, 
+		double x[], 
+		double z[]);
 __global__ void gpu_kernel_ten_1(double* norm_temp, 
 		double x[], 
 		double z[]);
@@ -337,21 +353,21 @@ static void vecset(int n,
 int main(int argc, char** argv){
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
 	printf(" DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION mode on\n");
-#endif
-#if defined(PROFILING)
-	printf(" PROFILING mode on\n");
 #endif
 	int	i, j, k, it;
 	double zeta;
 	double rnorm;
 	double norm_temp1, norm_temp2;
-	double t, mflops;
+	double t, mflops, tmax;
 	char class_npb;
 	boolean verified;
 	double zeta_verify_value, epsilon, err;
 
-	timer_clear(PROFILING_TOTAL_TIME);
-#if defined(PROFILING)
+	char *t_names[T_LAST];
+
+	for(i=0; i<T_LAST; i++){
+		timer_clear(i);
+	}
 	timer_clear(PROFILING_KERNEL_ONE);
 	timer_clear(PROFILING_KERNEL_TWO);
 	timer_clear(PROFILING_KERNEL_THREE);
@@ -363,7 +379,20 @@ int main(int argc, char** argv){
 	timer_clear(PROFILING_KERNEL_NINE);
 	timer_clear(PROFILING_KERNEL_TEN);
 	timer_clear(PROFILING_KERNEL_ELEVEN);
-#endif
+	timer_clear(PROFILING_KERNEL_FIVE_MERGED_KERNEL_SIX);
+
+	FILE* fp;
+	if((fp = fopen("timer.flag", "r")) != NULL){
+		timeron = TRUE;
+		t_names[T_INIT] = (char*)"init";
+		t_names[T_BENCH] = (char*)"benchmk";
+		t_names[T_CONJ_GRAD] = (char*)"conjgd";
+		fclose(fp);
+	}else{
+		timeron = FALSE;
+	}
+
+	timer_start(T_INIT);
 
 	firstrow = 0;
 	lastrow  = NA-1;
@@ -487,8 +516,12 @@ int main(int argc, char** argv){
 	}
 	zeta = 0.0;
 
+	timer_stop(T_INIT);
+
+	printf(" Initialization time = %15.3f seconds\n", timer_read(T_INIT));
+
 	setup_gpu();
-	timer_start(PROFILING_TOTAL_TIME);
+	timer_start(T_BENCH);
 
 	/*
 	 * --------------------------------------------------------------------
@@ -499,7 +532,9 @@ int main(int argc, char** argv){
 	 */
 	for(it = 1; it <= NITER; it++){
 		/* the call to the conjugate gradient routine */
+		if(timeron){timer_start(T_CONJ_GRAD);}
 		conj_grad_gpu(&rnorm);
+		if(timeron){timer_stop(T_CONJ_GRAD);}
 
 		/*
 		 * --------------------------------------------------------------------
@@ -519,7 +554,7 @@ int main(int argc, char** argv){
 		gpu_kernel_eleven(norm_temp2);
 	} /* end of main iter inv pow meth */
 
-	timer_stop(PROFILING_TOTAL_TIME);
+	timer_stop(T_BENCH);
 
 	/*
 	 * --------------------------------------------------------------------
@@ -527,7 +562,7 @@ int main(int argc, char** argv){
 	 * --------------------------------------------------------------------
 	 */
 
-	t = timer_read(PROFILING_TOTAL_TIME);
+	t = timer_read(T_BENCH);
 
 	printf(" Benchmark completed\n");
 
@@ -559,61 +594,6 @@ int main(int argc, char** argv){
 	}else{
 		mflops = 0.0;
 	}
-
-	char gpu_config[256];
-	char gpu_config_string[2048];
-#if defined(PROFILING)
-	sprintf(gpu_config, "%5s\t%25s\t%25s\t%25s\n", "GPU Kernel", "Threads Per Block", "Time in Seconds", "Time in Percentage");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " one", threads_per_block_on_kernel_one, timer_read(PROFILING_KERNEL_ONE), (timer_read(PROFILING_KERNEL_ONE)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " two", threads_per_block_on_kernel_two, timer_read(PROFILING_KERNEL_TWO), (timer_read(PROFILING_KERNEL_TWO)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " three", threads_per_block_on_kernel_three, timer_read(PROFILING_KERNEL_THREE), (timer_read(PROFILING_KERNEL_THREE)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " four", threads_per_block_on_kernel_four, timer_read(PROFILING_KERNEL_FOUR), (timer_read(PROFILING_KERNEL_FOUR)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " five", threads_per_block_on_kernel_five, timer_read(PROFILING_KERNEL_FIVE), (timer_read(PROFILING_KERNEL_FIVE)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " six", threads_per_block_on_kernel_six, timer_read(PROFILING_KERNEL_SIX), (timer_read(PROFILING_KERNEL_SIX)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " seven", threads_per_block_on_kernel_seven, timer_read(PROFILING_KERNEL_SEVEN), (timer_read(PROFILING_KERNEL_SEVEN)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " eight", threads_per_block_on_kernel_eight, timer_read(PROFILING_KERNEL_EIGHT), (timer_read(PROFILING_KERNEL_EIGHT)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " nine", threads_per_block_on_kernel_nine, timer_read(PROFILING_KERNEL_NINE), (timer_read(PROFILING_KERNEL_NINE)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " ten", threads_per_block_on_kernel_ten, timer_read(PROFILING_KERNEL_TEN), (timer_read(PROFILING_KERNEL_TEN)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " eleven", threads_per_block_on_kernel_eleven, timer_read(PROFILING_KERNEL_ELEVEN), (timer_read(PROFILING_KERNEL_ELEVEN)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-#else
-	sprintf(gpu_config, "%5s\t%25s\n", "GPU Kernel", "Threads Per Block");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " one", threads_per_block_on_kernel_one);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " two", threads_per_block_on_kernel_two);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " three", threads_per_block_on_kernel_three);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " four", threads_per_block_on_kernel_four);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " five", threads_per_block_on_kernel_five);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " six", threads_per_block_on_kernel_six);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " seven", threads_per_block_on_kernel_seven);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " eight", threads_per_block_on_kernel_eight);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " nine", threads_per_block_on_kernel_nine);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " ten", threads_per_block_on_kernel_ten);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " eleven", threads_per_block_on_kernel_eleven);
-	strcat(gpu_config_string, gpu_config);
-#endif
-
 	c_print_results((char*)"CG",
 			class_npb,
 			NA,
@@ -626,11 +606,6 @@ int main(int argc, char** argv){
 			verified,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			(char*)gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
@@ -639,6 +614,29 @@ int main(int argc, char** argv){
 			(char*)CS6,
 			(char*)CS7);
 
+	/*
+	 * ---------------------------------------------------------------------
+	 * more timers
+	 * ---------------------------------------------------------------------
+	 */
+	if(timeron){
+		tmax = timer_read(T_BENCH);
+		if(tmax == 0.0){tmax = 1.0;}
+		printf("  SECTION   Time (secs)\n");
+		for(i = 0; i < T_LAST; i++){
+			t = timer_read(i);
+			if(i == T_INIT){
+				printf("  %8s:%9.3f\n", t_names[i], t);
+			}else{
+				printf("  %8s:%9.3f  (%6.2f%%)\n", t_names[i], t, t*100.0/tmax);
+				if(i == T_CONJ_GRAD){
+					t = tmax - t;
+					printf("    --> %8s:%9.3f  (%6.2f%%)\n", "rest", t, t*100.0/tmax);
+				}
+			}
+		}
+	}
+
 	release_gpu();
 
 	return 0;
@@ -824,10 +822,13 @@ static void conj_grad_gpu(double* rnorm){
 		rho0 = rho;
 
 		/* obtain (z = z + alpha*p) and (r = r - alpha*q) */
-		gpu_kernel_five(alpha);
+		/* gpu_kernel_five(alpha); */
 
 		/* rho = r.r - now, obtain the norm of r: first, sum squares of r elements locally */
-		gpu_kernel_six(&rho);
+		/* gpu_kernel_six(&rho); */
+
+		/* (z = z + alpha*p) and (r = r - alpha*q) and (rho = r.r) */
+		gpu_kernel_five_merged_kernel_six(alpha, &rho);
 
 		/* obtain beta */
 		beta = rho / rho0;
@@ -846,19 +847,13 @@ static void conj_grad_gpu(double* rnorm){
 }
 
 static void gpu_kernel_one(){   
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_ONE);
-#endif
-	gpu_kernel_one<<<blocks_per_grid_on_kernel_one,
-		threads_per_block_on_kernel_one>>>(
+	gpu_kernel_one<<<kernel_one_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_ONE>>>(
 				p_device, 
 				q_device, 
 				r_device, 
 				x_device, 
 				z_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_ONE);
-#endif
 }
 
 __global__ void gpu_kernel_one(double p[], 
@@ -875,23 +870,17 @@ __global__ void gpu_kernel_one(double p[],
 	p[thread_id] = x_value;
 }
 
-static void gpu_kernel_two(double* rho_host){  
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_TWO);
-#endif
-	gpu_kernel_two<<<blocks_per_grid_on_kernel_two,
-		threads_per_block_on_kernel_two,
-		size_shared_data_on_kernel_two>>>(
-				r_device, 
-				rho_device, 
-				global_data_device);
+static void gpu_kernel_two(double* rho_host){   
+	gpu_kernel_two<<<kernel_two_blocks_per_grid,
+	THREADS_PER_BLOCK_ON_KERNEL_TWO,
+	amount_of_share_data_on_kernel_two>>>(
+			r_device, 
+			rho_device, 
+			global_data_device);
 	global_data_reduce=0.0; 
-	cudaMemcpy(global_data, global_data_device, size_reduce_memory_on_kernel_two, cudaMemcpyDeviceToHost);	
-	for(int i=0; i<blocks_per_grid_on_kernel_two; i++){global_data_reduce+=global_data[i];}
+	hipMemcpy(global_data, global_data_device, reduce_memory_on_kernel_two, hipMemcpyDeviceToHost);	
+	for(int i=0; i<kernel_two_blocks_per_grid; i++){global_data_reduce+=global_data[i];}
 	*rho_host=global_data_reduce;
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_TWO);
-#endif
 }
 
 __global__ void gpu_kernel_two(double r[],
@@ -918,20 +907,14 @@ __global__ void gpu_kernel_two(double r[],
 }
 
 static void gpu_kernel_three(){
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_THREE);
-#endif
-	gpu_kernel_three<<<blocks_per_grid_on_kernel_three,
-		threads_per_block_on_kernel_three,
-		size_shared_data_on_kernel_three>>>(
+	gpu_kernel_three<<<kernel_three_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_THREE,
+		amount_of_share_data_on_kernel_three>>>(
 				colidx_device,
 				rowstr_device,
 				a_device,
 				p_device,
 				q_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_THREE);
-#endif
 }
 
 __global__ void gpu_kernel_three(int colidx[], 
@@ -961,23 +944,17 @@ __global__ void gpu_kernel_three(int colidx[],
 }
 
 static void gpu_kernel_four(double* d_host){   
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_FOUR);
-#endif
-	gpu_kernel_four<<<blocks_per_grid_on_kernel_four,
-		threads_per_block_on_kernel_four,
-		size_shared_data_on_kernel_four>>>(
+	gpu_kernel_four<<<kernel_four_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_FOUR,
+		amount_of_share_data_on_kernel_four>>>(
 				d_device, 
 				p_device,
 				q_device,
 				global_data_device);
 	global_data_reduce=0.0; 
-	cudaMemcpy(global_data, global_data_device, size_reduce_memory_on_kernel_four, cudaMemcpyDeviceToHost);
-	for(int i=0; i<blocks_per_grid_on_kernel_four; i++){global_data_reduce+=global_data[i];}
+	hipMemcpy(global_data, global_data_device, reduce_memory_on_kernel_four, hipMemcpyDeviceToHost);
+	for(int i=0; i<kernel_four_blocks_per_grid; i++){global_data_reduce+=global_data[i];}
 	*d_host=global_data_reduce;
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_FOUR);
-#endif
 }
 
 __global__ void gpu_kernel_four(double* d, 
@@ -1003,23 +980,17 @@ __global__ void gpu_kernel_four(double* d,
 	if(local_id==0){global_data[blockIdx.x]=share_data[0];}
 }
 
-static void gpu_kernel_five(double alpha_host){
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_FIVE);
-#endif
-	gpu_kernel_five_1<<<blocks_per_grid_on_kernel_five,
-		threads_per_block_on_kernel_five>>>(
+static void gpu_kernel_five(double alpha_host){   
+	gpu_kernel_five_1<<<kernel_five_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_FIVE>>>(
 				alpha_host,
 				p_device,
 				z_device);
-	gpu_kernel_five_2<<<blocks_per_grid_on_kernel_five,
-		threads_per_block_on_kernel_five>>>(
+	gpu_kernel_five_2<<<kernel_five_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_FIVE>>>(
 				alpha_host,
 				q_device,
 				r_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_FIVE);
-#endif
 } 
 
 __global__ void gpu_kernel_five_1(double alpha, 
@@ -1038,22 +1009,61 @@ __global__ void gpu_kernel_five_2(double alpha,
 	r[j] -= alpha * q[j];
 }
 
-static void gpu_kernel_six(double* rho_host){
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_SIX);
-#endif
-	gpu_kernel_six<<<blocks_per_grid_on_kernel_six,
-		threads_per_block_on_kernel_six,
-		size_shared_data_on_kernel_six>>>(
+static void gpu_kernel_five_merged_kernel_six(double alpha_host, 
+		double* rho_host){
+	gpu_kernel_five_merged_kernel_six<<<kernel_five_blocks_per_grid, 
+	THREADS_PER_BLOCK_ON_KERNEL_FIVE,
+	amount_of_share_data_on_kernel_five>>>(
+			alpha_host,
+			p_device,
+			q_device,
+			r_device,
+			z_device,
+			global_data_device);
+	global_data_reduce=0.0;
+	hipMemcpy(global_data, global_data_device, reduce_memory_on_kernel_five, hipMemcpyDeviceToHost);
+	for(int i=0; i<kernel_five_blocks_per_grid; i++){global_data_reduce+=global_data[i];}
+	*rho_host=global_data_reduce;
+}
+
+__global__ void gpu_kernel_five_merged_kernel_six(double alpha, 
+		double* p, 
+		double* q, 
+		double* r, 
+		double* z, 
+		double global_data[]){
+	double* share_data = (double*)extern_share_data;
+	int thread_id = blockIdx.x * blockDim.x + threadIdx.x;
+	int local_id = threadIdx.x;
+	/* kernel_five computation */
+	if(thread_id < NA){
+		double r_value;
+		z[thread_id] = z[thread_id] + alpha*p[thread_id];
+		r_value = r[thread_id] - alpha*q[thread_id];
+		r[thread_id] = r_value;
+		share_data[local_id] = r_value * r_value;
+	}else{
+		share_data[local_id] = 0.0;
+	}	 
+	/* kernel_six computation */
+	__syncthreads();	  
+	for(int i=blockDim.x/2; i>0; i>>=1){
+		if(local_id<i){share_data[local_id]+=share_data[local_id+i];}
+		__syncthreads();
+	}
+	if(local_id==0){global_data[blockIdx.x]=share_data[0];}
+}
+
+static void gpu_kernel_six(double* rho_host){   
+	gpu_kernel_six<<<kernel_six_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_SIX,
+		amount_of_share_data_on_kernel_six>>>(
 				r_device, 
 				global_data_device);
 	global_data_reduce=0.0;
-	cudaMemcpy(global_data, global_data_device, size_reduce_memory_on_kernel_six, cudaMemcpyDeviceToHost);
-	for(int i=0; i<blocks_per_grid_on_kernel_six; i++){global_data_reduce+=global_data[i];}
+	hipMemcpy(global_data, global_data_device, reduce_memory_on_kernel_six, hipMemcpyDeviceToHost);
+	for(int i=0; i<kernel_six_blocks_per_grid; i++){global_data_reduce+=global_data[i];}
 	*rho_host=global_data_reduce;
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_SIX);
-#endif
 } 
 
 __global__ void gpu_kernel_six(double r[], 
@@ -1074,17 +1084,11 @@ __global__ void gpu_kernel_six(double r[],
 }
 
 static void gpu_kernel_seven(double beta_host){
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_SEVEN);
-#endif
-	gpu_kernel_seven<<<blocks_per_grid_on_kernel_seven,
-		threads_per_block_on_kernel_seven>>>(
-				beta_host,
-				p_device,
-				r_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_SEVEN);
-#endif
+	gpu_kernel_seven<<<kernel_seven_blocks_per_grid,
+	THREADS_PER_BLOCK_ON_KERNEL_SEVEN>>>(
+			beta_host,
+			p_device,
+			r_device);
 }
 
 __global__ void gpu_kernel_seven(double beta, 
@@ -1096,20 +1100,14 @@ __global__ void gpu_kernel_seven(double beta,
 }
 
 static void gpu_kernel_eight(){
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_EIGHT);
-#endif
-	gpu_kernel_eight<<<blocks_per_grid_on_kernel_eight,
-		threads_per_block_on_kernel_eight,
-		size_shared_data_on_kernel_eight>>>(
-				colidx_device, 
-				rowstr_device, 
-				a_device, 
-				r_device, 
-				z_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_EIGHT);
-#endif
+	gpu_kernel_eight<<<kernel_eight_blocks_per_grid,
+	THREADS_PER_BLOCK_ON_KERNEL_EIGHT,
+	amount_of_share_data_on_kernel_eight>>>(
+			colidx_device, 
+			rowstr_device, 
+			a_device, 
+			r_device, 
+			z_device);
 }
 
 __global__ void gpu_kernel_eight(int colidx[], 
@@ -1138,24 +1136,18 @@ __global__ void gpu_kernel_eight(int colidx[],
 	if(local_id==0){r[j]=share_data[0];}
 }
 
-static void gpu_kernel_nine(double* sum_host){ 
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_NINE);
-#endif 
-	gpu_kernel_nine<<<blocks_per_grid_on_kernel_nine,
-		threads_per_block_on_kernel_nine,
-		size_shared_data_on_kernel_nine>>>(
+static void gpu_kernel_nine(double* sum_host){   
+	gpu_kernel_nine<<<kernel_nine_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_NINE,
+		amount_of_share_data_on_kernel_nine>>>(
 				r_device, 
 				x_device, 
 				sum_device,
 				global_data_device);
 	global_data_reduce=0.0;
-	cudaMemcpy(global_data, global_data_device, size_reduce_memory_on_kernel_nine, cudaMemcpyDeviceToHost);
-	for(int i=0; i<blocks_per_grid_on_kernel_nine; i++){global_data_reduce+=global_data[i];}
+	hipMemcpy(global_data, global_data_device, reduce_memory_on_kernel_nine, hipMemcpyDeviceToHost);
+	for(int i=0; i<kernel_nine_blocks_per_grid; i++){global_data_reduce+=global_data[i];}
 	*sum_host=global_data_reduce;
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_NINE);
-#endif
 }
 
 __global__ void gpu_kernel_nine(double r[], double x[], double* sum, double global_data[]){
@@ -1181,23 +1173,48 @@ __global__ void gpu_kernel_nine(double r[], double x[], double* sum, double glob
 
 static void gpu_kernel_ten(double* norm_temp1, 
 		double* norm_temp2){
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_TEN);
-#endif
-	gpu_kernel_ten_1<<<blocks_per_grid_on_kernel_ten,threads_per_block_on_kernel_ten,size_shared_data_on_kernel_ten>>>(global_data_device,x_device,z_device);
-	gpu_kernel_ten_2<<<blocks_per_grid_on_kernel_ten,threads_per_block_on_kernel_ten,size_shared_data_on_kernel_ten>>>(global_data_two_device,x_device,z_device);
+	//gpu_kernel_ten<<<kernel_ten_blocks_per_grid,THREADS_PER_BLOCK_ON_KERNEL_TEN,amount_of_share_data_on_kernel_ten*2>>>(global_data_device,global_data_two_device,x_device,z_device);
+	gpu_kernel_ten_1<<<kernel_ten_blocks_per_grid,THREADS_PER_BLOCK_ON_KERNEL_TEN,amount_of_share_data_on_kernel_ten>>>(global_data_device,x_device,z_device);
+	gpu_kernel_ten_2<<<kernel_ten_blocks_per_grid,THREADS_PER_BLOCK_ON_KERNEL_TEN,amount_of_share_data_on_kernel_ten>>>(global_data_two_device,x_device,z_device);
 
 	global_data_reduce=0.0; 
 	global_data_two_reduce=0.0; 
-	cudaMemcpy(global_data, global_data_device, size_reduce_memory_on_kernel_ten, cudaMemcpyDeviceToHost);
-	cudaMemcpy(global_data_two, global_data_two_device, size_reduce_memory_on_kernel_ten, cudaMemcpyDeviceToHost);
+	hipMemcpy(global_data, global_data_device, reduce_memory_on_kernel_ten, hipMemcpyDeviceToHost);
+	hipMemcpy(global_data_two, global_data_two_device, reduce_memory_on_kernel_ten, hipMemcpyDeviceToHost);
 
-	for(int i=0; i<blocks_per_grid_on_kernel_ten; i++){global_data_reduce+=global_data[i];global_data_two_reduce+=global_data_two[i];}
+	for(int i=0; i<kernel_ten_blocks_per_grid; i++){global_data_reduce+=global_data[i];global_data_two_reduce+=global_data_two[i];}
 	*norm_temp1=global_data_reduce;
 	*norm_temp2=global_data_two_reduce;
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_TEN);
-#endif
+}
+
+__global__ void gpu_kernel_ten(double* norm_temp1,
+		double* norm_temp2, 
+		double x[], 
+		double z[]){
+	double* share_data_1 = (double*)(extern_share_data);
+	double* share_data_2 = (double*)(&share_data_1[THREADS_PER_BLOCK_ON_KERNEL_TEN]);
+
+	int thread_id = blockIdx.x * blockDim.x + threadIdx.x;
+	int local_id = threadIdx.x;
+
+	share_data_1[threadIdx.x] = 0.0;
+	share_data_2[threadIdx.x] = 0.0;
+
+	if(thread_id >= NA){return;}
+
+	share_data_1[threadIdx.x] = x[thread_id]*z[thread_id];
+	share_data_2[threadIdx.x] = z[thread_id]*z[thread_id];
+
+	__syncthreads();
+	for(int i=blockDim.x/2; i>0; i>>=1){
+		if(local_id<i){
+			share_data_1[local_id]+=share_data_1[local_id+i];
+			share_data_2[local_id]+=share_data_2[local_id+i];}
+		__syncthreads();
+	}
+	if(local_id==0){
+		norm_temp1[blockIdx.x]=share_data_1[0];
+		norm_temp2[blockIdx.x]=share_data_2[0];}
 }
 
 __global__ void gpu_kernel_ten_1(double* norm_temp, 
@@ -1245,17 +1262,11 @@ __global__ void gpu_kernel_ten_2(double* norm_temp,
 }
 
 static void gpu_kernel_eleven(double norm_temp2){   
-#if defined(PROFILING)
-	timer_start(PROFILING_KERNEL_ELEVEN);
-#endif
-	gpu_kernel_eleven<<<blocks_per_grid_on_kernel_eleven,
-		threads_per_block_on_kernel_eleven>>>(
+	gpu_kernel_eleven<<<kernel_eleven_blocks_per_grid,
+		THREADS_PER_BLOCK_ON_KERNEL_ELEVEN>>>(
 				norm_temp2,
 				x_device,
 				z_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_KERNEL_ELEVEN);
-#endif
 }
 
 __global__ void gpu_kernel_eleven(double norm_temp2, double x[], double z[]){
@@ -1368,177 +1379,41 @@ static void makea(int n,
 }
 
 static void release_gpu(){
-	cudaFree(colidx_device);
-	cudaFree(rowstr_device);
-	cudaFree(a_device);
-	cudaFree(p_device);
-	cudaFree(q_device);
-	cudaFree(r_device);
-	cudaFree(x_device);
-	cudaFree(z_device);
-	cudaFree(rho_device);
-	cudaFree(d_device);
-	cudaFree(alpha_device);
-	cudaFree(beta_device);
-	cudaFree(sum_device);
-	cudaFree(norm_temp1_device);
-	cudaFree(norm_temp2_device);
-	cudaFree(global_data_device);
-	cudaFree(global_data_two_device);
+	hipFree(colidx_device);
+	hipFree(rowstr_device);
+	hipFree(a_device);
+	hipFree(p_device);
+	hipFree(q_device);
+	hipFree(r_device);
+	hipFree(x_device);
+	hipFree(z_device);
+	hipFree(rho_device);
+	hipFree(d_device);
+	hipFree(alpha_device);
+	hipFree(beta_device);
+	hipFree(sum_device);
+	hipFree(norm_temp1_device);
+	hipFree(norm_temp2_device);
+	hipFree(global_data_device);
+	hipFree(global_data_two_device);
 }
 
 static void setup_gpu(){
-	/*
-	 * struct cudaDeviceProp{
-	 *  char name[256];
-	 *  size_t totalGlobalMem;
-	 *  size_t sharedMemPerBlock;
-	 *  int regsPerBlock;
-	 *  int warpSize;
-	 *  size_t memPitch;
-	 *  int maxThreadsPerBlock;
-	 *  int maxThreadsDim[3];
-	 *  int maxGridSize[3];
-	 *  size_t totalConstMem;
-	 *  int major;
-	 *  int minor;
-	 *  int clockRate;
-	 *  size_t textureAlignment;
-	 *  int deviceOverlap;
-	 *  int multiProcessorCount;
-	 *  int kernelExecTimeoutEnabled;
-	 *  int integrated;
-	 *  int canMapHostMemory;
-	 *  int computeMode;
-	 *  int concurrentKernels;
-	 *  int ECCEnabled;
-	 *  int pciBusID;
-	 *  int pciDeviceID;
-	 *  int tccDriver;
-	 * }
-	 */
-	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);
-
-	/* define gpu_device */
-	if(total_devices==0){
-		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
-		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
-	}else{
-		gpu_device_id = 0;
-	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_ONE>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_ONE<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_one = CG_THREADS_PER_BLOCK_ON_KERNEL_ONE;
-	}
-	else{
-		threads_per_block_on_kernel_one = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_TWO>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_TWO<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_two = CG_THREADS_PER_BLOCK_ON_KERNEL_TWO;
-	}
-	else{
-		threads_per_block_on_kernel_two = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_THREE>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_THREE<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_three = CG_THREADS_PER_BLOCK_ON_KERNEL_THREE;
-	}
-	else{
-		threads_per_block_on_kernel_three = gpu_device_properties.warpSize;
-	}	
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_four = CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR;
-	}
-	else{
-		threads_per_block_on_kernel_four = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_five = CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE;
-	}
-	else{
-		threads_per_block_on_kernel_five = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_SIX>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_SIX<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_six = CG_THREADS_PER_BLOCK_ON_KERNEL_SIX;
-	}
-	else{
-		threads_per_block_on_kernel_six = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_seven = CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN;
-	}
-	else{
-		threads_per_block_on_kernel_seven = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_eight = CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT;
-	}
-	else{
-		threads_per_block_on_kernel_eight = gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_NINE>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_NINE<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_nine = CG_THREADS_PER_BLOCK_ON_KERNEL_NINE;
-	}
-	else{
-		threads_per_block_on_kernel_nine=gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_TEN>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_TEN<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_ten = CG_THREADS_PER_BLOCK_ON_KERNEL_TEN;
-	}
-	else{
-		threads_per_block_on_kernel_ten=gpu_device_properties.warpSize;
-	}
-	if((CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN>=1)&&
-			(CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_kernel_eleven = CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN;
-	}
-	else{
-		threads_per_block_on_kernel_eleven = gpu_device_properties.warpSize;
-	}	
-
-	blocks_per_grid_on_kernel_one=(ceil((double)NA/(double)threads_per_block_on_kernel_one));
-	blocks_per_grid_on_kernel_two=(ceil((double)NA/(double)threads_per_block_on_kernel_two));   
-	blocks_per_grid_on_kernel_three=NA;
-	blocks_per_grid_on_kernel_four=(ceil((double)NA/(double)threads_per_block_on_kernel_four));
-	blocks_per_grid_on_kernel_five=(ceil((double)NA/(double)threads_per_block_on_kernel_five));
-	blocks_per_grid_on_kernel_six=(ceil((double)NA/(double)threads_per_block_on_kernel_six));
-	blocks_per_grid_on_kernel_seven=(ceil((double)NA/threads_per_block_on_kernel_seven));
-	blocks_per_grid_on_kernel_eight=NA;
-	blocks_per_grid_on_kernel_nine=(ceil((double)NA/(double)threads_per_block_on_kernel_nine));
-	blocks_per_grid_on_kernel_ten=(ceil((double)NA/(double)threads_per_block_on_kernel_ten));
-	blocks_per_grid_on_kernel_eleven=(ceil((double)NA/(double)threads_per_block_on_kernel_eleven));
-
-	global_data_elements=ceil(double(NA)/double(gpu_device_properties.warpSize));
-
-	size_global_data=global_data_elements*sizeof(double);
-	size_colidx_device=NZ*sizeof(int);
-	size_rowstr_device=(NA+1)*sizeof(int);
-	size_iv_device=NA*sizeof(int);
-	size_arow_device=NA*sizeof(int);
-	size_acol_device=NAZ*sizeof(int);
-	size_aelt_device=NAZ*sizeof(double);
-	size_a_device=NZ*sizeof(double);
-	size_x_device=(NA+2)*sizeof(double);
-	size_z_device=(NA+2)*sizeof(double);
-	size_p_device=(NA+2)*sizeof(double);
-	size_q_device=(NA+2)*sizeof(double);
-	size_r_device=(NA+2)*sizeof(double);
+	global_data_elements=ceil(double(NA)/double(MINIMUM_THREADS_PER_BLOCK));
+
+	size_global_data=sizeof(double)*(global_data_elements);
+	size_colidx_device=sizeof(int)*(NZ);
+	size_rowstr_device=sizeof(int)*(NA+1);
+	size_iv_device=sizeof(int)*(NA);
+	size_arow_device=sizeof(int)*(NA);
+	size_acol_device=sizeof(int)*(NAZ);
+	size_aelt_device=sizeof(double)*(NAZ);
+	size_a_device=sizeof(double)*(NZ);
+	size_x_device=sizeof(double)*(NA+2);
+	size_z_device=sizeof(double)*(NA+2);
+	size_p_device=sizeof(double)*(NA+2);
+	size_q_device=sizeof(double)*(NA+2);
+	size_r_device=sizeof(double)*(NA+2);
 	size_rho_device=sizeof(double);
 	size_d_device=sizeof(double);
 	size_alpha_device=sizeof(double);
@@ -1550,56 +1425,68 @@ static void setup_gpu(){
 	global_data=(double*)malloc(size_global_data);
 	global_data_two=(double*)malloc(size_global_data);	
 
-	cudaMalloc(&colidx_device, size_colidx_device);
-	cudaMalloc(&rowstr_device, size_rowstr_device);
-	cudaMalloc(&a_device, size_a_device);
-	cudaMalloc(&p_device, size_p_device);
-	cudaMalloc(&q_device, size_q_device);
-	cudaMalloc(&r_device, size_r_device);
-	cudaMalloc(&x_device, size_x_device);
-	cudaMalloc(&z_device, size_z_device);
-	cudaMalloc(&rho_device, size_rho_device);
-	cudaMalloc(&d_device, size_d_device);
-	cudaMalloc(&alpha_device, size_alpha_device);
-	cudaMalloc(&beta_device, size_beta_device);
-	cudaMalloc(&sum_device, size_sum_device);
-	cudaMalloc(&norm_temp1_device, size_norm_temp1_device);
-	cudaMalloc(&norm_temp2_device, size_norm_temp2_device);
-	cudaMalloc(&global_data_device, size_global_data);
-	cudaMalloc(&global_data_two_device, size_global_data);
-
-	cudaMemcpy(colidx_device, colidx, size_colidx_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(rowstr_device, rowstr, size_rowstr_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(a_device, a, size_a_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(p_device, p, size_p_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(q_device, q, size_q_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(r_device, r, size_r_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(x_device, x, size_x_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(z_device, z, size_z_device, cudaMemcpyHostToDevice);	
-
-	size_shared_data_on_kernel_one=threads_per_block_on_kernel_one*sizeof(double);
-	size_shared_data_on_kernel_two=threads_per_block_on_kernel_two*sizeof(double);
-	size_shared_data_on_kernel_three=threads_per_block_on_kernel_three*sizeof(double);
-	size_shared_data_on_kernel_four=threads_per_block_on_kernel_four*sizeof(double);
-	size_shared_data_on_kernel_five=threads_per_block_on_kernel_five*sizeof(double);
-	size_shared_data_on_kernel_six=threads_per_block_on_kernel_six*sizeof(double);
-	size_shared_data_on_kernel_seven=threads_per_block_on_kernel_seven*sizeof(double);
-	size_shared_data_on_kernel_eight=threads_per_block_on_kernel_eight*sizeof(double);
-	size_shared_data_on_kernel_nine=threads_per_block_on_kernel_nine*sizeof(double);
-	size_shared_data_on_kernel_ten=threads_per_block_on_kernel_ten*sizeof(double);
-	size_shared_data_on_kernel_eleven=threads_per_block_on_kernel_eleven*sizeof(double);
-
-	size_reduce_memory_on_kernel_one=blocks_per_grid_on_kernel_one*sizeof(double);
-	size_reduce_memory_on_kernel_two=blocks_per_grid_on_kernel_two*sizeof(double);
-	size_reduce_memory_on_kernel_three=blocks_per_grid_on_kernel_three*sizeof(double);
-	size_reduce_memory_on_kernel_four=blocks_per_grid_on_kernel_four*sizeof(double);
-	size_reduce_memory_on_kernel_five=blocks_per_grid_on_kernel_five*sizeof(double);
-	size_reduce_memory_on_kernel_six=blocks_per_grid_on_kernel_six*sizeof(double);
-	size_reduce_memory_on_kernel_seven=blocks_per_grid_on_kernel_seven*sizeof(double);
-	size_reduce_memory_on_kernel_eight=blocks_per_grid_on_kernel_eight*sizeof(double);
-	size_reduce_memory_on_kernel_nine=blocks_per_grid_on_kernel_nine*sizeof(double);
-	size_reduce_memory_on_kernel_ten=blocks_per_grid_on_kernel_ten*sizeof(double);
-	size_reduce_memory_on_kernel_eleven=blocks_per_grid_on_kernel_eleven*sizeof(double);
+	hipMalloc(&colidx_device, size_colidx_device);
+	hipMalloc(&rowstr_device, size_rowstr_device);
+	hipMalloc(&a_device, size_a_device);
+	hipMalloc(&p_device, size_p_device);
+	hipMalloc(&q_device, size_q_device);
+	hipMalloc(&r_device, size_r_device);
+	hipMalloc(&x_device, size_x_device);
+	hipMalloc(&z_device, size_z_device);
+	hipMalloc(&rho_device, size_rho_device);
+	hipMalloc(&d_device, size_d_device);
+	hipMalloc(&alpha_device, size_alpha_device);
+	hipMalloc(&beta_device, size_beta_device);
+	hipMalloc(&sum_device, size_sum_device);
+	hipMalloc(&norm_temp1_device, size_norm_temp1_device);
+	hipMalloc(&norm_temp2_device, size_norm_temp2_device);
+	hipMalloc(&global_data_device, size_global_data);
+	hipMalloc(&global_data_two_device, size_global_data);
+
+	hipMemcpy(colidx_device, colidx, size_colidx_device, hipMemcpyHostToDevice);
+	hipMemcpy(rowstr_device, rowstr, size_rowstr_device, hipMemcpyHostToDevice);
+	hipMemcpy(a_device, a, size_a_device, hipMemcpyHostToDevice);
+	hipMemcpy(p_device, p, size_p_device, hipMemcpyHostToDevice);
+	hipMemcpy(q_device, q, size_q_device, hipMemcpyHostToDevice);
+	hipMemcpy(r_device, r, size_r_device, hipMemcpyHostToDevice);
+	hipMemcpy(x_device, x, size_x_device, hipMemcpyHostToDevice);
+	hipMemcpy(z_device, z, size_z_device, hipMemcpyHostToDevice);	
+
+	kernel_one_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_ONE));
+	kernel_two_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_TWO));   
+	kernel_three_blocks_per_grid=NA;
+	kernel_four_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_FOUR));
+	kernel_five_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_FIVE));
+	kernel_six_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_SIX));
+	kernel_seven_blocks_per_grid=(ceil((double)NA/THREADS_PER_BLOCK_ON_KERNEL_SEVEN));
+	kernel_eight_blocks_per_grid=NA;
+	kernel_nine_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_NINE));
+	kernel_ten_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_TEN));
+	kernel_eleven_blocks_per_grid=(ceil((double)NA/(double)THREADS_PER_BLOCK_ON_KERNEL_ELEVEN));
+
+	amount_of_share_data_on_kernel_one=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_ONE;
+	amount_of_share_data_on_kernel_two=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_TWO;
+	amount_of_share_data_on_kernel_three=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_THREE;
+	amount_of_share_data_on_kernel_four=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_FOUR;
+	amount_of_share_data_on_kernel_five=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_FIVE;
+	amount_of_share_data_on_kernel_six=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_SIX;
+	amount_of_share_data_on_kernel_seven=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_SEVEN;
+	amount_of_share_data_on_kernel_eight=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_EIGHT;
+	amount_of_share_data_on_kernel_nine=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_NINE;
+	amount_of_share_data_on_kernel_ten=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_TEN;
+	amount_of_share_data_on_kernel_eleven=sizeof(double)*THREADS_PER_BLOCK_ON_KERNEL_ELEVEN;
+
+	reduce_memory_on_kernel_one=kernel_one_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_two=kernel_two_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_three=kernel_three_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_four=kernel_four_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_five=kernel_five_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_six=kernel_six_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_seven=kernel_seven_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_eight=kernel_eight_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_nine=kernel_nine_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_ten=kernel_ten_blocks_per_grid*sizeof(double);
+	reduce_memory_on_kernel_eleven=kernel_eleven_blocks_per_grid*sizeof(double);
 }
 
 /*
diff --git a/CUDA/CG/setup.ini b/CUDA/CG/setup.ini
new file mode 100644
index 0000000..684e80d
--- /dev/null
+++ b/CUDA/CG/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/cg.S;../bin/cg.W;../bin/cg.A;../bin/cg.B
+use_clang_plugin = false
+llvm_pass = make -f Makefile.all llvm
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/EP/Makefile b/CUDA/EP/Makefile
index 7c96fa7..66dd32f 100644
--- a/CUDA/EP/Makefile
+++ b/CUDA/EP/Makefile
@@ -16,5 +16,5 @@ ep.o:		ep.cu npbparams.hpp
 	${CCOMPILE} ep.cu
 
 clean:
-	- rm -f *.o *~ 
+	- rm -f *.o *.bc seq.txt *~ 
 	- rm -f npbparams.hpp core
diff --git a/CUDA/EP/Makefile.all b/CUDA/EP/Makefile.all
new file mode 100644
index 0000000..b3c39fe
--- /dev/null
+++ b/CUDA/EP/Makefile.all
@@ -0,0 +1,23 @@
+SHELL=/bin/sh
+BENCHMARK=ep
+BENCHMARKU=EP
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+llvm:
+	$(MAKE) CLASS=S INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=W INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=A INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=B INJECT_CODE_LLVM=1
+	$(MAKE) clean 
diff --git a/CUDA/EP/ep.cu b/CUDA/EP/ep.cu
index b8194c2..89e9ee5 100644
--- a/CUDA/EP/ep.cu
+++ b/CUDA/EP/ep.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -58,7 +59,7 @@
  * ------------------------------------------------------------------------------
  */
 
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -71,35 +72,33 @@
 #define	A (1220703125.0)
 #define	S (271828183.0)
 #define NK_PLUS ((2*NK)+1)
-#define RECOMPUTATION (128)
-#define PROFILING_TOTAL_TIME (0)
+#define THREADS_PER_BLOCK (64)
+#define COALESCED_COMPUTATION (2*THREADS_PER_BLOCK)
+#define SHARE_MEMORY_Q (NQ*THREADS_PER_BLOCK)
+#define SHARE_MEMORY_SX (THREADS_PER_BLOCK)
+#define SHARE_MEMORY_SY (THREADS_PER_BLOCK)
 
 /* global variables */
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
+/* static double x[NK_PLUS]; */
 static double q[NQ];
 #else
+/* static double (*x)=(double*)malloc(sizeof(double)*(NK_PLUS)); */
 static double (*q)=(double*)malloc(sizeof(double)*(NQ));
 #endif
 /* gpu variables */
-double* q_host;
 double* q_device;
-double* sx_host;
 double* sx_device;
-double* sy_host;
 double* sy_device;
-int threads_per_block;
-int blocks_per_grid;
-size_t size_q;
-size_t size_sx;
-size_t size_sy;
-int gpu_device_id;
-int total_devices;
-cudaDeviceProp gpu_device_properties;
+size_t size_q_device;
+size_t size_sx_device;
+size_t size_sy_device;
 
 /* function declarations */
 __global__ void gpu_kernel(double* q_device, 
 		double* sx_device, 
 		double* sy_device,
+		int k_offset,
 		double an);
 __device__ double randlc_device(double* x, 
 		double a);
@@ -108,23 +107,28 @@ static void setup_gpu();
 __device__ void vranlc_device(int n, 
 		double* x_seed, 
 		double a, 
-		double* y);
+		double y[]);
 
 /* ep */
 int main(int argc, char** argv){
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
 	printf(" DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION mode on\n");
-#endif
-#if defined(PROFILING)
-	printf(" PROFILING mode on\n");
 #endif
 	double Mops, t1;
-	double sx, sy, tm, an, gc;
+	double sx, sy, tm, an, tt, gc;
 	double sx_verify_value, sy_verify_value, sx_err, sy_err;
-	int i, j, nit, block;
-	boolean verified;
+	int i, j, nit;
+	boolean verified, timers_enabled;
 	char size[16];
 
+	FILE* fp;
+	if((fp = fopen("timer.flag", "r"))==NULL){
+		timers_enabled = FALSE;
+	}else{
+		timers_enabled = TRUE;
+		fclose(fp);
+	}
+
 	/*
 	 * --------------------------------------------------------------------
 	 * because the size of the problem is too large to store in a 32-bit
@@ -140,7 +144,14 @@ int main(int argc, char** argv){
 	printf("\n\n NAS Parallel Benchmarks 4.1 CUDA C++ version - EP Benchmark\n\n");
 	printf(" Number of random numbers generated: %15s\n", size);
 
-	verified = FALSE;		
+	verified = FALSE;
+
+	setup_gpu();
+
+	timer_clear(0);
+	timer_clear(1);
+	timer_clear(2);
+	timer_start(0);
 
 	t1 = A;
 
@@ -149,6 +160,7 @@ int main(int argc, char** argv){
 	}
 
 	an = t1;
+	tt = S;
 	gc = 0.0;
 	sx = 0.0;
 	sy = 0.0;
@@ -157,34 +169,33 @@ int main(int argc, char** argv){
 		q[i] = 0.0;
 	}
 
-	setup_gpu();
-
-	timer_clear(PROFILING_TOTAL_TIME);
-	timer_start(PROFILING_TOTAL_TIME);
-
-	gpu_kernel<<<blocks_per_grid, 
-		threads_per_block>>>(q_device,
-				sx_device,
-				sy_device,
-				an);
+	gpu_kernel<<<ceil(double(NN)/double(THREADS_PER_BLOCK)), THREADS_PER_BLOCK>>>(q_device,
+			sx_device,
+			sy_device,
+			-1,
+			an);
 
-	timer_stop(PROFILING_TOTAL_TIME);
-	tm = timer_read(PROFILING_TOTAL_TIME);		
+	double (*q_host)[NQ]=(double(*)[NQ])malloc(size_q_device);
+	double (*sx_host)=(double*)malloc(size_sx_device);
+	double (*sy_host)=(double*)malloc(size_sy_device);	
 
-	cudaMemcpy(q_host, q_device, size_q, cudaMemcpyDeviceToHost);
-	cudaMemcpy(sx_host, sx_device, size_sx, cudaMemcpyDeviceToHost);
-	cudaMemcpy(sy_host, sy_device, size_sy, cudaMemcpyDeviceToHost);
+	hipMemcpy(q_host, q_device, size_q_device, hipMemcpyDeviceToHost);
+	hipMemcpy(sx_host, sx_device, size_sx_device, hipMemcpyDeviceToHost);
+	hipMemcpy(sy_host, sy_device, size_sy_device, hipMemcpyDeviceToHost);
 
-	for(block=0; block<blocks_per_grid; block++){
-		for(i=0; i<NQ; i++){
-			q[i]+=q_host[block*NQ+i];
+	for(i=0; i<NN/THREADS_PER_BLOCK; i++){
+		for(j=0; j<NQ; j++){
+			q[j]=q[j]+q_host[i][j];
 		}
-		sx+=sx_host[block];
-		sy+=sy_host[block];
+		sx+=sx_host[i];
+		sy+=sy_host[i];
 	}
 	for(i=0; i<NQ; i++){
 		gc+=q[i];
-	}				
+	}
+
+	timer_stop(0);
+	tm = timer_read(0);
 
 	nit = 0;
 	verified = TRUE;
@@ -225,24 +236,10 @@ int main(int argc, char** argv){
 	printf(" No. Gaussian Pairs = %15.0f\n", gc);
 	printf(" Sums = %25.15e %25.15e\n", sx, sy);
 	printf(" Counts: \n");
-	for(i=0; i<NQ; i++){
+	for(i=0; i<NQ-1; i++){
 		printf("%3d%15.0f\n", i, q[i]);
 	}
 
-	char gpu_config[256];
-	char gpu_config_string[2048];
-#if defined(PROFILING)
-	sprintf(gpu_config, "%5s\t%25s\t%25s\t%25s\n", "GPU Kernel", "Threads Per Block", "Time in Seconds", "Time in Percentage");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " ep", threads_per_block, timer_read(PROFILING_TOTAL_TIME), (timer_read(PROFILING_TOTAL_TIME)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-#else
-	sprintf(gpu_config, "%5s\t%25s\n", "GPU Kernel", "Threads Per Block");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " ep", threads_per_block);
-	strcat(gpu_config_string, gpu_config);
-#endif
-
 	c_print_results((char*)"EP",
 			CLASS,
 			M+1,
@@ -255,18 +252,23 @@ int main(int argc, char** argv){
 			verified,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
 			(char*)CS4,
 			(char*)CS5,
 			(char*)CS6,
-			(char*)CS7);	
+			(char*)CS7);
+
+	if(timers_enabled){
+		if(tm <= 0.0){tm = 1.0;}
+		tt = timer_read(0);
+		printf("\nTotal time:     %9.3f (%6.2f)\n", tt, tt*100.0/tm);
+		tt = timer_read(1);
+		printf("Gaussian pairs: %9.3f (%6.2f)\n", tt, tt*100.0/tm);
+		tt = timer_read(2);
+		printf("Random numbers: %9.3f (%6.2f)\n", tt, tt*100.0/tm);
+	}
 
 	release_gpu();
 
@@ -276,30 +278,25 @@ int main(int argc, char** argv){
 __global__ void gpu_kernel(double* q_global, 
 		double* sx_global, 
 		double* sy_global,
-		double an){	
-	double x_local[2*RECOMPUTATION];
-	double q_local[NQ]; 
-	double sx_local, sy_local;
+		int k_offset,
+		double an){
+	__shared__ double q_shared[SHARE_MEMORY_Q]; 
+	__shared__ double sx_shared[SHARE_MEMORY_SX];
+	__shared__ double sy_shared[SHARE_MEMORY_SY];
+	int    i, j, ii, ik, kk, l;
 	double t1, t2, t3, t4, x1, x2, seed;
-	int i, ii, ik, kk, l;
 
-	q_local[0]=0.0;
-	q_local[1]=0.0;
-	q_local[2]=0.0;
-	q_local[3]=0.0;
-	q_local[4]=0.0;
-	q_local[5]=0.0;
-	q_local[6]=0.0;
-	q_local[7]=0.0;
-	q_local[8]=0.0;
-	q_local[9]=0.0;
-	sx_local=0.0;
-	sy_local=0.0;	
+	double x[2*COALESCED_COMPUTATION];
 
-	kk=blockIdx.x*blockDim.x+threadIdx.x;
+	int k=blockIdx.x*blockDim.x+threadIdx.x;
 
-	if(kk>=NN){return;}
+	double sx_local = 0.0;
+	double sy_local = 0.0;
+	for(j=0; j<NQ; j++){
+		q_shared[j*blockDim.x + threadIdx.x] = 0.0;
+	}
 
+	kk=k_offset+k+1; 
 	t1=S;
 	t2=an;
 
@@ -312,44 +309,62 @@ __global__ void gpu_kernel(double* q_global,
 		kk=ik;
 	} 
 
-	seed=t1;
-	for(ii=0; ii<NK; ii=ii+RECOMPUTATION){
+	seed=t1;	
+	for(ii=0; ii<NK; ii=ii+COALESCED_COMPUTATION){
 		/* compute uniform pseudorandom numbers */
-		vranlc_device(2*RECOMPUTATION, &seed, A, x_local);
-
+		vranlc_device(2*COALESCED_COMPUTATION, &seed, A, x);
 		/*
 		 * compute gaussian deviates by acceptance-rejection method and
 		 * tally counts in concentric square annuli. this loop is not
 		 * vectorizable.
 		 */
-		for(i=0; i<RECOMPUTATION; i++){
-			x1=2.0*x_local[2*i]-1.0;
-			x2=2.0*x_local[2*i+1]-1.0;
+		for(i=0; i<COALESCED_COMPUTATION; i++){
+			x1=2.0*x[2*i]-1.0;
+			x2=2.0*x[2*i+1]-1.0;
 			t1=x1*x1+x2*x2;
 			if(t1<=1.0){
 				t2=sqrt(-2.0*log(t1)/t1);
 				t3=(x1*t2);
 				t4=(x2*t2);
 				l=max(fabs(t3), fabs(t4));
-				q_local[l]+=1.0;
+				q_shared[l*blockDim.x+threadIdx.x]+=1.0;
 				sx_local+=t3;
 				sy_local+=t4;
 			}
 		}
 	}
+	sx_shared[threadIdx.x]=sx_local;
+	sy_shared[threadIdx.x]=sy_local;
 
-	atomicAdd(q_global+blockIdx.x*NQ+0, q_local[0]); 
-	atomicAdd(q_global+blockIdx.x*NQ+1, q_local[1]); 
-	atomicAdd(q_global+blockIdx.x*NQ+2, q_local[2]); 
-	atomicAdd(q_global+blockIdx.x*NQ+3, q_local[3]); 
-	atomicAdd(q_global+blockIdx.x*NQ+4, q_local[4]); 
-	atomicAdd(q_global+blockIdx.x*NQ+5, q_local[5]); 
-	atomicAdd(q_global+blockIdx.x*NQ+6, q_local[6]); 
-	atomicAdd(q_global+blockIdx.x*NQ+7, q_local[7]); 
-	atomicAdd(q_global+blockIdx.x*NQ+8, q_local[8]);
-	atomicAdd(q_global+blockIdx.x*NQ+9, q_local[9]); 
-	atomicAdd(sx_global+blockIdx.x, sx_local); 
-	atomicAdd(sy_global+blockIdx.x, sy_local);
+	__syncthreads();
+
+	/* reduce on q_shared */
+	for(j=0; j<NQ; j++){
+		for(i=blockDim.x/2; i>0; i>>=1){
+			if(threadIdx.x<i){
+				q_shared[j*blockDim.x+threadIdx.x]+=q_shared[j*blockDim.x+threadIdx.x+i];	        
+			}
+			__syncthreads();
+		}
+	}
+
+	/* reduce on sx_shared and sx_shared */
+	for(i=blockDim.x/2; i>0; i>>=1){
+		if(threadIdx.x<i){
+			sx_shared[threadIdx.x]+=sx_shared[threadIdx.x+i];
+			sy_shared[threadIdx.x]+=sy_shared[threadIdx.x+i];
+		}
+		__syncthreads();
+	}
+
+	/* put the results on the global memory */
+	if(threadIdx.x==0){
+		for(j=0; j<NQ; j++){
+			q_global[blockIdx.x*NQ+j]=q_shared[j*blockDim.x];
+		}
+		sx_global[blockIdx.x]=sx_shared[0];
+		sy_global[blockIdx.x]=sy_shared[0];
+	}
 }
 
 __device__ double randlc_device(double* x, 
@@ -371,85 +386,25 @@ __device__ double randlc_device(double* x,
 }
 
 static void release_gpu(){
-	cudaFree(q_device);
-	cudaFree(sx_device);
-	cudaFree(sy_device);
+	hipFree(q_device);
+	hipFree(sx_device);
+	hipFree(sy_device);
 }
 
 static void setup_gpu(){
-	/*
-	 * struct cudaDeviceProp{
-	 *  char name[256];
-	 *  size_t totalGlobalMem;
-	 *  size_t sharedMemPerBlock;
-	 *  int regsPerBlock;
-	 *  int warpSize;
-	 *  size_t memPitch;
-	 *  int maxThreadsPerBlock;
-	 *  int maxThreadsDim[3];
-	 *  int maxGridSize[3];
-	 *  size_t totalConstMem;
-	 *  int major;
-	 *  int minor;
-	 *  int clockRate;
-	 *  size_t textureAlignment;
-	 *  int deviceOverlap;
-	 *  int multiProcessorCount;
-	 *  int kernelExecTimeoutEnabled;
-	 *  int integrated;
-	 *  int canMapHostMemory;
-	 *  int computeMode;
-	 *  int concurrentKernels;
-	 *  int ECCEnabled;
-	 *  int pciBusID;
-	 *  int pciDeviceID;
-	 *  int tccDriver;
-	 * }
-	 */
-	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);	
-
-	/* define gpu_device */
-	if(total_devices==0){
-		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
-		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
-	}else{
-		gpu_device_id = 0;
-	}
-
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((EP_THREADS_PER_BLOCK>=1)&&
-			(EP_THREADS_PER_BLOCK<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block = EP_THREADS_PER_BLOCK;
-	}else{
-		threads_per_block = gpu_device_properties.warpSize;
-	}	
-
-	blocks_per_grid = (ceil((double)NN/(double)threads_per_block));
-
-	size_q = blocks_per_grid * NQ * sizeof(double);
-	size_sx = blocks_per_grid * sizeof(double);
-	size_sy = blocks_per_grid * sizeof(double);
-
-	q_host=(double*)malloc(size_q);	
-	sx_host=(double*)malloc(size_sx);
-	sy_host=(double*)malloc(size_sy);
+	size_q_device=NN/THREADS_PER_BLOCK*NQ*sizeof(double);
+	size_sx_device=NN/THREADS_PER_BLOCK*sizeof(double);
+	size_sy_device=NN/THREADS_PER_BLOCK*sizeof(double);
 
-	cudaMalloc(&q_device, size_q);
-	cudaMalloc(&sx_device, size_sx);
-	cudaMalloc(&sy_device, size_sy);
+	hipMalloc(&q_device, size_q_device);
+	hipMalloc(&sx_device, size_sx_device);
+	hipMalloc(&sy_device, size_sy_device);
 }
 
 __device__ void vranlc_device(int n, 
 		double* x_seed, 
 		double a, 
-		double* y){
+		double y[]){
 	int i;
 	double x,t1,t2,t3,t4,a1,a2,x1,x2,z;
 	t1 = R23 * a;
diff --git a/CUDA/EP/setup.ini b/CUDA/EP/setup.ini
new file mode 100644
index 0000000..7f86e68
--- /dev/null
+++ b/CUDA/EP/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/ep.S;../bin/ep.W;../bin/ep.A;../bin/ep.B
+use_clang_plugin = false
+llvm_pass = make -f Makefile.all llvm
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/FT/Makefile b/CUDA/FT/Makefile
index 2b4be48..b788ce5 100644
--- a/CUDA/FT/Makefile
+++ b/CUDA/FT/Makefile
@@ -10,11 +10,11 @@ OBJS = ft.o ${COMMON}/c_${RAND}.o ${COMMON}/c_print_results.o \
 include ../sys/make.common
 
 ${PROGRAM}: config ${OBJS}
-	${CLINK} ${CLINKFLAGS} -o ${PROGRAM} ${OBJS} ${C_LIB}
+	${CLINK} ${CLINKFLAGS} -o ${PROGRAM} ${OBJS} ${C_LIB} -fopenmp
 
 ft.o:             ft.cu npbparams.hpp
 	${CCOMPILE} ft.cu
 
 clean:
-	- rm -f *.o *~ mputil*
+	- rm -f *.o *.bc seq.txt *~ mputil*
 	- rm -f ft npbparams.hpp core
diff --git a/CUDA/FT/Makefile.all b/CUDA/FT/Makefile.all
new file mode 100644
index 0000000..04edd05
--- /dev/null
+++ b/CUDA/FT/Makefile.all
@@ -0,0 +1,23 @@
+SHELL=/bin/sh
+BENCHMARK=ft
+BENCHMARKU=FT
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+llvm:
+	$(MAKE) CLASS=S INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=W INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=A INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=B INJECT_CODE_LLVM=1
+	$(MAKE) clean 
diff --git a/CUDA/FT/ft.cu b/CUDA/FT/ft.cu
index 0f80c12..27d248d 100644
--- a/CUDA/FT/ft.cu
+++ b/CUDA/FT/ft.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -58,7 +59,7 @@
  */
 
 #include <omp.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -130,27 +131,22 @@
 #define	PI (3.141592653589793238)
 #define	ALPHA (1.0e-6)
 #define AP (-4.0*ALPHA*PI*PI)
-#define OMP_THREADS (3)
-#define TASK_INDEXMAP (0)
-#define TASK_INITIAL_CONDITIONS (1)
-#define TASK_INIT_UI (2)
-#define PROFILING_TOTAL_TIME (0)
-#define PROFILING_INDEXMAP (1)
-#define PROFILING_INITIAL_CONDITIONS (2)
-#define PROFILING_INIT_UI (3)
-#define PROFILING_EVOLVE (4)
-#define PROFILING_FFTX_1 (5)
-#define PROFILING_FFTX_2 (6)
-#define PROFILING_FFTX_3 (7)
-#define PROFILING_FFTY_1 (8)
-#define PROFILING_FFTY_2 (9)
-#define PROFILING_FFTY_3 (10)
-#define PROFILING_FFTZ_1 (11)
-#define PROFILING_FFTZ_2 (12)
-#define PROFILING_FFTZ_3 (13)
-#define PROFILING_CHECKSUM (14)
-#define PROFILING_INIT (15)
+#define	T_TOTAL (1)
+#define	T_SETUP (2)
+#define	T_FFT (3)
+#define	T_EVOLVE (4)
+#define	T_CHECKSUM (5)
+#define	T_FFTX (6)
+#define	T_FFTY (7)
+#define	T_FFTZ (8)
+#define T_MAX (8)
 #define CHECKSUM_TASKS (1024)
+#define THREADS_PER_BLOCK_AT_CHECKSUM (128)
+#define DEFAULT_GPU (0)
+#define OMP_THREADS (3)
+#define COMPUTE_INDEXMAP (0)
+#define COMPUTE_INITIAL_CONDITIONS (1)
+#define COMPUTE_FFT_INIT (2)
 
 /* global variables */
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
@@ -169,10 +165,18 @@ static dcomplex (*u1)=(dcomplex*)malloc(sizeof(dcomplex)*(NTOTAL));
 static int (*dims)=(int*)malloc(sizeof(int)*(3));
 #endif
 static int niter;
+static boolean timers_enabled;
 /* gpu variables */
+int THREADS_PER_BLOCK_AT_COMPUTE_INDEXMAP;
+int THREADS_PER_BLOCK_AT_COMPUTE_INITIAL_CONDITIONS;
+int THREADS_PER_BLOCK_AT_INIT_UI;
+int THREADS_PER_BLOCK_AT_EVOLVE;
+int THREADS_PER_BLOCK_AT_FFT1;
+int THREADS_PER_BLOCK_AT_FFT2;
+int THREADS_PER_BLOCK_AT_FFT3;
+dcomplex* sums_device;
 double* starts_device;
 double* twiddle_device;
-dcomplex* sums_device;
 dcomplex* u_device;
 dcomplex* u0_device;
 dcomplex* u1_device;
@@ -187,39 +191,6 @@ size_t size_u0_device;
 size_t size_u1_device;
 size_t size_y0_device;
 size_t size_y1_device;
-size_t size_shared_data;
-int blocks_per_grid_on_compute_indexmap;
-int blocks_per_grid_on_compute_initial_conditions;
-int blocks_per_grid_on_init_ui;
-int blocks_per_grid_on_evolve;
-int blocks_per_grid_on_fftx_1;
-int blocks_per_grid_on_fftx_2;
-int blocks_per_grid_on_fftx_3;
-int blocks_per_grid_on_ffty_1;
-int blocks_per_grid_on_ffty_2;
-int blocks_per_grid_on_ffty_3;
-int blocks_per_grid_on_fftz_1;
-int blocks_per_grid_on_fftz_2;
-int blocks_per_grid_on_fftz_3;
-int blocks_per_grid_on_checksum;
-int threads_per_block_on_compute_indexmap;
-int threads_per_block_on_compute_initial_conditions;
-int threads_per_block_on_init_ui;
-int threads_per_block_on_evolve;
-int threads_per_block_on_fftx_1;
-int threads_per_block_on_fftx_2;
-int threads_per_block_on_fftx_3;
-int threads_per_block_on_ffty_1;
-int threads_per_block_on_ffty_2;
-int threads_per_block_on_ffty_3;
-int threads_per_block_on_fftz_1;
-int threads_per_block_on_fftz_2;
-int threads_per_block_on_fftz_3;
-int threads_per_block_on_checksum;
-int gpu_device_id;
-int total_devices;
-cudaDeviceProp gpu_device_properties;
-extern __shared__ double extern_share_data[];
 
 /* function declarations */
 static void cffts1_gpu(const int is, 
@@ -315,6 +286,7 @@ static void ipow46(double a,
 __device__ void ipow46_device(double a, 
 		int exponent, 
 		double* result);
+static void print_timers();
 __device__ double randlc_device(double* x, 
 		double a);
 static void release_gpu();
@@ -336,9 +308,7 @@ int main(int argc, char** argv){
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
 	printf(" DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION mode on\n");
 #endif
-#if defined(PROFILING)
-	printf(" PROFILING mode on\n");
-#endif
+	int i;
 	int iter=0;
 	double total_time, mflops;
 	boolean verified;
@@ -350,20 +320,23 @@ int main(int argc, char** argv){
 	 * this reduces variable startup costs, which is important for such a 
 	 * short benchmark. the other NPB 2 implementations are similar. 
 	 * ---------------------------------------------------------------------
-	 */	
+	 */
+	for(i=0; i<T_MAX; i++){
+		timer_clear(i);
+	}
 	setup();
 	setup_gpu();
 	init_ui_gpu(u0_device, u1_device, twiddle_device);
-#pragma omp parallel
+	#pragma omp parallel
 	{
-		if(omp_get_thread_num()==TASK_INDEXMAP){
+		if(omp_get_thread_num()==COMPUTE_INDEXMAP){
 			compute_indexmap_gpu(twiddle_device);
-		}else if(omp_get_thread_num()==TASK_INITIAL_CONDITIONS){
+		}else if(omp_get_thread_num()==COMPUTE_INITIAL_CONDITIONS){
 			compute_initial_conditions_gpu(u1_device);
-		}else if(omp_get_thread_num()==TASK_INIT_UI){
+		}else if(omp_get_thread_num()==COMPUTE_FFT_INIT){
 			fft_init_gpu(MAXDIM);
 		}		
-	}cudaDeviceSynchronize();
+	}hipDeviceSynchronize();
 	fft_gpu(1, u1_device, u0_device);
 
 	/*
@@ -372,51 +345,47 @@ int main(int argc, char** argv){
 	 * be timed, in contrast to other benchmarks. 
 	 * ---------------------------------------------------------------------
 	 */
-	timer_clear(PROFILING_TOTAL_TIME);
-#if defined(PROFILING)
-	timer_clear(PROFILING_INDEXMAP);
-	timer_clear(PROFILING_INITIAL_CONDITIONS);
-	timer_clear(PROFILING_INITIAL_CONDITIONS);
-	timer_clear(PROFILING_EVOLVE);
-	timer_clear(PROFILING_FFTX_1);
-	timer_clear(PROFILING_FFTX_2);
-	timer_clear(PROFILING_FFTX_3);
-	timer_clear(PROFILING_FFTY_1);
-	timer_clear(PROFILING_FFTY_2);
-	timer_clear(PROFILING_FFTY_3);
-	timer_clear(PROFILING_FFTZ_1);
-	timer_clear(PROFILING_FFTZ_2);
-	timer_clear(PROFILING_FFTZ_3);
-	timer_clear(PROFILING_CHECKSUM);
-#endif
+	for(i=0; i<T_MAX; i++){
+		timer_clear(i);
+	}
 
-	timer_start(PROFILING_TOTAL_TIME);
-#pragma omp parallel
+	timer_start(T_TOTAL);
+	if(timers_enabled==TRUE){timer_start(T_SETUP);}
+	#pragma omp parallel
 	{
-		if(omp_get_thread_num()==TASK_INDEXMAP){
+		if(omp_get_thread_num()==COMPUTE_INDEXMAP){
 			compute_indexmap_gpu(twiddle_device);
-		}else if(omp_get_thread_num()==TASK_INITIAL_CONDITIONS){
+		}else if(omp_get_thread_num()==COMPUTE_INITIAL_CONDITIONS){
 			compute_initial_conditions_gpu(u1_device);
-		}else if(omp_get_thread_num()==TASK_INIT_UI){
+		}else if(omp_get_thread_num()==COMPUTE_FFT_INIT){
 			fft_init_gpu(MAXDIM);
 		}		
-	}cudaDeviceSynchronize();
+	}hipDeviceSynchronize();
+	if(timers_enabled==TRUE){timer_stop(T_SETUP);}
+	if(timers_enabled==TRUE){timer_start(T_FFT);}
 	fft_gpu(1, u1_device, u0_device);
+	if(timers_enabled==TRUE){timer_stop(T_FFT);}
 	for(iter=1; iter<=niter; iter++){
+		if(timers_enabled==TRUE){timer_start(T_EVOLVE);}
 		evolve_gpu(u0_device, u1_device, twiddle_device);
+		if(timers_enabled==TRUE){timer_stop(T_EVOLVE);}
+		if(timers_enabled==TRUE){timer_start(T_FFT);}
 		fft_gpu(-1, u1_device, u1_device);
+		if(timers_enabled==TRUE){timer_stop(T_FFT);}
+		if(timers_enabled==TRUE){timer_start(T_CHECKSUM);}
 		checksum_gpu(iter, u1_device);
+		if(timers_enabled==TRUE){timer_stop(T_CHECKSUM);}
 	}
 
-	cudaMemcpy(sums, sums_device, size_sums_device, cudaMemcpyDeviceToHost);
+	hipMemcpy(sums, sums_device, size_sums_device, hipMemcpyDeviceToHost);
 	for(iter=1; iter<=niter; iter++){
 		printf("T = %5d     Checksum = %22.12e %22.12e\n", iter, sums[iter].real, sums[iter].imag);
 	}		
 
 	verify(NX, NY, NZ, niter, &verified, &class_npb);
 
-	timer_stop(PROFILING_TOTAL_TIME);
-	total_time = timer_read(PROFILING_TOTAL_TIME);		
+	timer_stop(T_TOTAL);
+	total_time = timer_read(T_TOTAL);		
 
 	if(total_time != 0.0){
 		mflops = 1.0e-6 * ((double)(NTOTAL)) *
@@ -426,73 +395,6 @@ int main(int argc, char** argv){
 	}else{
 		mflops = 0.0;
 	}
-
-	char gpu_config[256];
-	char gpu_config_string[2048];
-#if defined(PROFILING)
-	sprintf(gpu_config, "%5s\t%25s\t%25s\t%25s\n", "GPU Kernel", "Threads Per Block", "Time in Seconds", "Time in Percentage");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " indexmap", threads_per_block_on_compute_indexmap, timer_read(PROFILING_INDEXMAP), (timer_read(PROFILING_INDEXMAP)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " initial conditions", threads_per_block_on_compute_initial_conditions, timer_read(PROFILING_INITIAL_CONDITIONS), (timer_read(PROFILING_INITIAL_CONDITIONS)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " init ui", threads_per_block_on_init_ui, timer_read(PROFILING_INIT_UI), (timer_read(PROFILING_INIT_UI)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " evolve", threads_per_block_on_evolve, timer_read(PROFILING_EVOLVE), (timer_read(PROFILING_EVOLVE)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " fftx 1", threads_per_block_on_fftx_1, timer_read(PROFILING_FFTX_1), (timer_read(PROFILING_FFTX_1)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " fftx 2", threads_per_block_on_fftx_2, timer_read(PROFILING_FFTX_2), (timer_read(PROFILING_FFTX_2)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " fftx 3", threads_per_block_on_fftx_3, timer_read(PROFILING_FFTX_3), (timer_read(PROFILING_FFTX_3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " ffty 1", threads_per_block_on_ffty_1, timer_read(PROFILING_FFTY_1), (timer_read(PROFILING_FFTY_1)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " ffty 2", threads_per_block_on_ffty_2, timer_read(PROFILING_FFTY_2), (timer_read(PROFILING_FFTY_2)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " ffty 3", threads_per_block_on_ffty_3, timer_read(PROFILING_FFTY_3), (timer_read(PROFILING_FFTY_3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " fftz 1", threads_per_block_on_fftz_1, timer_read(PROFILING_FFTZ_1), (timer_read(PROFILING_FFTZ_1)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " fftz 2", threads_per_block_on_fftz_2, timer_read(PROFILING_FFTZ_2), (timer_read(PROFILING_FFTZ_2)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " fftz 3", threads_per_block_on_fftz_3, timer_read(PROFILING_FFTZ_3), (timer_read(PROFILING_FFTZ_3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " checksum", threads_per_block_on_checksum, timer_read(PROFILING_CHECKSUM), (timer_read(PROFILING_CHECKSUM)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-#else
-	sprintf(gpu_config, "%5s\t%25s\n", "GPU Kernel", "Threads Per Block");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " indexmap", threads_per_block_on_compute_indexmap);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " initial conditions", threads_per_block_on_compute_initial_conditions);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " init ui", threads_per_block_on_init_ui);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " evolve", threads_per_block_on_evolve);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " fftx 1", threads_per_block_on_fftx_1);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " fftx 2", threads_per_block_on_fftx_2);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " fftx 3", threads_per_block_on_fftx_3);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " ffty 1", threads_per_block_on_ffty_1);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " ffty 2", threads_per_block_on_ffty_2);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " ffty 3", threads_per_block_on_ffty_3);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " fftz 1", threads_per_block_on_fftz_1);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " fftz 2", threads_per_block_on_fftz_2);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " fftz 3", threads_per_block_on_fftz_3);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " checksum", threads_per_block_on_checksum);
-	strcat(gpu_config_string, gpu_config);
-#endif
-
 	c_print_results((char*)"FT", 
 			class_npb, 
 			NX, 
@@ -504,19 +406,15 @@ int main(int argc, char** argv){
 			(char*)"          floating point", 
 			verified, 
 			(char*)NPBVERSION, 
-			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			(char*)gpu_config_string,
+			(char*)COMPILETIME, 
 			(char*)CS1, 
 			(char*)CS2, 
 			(char*)CS3, 
 			(char*)CS4, 
 			(char*)CS5, 
 			(char*)CS6, 
-			(char*)CS7);	
+			(char*)CS7);
+	if(timers_enabled==TRUE){print_timers();}
 
 	release_gpu();
 
@@ -529,40 +427,25 @@ static void cffts1_gpu(const int is,
 		dcomplex x_out[], 
 		dcomplex y0[], 
 		dcomplex y1[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTX_1);
-#endif
-	cffts1_gpu_kernel_1<<<blocks_per_grid_on_fftx_1,
-		threads_per_block_on_fftx_1>>>(x_in, 
-				y0);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTX_1);
-#endif
-
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTX_2);
-#endif
-	cffts1_gpu_kernel_2<<<blocks_per_grid_on_fftx_2,
-		threads_per_block_on_fftx_2>>>(is, 
-				y0, 
-				y1, 
-				u);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTX_2);
-#endif
-
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTX_3);
-#endif
-	cffts1_gpu_kernel_3<<<blocks_per_grid_on_fftx_3,
-		threads_per_block_on_fftx_3>>>(x_out, 
-				y0);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTX_3);
-#endif
+	if(timers_enabled){timer_start(T_FFTX);}
+
+	int blocks_per_grid_kernel_1=ceil(double(NX*NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT1));
+	int blocks_per_grid_kernel_2=ceil(double(NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT1));
+	int blocks_per_grid_kernel_3=ceil(double(NX*NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT1));
+
+	cffts1_gpu_kernel_1<<<blocks_per_grid_kernel_1, THREADS_PER_BLOCK_AT_FFT1>>>(x_in, 
+			y0);
+	hipDeviceSynchronize();
+	cffts1_gpu_kernel_2<<<blocks_per_grid_kernel_2, THREADS_PER_BLOCK_AT_FFT1>>>(is, 
+			y0, 
+			y1, 
+			u);
+	hipDeviceSynchronize();
+	cffts1_gpu_kernel_3<<<blocks_per_grid_kernel_3, THREADS_PER_BLOCK_AT_FFT1>>>(x_out, 
+			y0);
+	hipDeviceSynchronize();
+
+	if(timers_enabled){timer_stop(T_FFTX);}
 }
 
 /*
@@ -724,40 +607,25 @@ static void cffts2_gpu(int is,
 		dcomplex x_out[], 
 		dcomplex y0[], 
 		dcomplex y1[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTY_1);
-#endif
-	cffts2_gpu_kernel_1<<<blocks_per_grid_on_ffty_1,
-		threads_per_block_on_ffty_1>>>(x_in, 
-				y0);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTY_1);
-#endif
-
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTY_2);
-#endif
-	cffts2_gpu_kernel_2<<<blocks_per_grid_on_ffty_2,
-		threads_per_block_on_ffty_2>>>(is, 
-				y0, 
-				y1, 
-				u);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTY_2);
-#endif
-
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTY_3);
-#endif
-	cffts2_gpu_kernel_3<<<blocks_per_grid_on_ffty_3,
-		threads_per_block_on_ffty_3>>>(x_out, 
-				y0);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTY_3);
-#endif
+	if(timers_enabled){timer_start(T_FFTY);}
+
+	int blocks_per_grid_kernel_1=ceil(double(NX*NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT2));
+	int blocks_per_grid_kernel_2=ceil(double(NX*NZ)/double(THREADS_PER_BLOCK_AT_FFT2));
+	int blocks_per_grid_kernel_3=ceil(double(NX*NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT2));
+
+	cffts2_gpu_kernel_1<<<blocks_per_grid_kernel_1, THREADS_PER_BLOCK_AT_FFT2>>>(x_in, 
+			y0);
+	hipDeviceSynchronize();
+	cffts2_gpu_kernel_2<<<blocks_per_grid_kernel_2, THREADS_PER_BLOCK_AT_FFT2>>>(is, 
+			y0, 
+			y1, 
+			u);
+	hipDeviceSynchronize();
+	cffts2_gpu_kernel_3<<<blocks_per_grid_kernel_3, THREADS_PER_BLOCK_AT_FFT2>>>(x_out, 
+			y0);
+	hipDeviceSynchronize();
+
+	if(timers_enabled){timer_stop(T_FFTY);}
 }
 
 /*
@@ -915,40 +783,25 @@ static void cffts3_gpu(int is,
 		dcomplex x_out[], 
 		dcomplex y0[], 
 		dcomplex y1[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTZ_1);
-#endif
-	cffts3_gpu_kernel_1<<<blocks_per_grid_on_fftz_1,
-		threads_per_block_on_fftz_1>>>(x_in, 
-				y0);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTZ_1);
-#endif
-
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTZ_2);
-#endif
-	cffts3_gpu_kernel_2<<<blocks_per_grid_on_fftz_2,
-		threads_per_block_on_fftz_2>>>(is, 
-				y0, 
-				y1, 
-				u);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTZ_2);
-#endif
-
-#if defined(PROFILING)
-	timer_start(PROFILING_FFTZ_3);
-#endif
-	cffts3_gpu_kernel_3<<<blocks_per_grid_on_fftz_3,
-		threads_per_block_on_fftz_3>>>(x_out, 
-				y0);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_FFTZ_3);
-#endif	
+	if(timers_enabled){timer_start(T_FFTZ);}
+
+	int blocks_per_grid_kernel_1=ceil(double(NX*NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT3));
+	int blocks_per_grid_kernel_2=ceil(double(NX*NY)/double(THREADS_PER_BLOCK_AT_FFT3));
+	int blocks_per_grid_kernel_3=ceil(double(NX*NY*NZ)/double(THREADS_PER_BLOCK_AT_FFT3));
+
+	cffts3_gpu_kernel_1<<<blocks_per_grid_kernel_1, THREADS_PER_BLOCK_AT_FFT3>>>(x_in, 
+			y0);
+	hipDeviceSynchronize();
+	cffts3_gpu_kernel_2<<<blocks_per_grid_kernel_2, THREADS_PER_BLOCK_AT_FFT3>>>(is, 
+			y0, 
+			y1, 
+			u);
+	hipDeviceSynchronize();
+	cffts3_gpu_kernel_3<<<blocks_per_grid_kernel_3, THREADS_PER_BLOCK_AT_FFT3>>>(x_out, 
+			y0);
+	hipDeviceSynchronize();
+
+	if(timers_enabled){timer_stop(T_FFTZ);}
 }
 
 /*
@@ -1108,23 +961,18 @@ __global__ void cffts3_gpu_kernel_3(dcomplex x_out[],
 
 static void checksum_gpu(int iteration,
 		dcomplex u1[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_CHECKSUM);
-#endif
-	checksum_gpu_kernel<<<blocks_per_grid_on_checksum,
-		threads_per_block_on_checksum,
-		size_shared_data>>>(iteration, 
-				u1, 
-				sums_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_CHECKSUM);
-#endif
+	int blocks_per_grid=ceil(double(CHECKSUM_TASKS)/double(THREADS_PER_BLOCK_AT_CHECKSUM));
+
+	checksum_gpu_kernel<<<blocks_per_grid, THREADS_PER_BLOCK_AT_CHECKSUM>>>(iteration, 
+			u1, 
+			sums_device);
+	hipDeviceSynchronize();
 }
 
 __global__ void checksum_gpu_kernel(int iteration, 
 		dcomplex u1[], 
 		dcomplex sums[]){
-	dcomplex* share_sums = (dcomplex*)(extern_share_data);
+	__shared__ dcomplex share_sums[THREADS_PER_BLOCK_AT_CHECKSUM];
 	int j = (blockIdx.x * blockDim.x + threadIdx.x) + 1;
 	int q, r, s;
 
@@ -1153,14 +1001,9 @@ __global__ void checksum_gpu_kernel(int iteration,
 }
 
 static void compute_indexmap_gpu(double twiddle[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_INDEXMAP);
-#endif
-	compute_indexmap_gpu_kernel<<<blocks_per_grid_on_compute_indexmap,
-		threads_per_block_on_compute_indexmap>>>(twiddle);
-#if defined(PROFILING)
-	timer_stop(PROFILING_INDEXMAP);
-#endif
+	int blocks_per_grid=ceil(double(NTOTAL)/double(THREADS_PER_BLOCK_AT_COMPUTE_INDEXMAP));
+
+	compute_indexmap_gpu_kernel<<<blocks_per_grid, THREADS_PER_BLOCK_AT_COMPUTE_INDEXMAP>>>(twiddle);
 }
 
 __global__ void compute_indexmap_gpu_kernel(double twiddle[]){
@@ -1185,10 +1028,7 @@ __global__ void compute_indexmap_gpu_kernel(double twiddle[]){
 	twiddle[thread_id] = exp(AP*(double)(ii*ii+kj2));
 }
 
-static void compute_initial_conditions_gpu(dcomplex u0[]){  
-#if defined(PROFILING)
-	timer_start(PROFILING_INITIAL_CONDITIONS);
-#endif  
+static void compute_initial_conditions_gpu(dcomplex u0[]){    
 	int z;
 	double start, an, starts[NZ];
 
@@ -1204,14 +1044,12 @@ static void compute_initial_conditions_gpu(dcomplex u0[]){
 		starts[z] = start;
 	}
 
-	cudaMemcpy(starts_device, starts, size_starts_device, cudaMemcpyHostToDevice);
+	hipMemcpy(starts_device, starts, size_starts_device, hipMemcpyHostToDevice);
+
+	int blocks_per_grid=ceil(double(NZ)/double(THREADS_PER_BLOCK_AT_COMPUTE_INITIAL_CONDITIONS));
 
-	compute_initial_conditions_gpu_kernel<<<blocks_per_grid_on_compute_initial_conditions,
-		threads_per_block_on_compute_initial_conditions>>>(u0, 
-				starts_device);
-#if defined(PROFILING)
-	timer_stop(PROFILING_INITIAL_CONDITIONS);
-#endif  
+	compute_initial_conditions_gpu_kernel<<<blocks_per_grid, THREADS_PER_BLOCK_AT_COMPUTE_INITIAL_CONDITIONS>>>(u0, 
+			starts_device);
 }
 
 __global__ void compute_initial_conditions_gpu_kernel(dcomplex u0[], 
@@ -1229,17 +1067,12 @@ __global__ void compute_initial_conditions_gpu_kernel(dcomplex u0[],
 static void evolve_gpu(dcomplex u0[], 
 		dcomplex u1[],
 		double twiddle[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_EVOLVE);
-#endif  
-	evolve_gpu_kernel<<<blocks_per_grid_on_evolve,
-		threads_per_block_on_evolve>>>(u0, 
-				u1,
-				twiddle);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_EVOLVE);
-#endif  
+	int blocks_per_grid=ceil(double(NTOTAL)/double(THREADS_PER_BLOCK_AT_EVOLVE));
+
+	evolve_gpu_kernel<<<blocks_per_grid, THREADS_PER_BLOCK_AT_EVOLVE>>>(u0, 
+			u1,
+			twiddle);
+	hipDeviceSynchronize();
 }
 
 __global__ void evolve_gpu_kernel(dcomplex u0[], 
@@ -1278,9 +1111,6 @@ static void fft_gpu(int dir,
 }
 
 static void fft_init_gpu(int n){
-#if defined(PROFILING)
-	timer_start(PROFILING_INIT);
-#endif  
 	int m,ku,i,j,ln;
 	double t, ti;
 	/*
@@ -1302,10 +1132,7 @@ static void fft_init_gpu(int n){
 		ku = ku + ln;
 		ln = 2 * ln;
 	}
-	cudaMemcpy(u_device, u, size_u_device, cudaMemcpyHostToDevice);
-#if defined(PROFILING)
-	timer_stop(PROFILING_INIT);
-#endif 
+	hipMemcpy(u_device, u, size_u_device, hipMemcpyHostToDevice);
 }
 
 static int ilog2(int n){
@@ -1339,17 +1166,12 @@ __device__ int ilog2_device(int n){
 static void init_ui_gpu(dcomplex u0[],
 		dcomplex u1[],
 		double twiddle[]){
-#if defined(PROFILING)
-	timer_start(PROFILING_INIT_UI);
-#endif  
-	init_ui_gpu_kernel<<<blocks_per_grid_on_init_ui,
-		threads_per_block_on_init_ui>>>(u0, 
-				u1,
-				twiddle);
-	cudaDeviceSynchronize();
-#if defined(PROFILING)
-	timer_stop(PROFILING_INIT_UI);
-#endif  
+	int blocks_per_grid=ceil(double(NTOTAL)/double(THREADS_PER_BLOCK_AT_INIT_UI));
+
+	init_ui_gpu_kernel<<<blocks_per_grid, THREADS_PER_BLOCK_AT_EVOLVE>>>(u0, 
+			u1,
+			twiddle);
+	hipDeviceSynchronize();
 }
 
 __global__ void init_ui_gpu_kernel(dcomplex u0[],
@@ -1428,6 +1250,28 @@ __device__ void ipow46_device(double a,
 	*result = r;
 }
 
+static void print_timers(){
+	int i;
+	double t, t_m;
+	char* tstrings[T_MAX+1];
+	tstrings[1] = (char*)"          total "; 
+	tstrings[2] = (char*)"          setup "; 
+	tstrings[3] = (char*)"            fft "; 
+	tstrings[4] = (char*)"         evolve "; 
+	tstrings[5] = (char*)"       checksum "; 
+	tstrings[6] = (char*)"           fftx "; 
+	tstrings[7] = (char*)"           ffty "; 
+	tstrings[8] = (char*)"           fftz ";
+
+	t_m = timer_read(T_TOTAL);
+	if(t_m <= 0.0){t_m = 1.00;}
+	for(i = 1; i <= T_MAX; i++){
+		t = timer_read(i);
+		printf(" timer %2d(%16s) :%9.4f (%6.2f%%)\n", 
+				i, tstrings[i], t, t*100.0/t_m);
+	}
+}
+
 __device__ double randlc_device(double* x, 
 		double a){
 	double t1,t2,t3,t4,a1,a2,x1,x2,z;
@@ -1447,17 +1291,26 @@ __device__ double randlc_device(double* x,
 }
 
 static void release_gpu(){
-	cudaFree(sums_device);
-	cudaFree(starts_device);
-	cudaFree(twiddle_device);
-	cudaFree(u_device);
-	cudaFree(u0_device);
-	cudaFree(u1_device);
-	cudaFree(y0_device);
-	cudaFree(y1_device);
+	hipFree(sums_device);
+	hipFree(starts_device);
+	hipFree(twiddle_device);
+	hipFree(u_device);
+	hipFree(u0_device);
+	hipFree(u1_device);
+	hipFree(y0_device);
+	hipFree(y1_device);
 }
 
 static void setup(){
+	FILE* fp;
+
+	if((fp = fopen("timer.flag", "r")) != NULL){
+		timers_enabled = TRUE;
+		fclose(fp);
+	}else{
+		timers_enabled = FALSE;
+	}
+
 	niter = NITER_DEFAULT;
 
 	printf("\n\n NAS Parallel Benchmarks 4.1 CUDA C++ version - FT Benchmark\n\n");
@@ -1467,172 +1320,37 @@ static void setup(){
 }
 
 static void setup_gpu(){
-	/*
-	 * struct cudaDeviceProp{
-	 *  char name[256];
-	 *  size_t totalGlobalMem;
-	 *  size_t sharedMemPerBlock;
-	 *  int regsPerBlock;
-	 *  int warpSize;
-	 *  size_t memPitch;
-	 *  int maxThreadsPerBlock;
-	 *  int maxThreadsDim[3];
-	 *  int maxGridSize[3];
-	 *  size_t totalConstMem;
-	 *  int major;
-	 *  int minor;
-	 *  int clockRate;
-	 *  size_t textureAlignment;
-	 *  int deviceOverlap;
-	 *  int multiProcessorCount;
-	 *  int kernelExecTimeoutEnabled;
-	 *  int integrated;
-	 *  int canMapHostMemory;
-	 *  int computeMode;
-	 *  int concurrentKernels;
-	 *  int ECCEnabled;
-	 *  int pciBusID;
-	 *  int pciDeviceID;
-	 *  int tccDriver;
-	 * }
-	 */
-	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);
-
-	/* define gpu_device */
-	if(total_devices==0){
-		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
-		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
-	}else{
-		gpu_device_id = 0;
-	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((FT_THREADS_PER_BLOCK_ON_COMPUTE_INDEXMAP>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_COMPUTE_INDEXMAP<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_compute_indexmap = FT_THREADS_PER_BLOCK_ON_COMPUTE_INDEXMAP;
-	}else{
-		threads_per_block_on_compute_indexmap = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_COMPUTE_INITIAL_CONDITIONS>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_COMPUTE_INITIAL_CONDITIONS<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_compute_initial_conditions = FT_THREADS_PER_BLOCK_ON_COMPUTE_INITIAL_CONDITIONS;
-	}else{
-		threads_per_block_on_compute_initial_conditions = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_INIT_UI>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_INIT_UI<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_init_ui = FT_THREADS_PER_BLOCK_ON_INIT_UI;
-	}else{
-		threads_per_block_on_init_ui=gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_EVOLVE>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_EVOLVE<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_evolve = FT_THREADS_PER_BLOCK_ON_EVOLVE;
-	}else{
-		threads_per_block_on_evolve=gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTX_1>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTX_1<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_fftx_1 = FT_THREADS_PER_BLOCK_ON_FFTX_1;
-	}else{
-		threads_per_block_on_fftx_1 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTX_2>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTX_2<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_fftx_2 = FT_THREADS_PER_BLOCK_ON_FFTX_2;
-	}else{
-		threads_per_block_on_fftx_2 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTX_3>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTX_3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_fftx_3 = FT_THREADS_PER_BLOCK_ON_FFTX_3;
-	}else{
-		threads_per_block_on_fftx_3 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTY_1>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTY_1<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_ffty_1 = FT_THREADS_PER_BLOCK_ON_FFTY_1;
-	}else{
-		threads_per_block_on_ffty_1 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTY_2>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTY_2<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_ffty_2 = FT_THREADS_PER_BLOCK_ON_FFTY_2;
-	}else{
-		threads_per_block_on_ffty_2 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTY_3>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTY_3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_ffty_3 = FT_THREADS_PER_BLOCK_ON_FFTY_3;
-	}else{
-		threads_per_block_on_ffty_3 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTZ_1>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTZ_1<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_fftz_1 = FT_THREADS_PER_BLOCK_ON_FFTZ_1;
-	}else{
-		threads_per_block_on_fftz_1 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTZ_2>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTZ_2<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_fftz_2 = FT_THREADS_PER_BLOCK_ON_FFTZ_2;
-	}else{
-		threads_per_block_on_fftz_2 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_FFTZ_3>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_FFTZ_3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_fftz_3 = FT_THREADS_PER_BLOCK_ON_FFTZ_3;
-	}else{
-		threads_per_block_on_fftz_3 = gpu_device_properties.warpSize;
-	}
-	if((FT_THREADS_PER_BLOCK_ON_CHECKSUM>=1)&&
-			(FT_THREADS_PER_BLOCK_ON_CHECKSUM<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_checksum = FT_THREADS_PER_BLOCK_ON_CHECKSUM;
-	}else{
-		threads_per_block_on_checksum = gpu_device_properties.warpSize;
-	}	
-
-	blocks_per_grid_on_compute_indexmap=ceil(double(NTOTAL)/double(threads_per_block_on_compute_indexmap));
-	blocks_per_grid_on_compute_initial_conditions=ceil(double(NZ)/double(threads_per_block_on_compute_initial_conditions));
-	blocks_per_grid_on_init_ui=ceil(double(NTOTAL)/double(threads_per_block_on_init_ui));
-	blocks_per_grid_on_evolve=ceil(double(NTOTAL)/double(threads_per_block_on_evolve));
-	blocks_per_grid_on_fftx_1=ceil(double(NX*NY*NZ)/double(threads_per_block_on_fftx_1));
-	blocks_per_grid_on_fftx_2=ceil(double(NY*NZ)/double(threads_per_block_on_fftx_2));
-	blocks_per_grid_on_fftx_3=ceil(double(NX*NY*NZ)/double(threads_per_block_on_fftx_3));
-	blocks_per_grid_on_ffty_1=ceil(double(NX*NY*NZ)/double(threads_per_block_on_ffty_1));
-	blocks_per_grid_on_ffty_2=ceil(double(NX*NZ)/double(threads_per_block_on_ffty_2));
-	blocks_per_grid_on_ffty_3=ceil(double(NX*NY*NZ)/double(threads_per_block_on_ffty_3));
-	blocks_per_grid_on_fftz_1=ceil(double(NX*NY*NZ)/double(threads_per_block_on_fftz_1));
-	blocks_per_grid_on_fftz_2=ceil(double(NX*NY)/double(threads_per_block_on_fftz_2));
-	blocks_per_grid_on_fftz_3=ceil(double(NX*NY*NZ)/double(threads_per_block_on_fftz_3));
-	blocks_per_grid_on_checksum=ceil(double(CHECKSUM_TASKS)/double(threads_per_block_on_checksum));
-
-	size_sums_device=(NITER_DEFAULT+1)*sizeof(dcomplex);
-	size_starts_device=NZ*sizeof(double);
-	size_twiddle_device=NTOTAL*sizeof(double);
-	size_u_device=MAXDIM*sizeof(dcomplex);
-	size_u0_device=NTOTAL*sizeof(dcomplex);
-	size_u1_device=NTOTAL*sizeof(dcomplex);
-	size_y0_device=NTOTAL*sizeof(dcomplex);
-	size_y1_device=NTOTAL*sizeof(dcomplex);
-	size_shared_data=threads_per_block_on_checksum*sizeof(dcomplex);
-
-	cudaMalloc(&sums_device, size_sums_device);
-	cudaMalloc(&starts_device, size_starts_device);
-	cudaMalloc(&twiddle_device, size_twiddle_device);
-	cudaMalloc(&u_device, size_u_device);
-	cudaMalloc(&u0_device, size_u0_device);
-	cudaMalloc(&u1_device, size_u1_device);
-	cudaMalloc(&y0_device, size_y0_device);
-	cudaMalloc(&y1_device, size_y1_device);
-
-	omp_set_num_threads(OMP_THREADS);	
+	hipDeviceProp_t deviceProp;
+	hipSetDevice(DEFAULT_GPU);	
+	hipGetDeviceProperties(&deviceProp, DEFAULT_GPU);	
+
+	THREADS_PER_BLOCK_AT_COMPUTE_INDEXMAP = deviceProp.maxThreadsPerBlock;
+	THREADS_PER_BLOCK_AT_COMPUTE_INITIAL_CONDITIONS = 128;
+	THREADS_PER_BLOCK_AT_INIT_UI = deviceProp.maxThreadsPerBlock;	
+	THREADS_PER_BLOCK_AT_EVOLVE = deviceProp.maxThreadsPerBlock;
+	THREADS_PER_BLOCK_AT_FFT1 = deviceProp.maxThreadsPerBlock;
+	THREADS_PER_BLOCK_AT_FFT2 = deviceProp.maxThreadsPerBlock;
+	THREADS_PER_BLOCK_AT_FFT3 = deviceProp.maxThreadsPerBlock;
+
+	size_sums_device=sizeof(dcomplex)*(NITER_DEFAULT+1);
+	size_starts_device=sizeof(double)*(NZ);
+	size_twiddle_device=sizeof(double)*(NTOTAL);
+	size_u_device=sizeof(dcomplex)*(MAXDIM);
+	size_u0_device=sizeof(dcomplex)*(NTOTAL);
+	size_u1_device=sizeof(dcomplex)*(NTOTAL);
+	size_y0_device=sizeof(dcomplex)*(NTOTAL);
+	size_y1_device=sizeof(dcomplex)*(NTOTAL);
+
+	hipMalloc(&sums_device, size_sums_device);
+	hipMalloc(&starts_device, size_starts_device);
+	hipMalloc(&twiddle_device, size_twiddle_device);
+	hipMalloc(&u_device, size_u_device);
+	hipMalloc(&u0_device, size_u0_device);
+	hipMalloc(&u1_device, size_u1_device);
+	hipMalloc(&y0_device, size_y0_device);
+	hipMalloc(&y1_device, size_y1_device);
+
+	omp_set_num_threads(OMP_THREADS);
 }
 
 static void verify(int d1,
diff --git a/CUDA/FT/setup.ini b/CUDA/FT/setup.ini
new file mode 100644
index 0000000..9d05c7e
--- /dev/null
+++ b/CUDA/FT/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/ft.S;../bin/ft.W;../bin/ft.A;../bin/ft.B
+use_clang_plugin = false
+llvm_pass = make -f Makefile.all llvm
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/IS/Makefile b/CUDA/IS/Makefile
index 2ef3b1e..1a040c6 100644
--- a/CUDA/IS/Makefile
+++ b/CUDA/IS/Makefile
@@ -16,5 +16,5 @@ is.o:		is.cu npbparams.hpp
 	${CCOMPILE} is.cu
 
 clean:
-	- rm -f *.o *~ 
+	- rm -f *.o *.bc seq.txt *~ 
 	- rm -f npbparams.hpp core
diff --git a/CUDA/IS/is.cu b/CUDA/IS/is.cu
index e0020e0..b108517 100644
--- a/CUDA/IS/is.cu
+++ b/CUDA/IS/is.cu
@@ -66,6 +66,10 @@
 #define PROFILING_RANK (2)
 #define PROFILING_VERIFY (3)
 
+#define THREADS_PER_BLOCK (256)
+#define SHARE_MEMORY_ON_RANK_GPU_KERNEL_4 (2*THREADS_PER_BLOCK)
+#define SHARE_MEMORY_ON_RANK_GPU_KERNEL_5 (2*THREADS_PER_BLOCK)
+
 /*****************************************************************/
 /* for serial IS, buckets are not really req'd to solve NPB1 IS  */
 /* spec, but their use on some machines improves performance, on */
@@ -155,19 +159,20 @@
 
 /*************************************/
 /* typedef: if necessary, change the */
-/* size of INT_TYPE here by changing the  */
-/* INT_TYPE type to, say, long            */
+/* size of int here by changing the  */
+/* int type to, say, long            */
 /*************************************/
 #if CLASS == 'D'
-typedef long INT_TYPE;
+/* #TODO *//* is necessary implement INT_TYPE for class D */
+/* typedef long INT_TYPE; */
 #else
-typedef int INT_TYPE;
+/* typedef int INT_TYPE; */
 #endif
 
 /**********************/
 /* partial verif info */
 /**********************/
-INT_TYPE test_index_array[TEST_ARRAY_SIZE],
+int test_index_array[TEST_ARRAY_SIZE],
     test_rank_array[TEST_ARRAY_SIZE],
 
     S_test_index_array[TEST_ARRAY_SIZE] = 
@@ -201,16 +206,28 @@ INT_TYPE test_index_array[TEST_ARRAY_SIZE],
 {1,36538729,1978098519,2145192618,2147425337};
 
 /* global variables */
-INT_TYPE passed_verification;
-INT_TYPE* key_array_device; 
-INT_TYPE* key_buff1_device; 
-INT_TYPE* key_buff2_device;
-INT_TYPE* index_array_device; 
-INT_TYPE* rank_array_device;
-INT_TYPE* partial_verify_vals_device;
-INT_TYPE* passed_verification_device;
-INT_TYPE* key_scan_device; 
-INT_TYPE* sum_device;
+int passed_verification;
+int timer_on;
+int threads_per_block;
+int blocks_per_grid;
+int amount_of_work;
+int THREADS_PER_BLOCK_AT_CREATE_SEQ_GPU_KERNEL;
+int AMOUNT_OF_WORK_AT_CREATE_SEQ_GPU_KERNEL;
+int THREADS_PER_BLOCK_AT_RANK_GPU_KERNEL_2;
+int AMOUNT_OF_WORK_AT_RANK_GPU_KERNEL_2;
+int THREADS_PER_BLOCK_AT_RANK_GPU_KERNEL_3;
+int AMOUNT_OF_WORK_AT_RANK_GPU_KERNEL_3;
+int THREADS_PER_BLOCK_AT_FULL_VERIFY;
+int AMOUNT_OF_WORK_AT_FULL_VERIFY;
+int* key_array_device; 
+int* key_buff1_device; 
+int* key_buff2_device;
+int* index_array_device; 
+int* rank_array_device;
+int* partial_verify_vals_device;
+int* passed_verification_device;
+int* key_scan_device; 
+int* sum_device;
 size_t size_test_array_device;
 size_t size_key_array_device; 
 size_t size_key_buff1_device; 
@@ -221,113 +238,71 @@ size_t size_partial_verify_vals_device;
 size_t size_passed_verification_device;
 size_t size_key_scan_device; 
 size_t size_sum_device;
-size_t size_shared_data_on_rank_4;
-size_t size_shared_data_on_rank_5;
-size_t size_shared_data_on_full_verify_3;
-INT_TYPE threads_per_block_on_create_seq;
-INT_TYPE threads_per_block_on_rank;
-INT_TYPE threads_per_block_on_rank_1;
-INT_TYPE threads_per_block_on_rank_2;
-INT_TYPE threads_per_block_on_rank_3;
-INT_TYPE threads_per_block_on_rank_4;
-INT_TYPE threads_per_block_on_rank_5;
-INT_TYPE threads_per_block_on_rank_6;
-INT_TYPE threads_per_block_on_rank_7;
-INT_TYPE threads_per_block_on_full_verify;
-INT_TYPE threads_per_block_on_full_verify_1;
-INT_TYPE threads_per_block_on_full_verify_2;
-INT_TYPE threads_per_block_on_full_verify_3;
-INT_TYPE blocks_per_grid_on_create_seq;
-INT_TYPE blocks_per_grid_on_rank_1;
-INT_TYPE blocks_per_grid_on_rank_2;
-INT_TYPE blocks_per_grid_on_rank_3;
-INT_TYPE blocks_per_grid_on_rank_4;
-INT_TYPE blocks_per_grid_on_rank_5;
-INT_TYPE blocks_per_grid_on_rank_6;
-INT_TYPE blocks_per_grid_on_rank_7;
-INT_TYPE blocks_per_grid_on_full_verify_1;
-INT_TYPE blocks_per_grid_on_full_verify_2;
-INT_TYPE blocks_per_grid_on_full_verify_3;
-INT_TYPE amount_of_work_on_create_seq;
-INT_TYPE amount_of_work_on_rank_1;
-INT_TYPE amount_of_work_on_rank_2;
-INT_TYPE amount_of_work_on_rank_3;
-INT_TYPE amount_of_work_on_rank_4;
-INT_TYPE amount_of_work_on_rank_5;
-INT_TYPE amount_of_work_on_rank_6;
-INT_TYPE amount_of_work_on_rank_7;
-INT_TYPE amount_of_work_on_full_verify_1;
-INT_TYPE amount_of_work_on_full_verify_2;
-INT_TYPE amount_of_work_on_full_verify_3;
-int gpu_device_id;
-int total_devices;
-cudaDeviceProp gpu_device_properties;
-extern __shared__ INT_TYPE extern_share_data[];
 
 /* function declarations */
 static void create_seq_gpu(double seed, 
 		double a);
-__global__ void create_seq_gpu_kernel(INT_TYPE* key_array,
+__global__ void create_seq_gpu_kernel(int* key_array,
 		double seed,
 		double a,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__device__ double find_my_seed_device(INT_TYPE kn,
-		INT_TYPE np,
+		int number_of_blocks,
+		int amount_of_work);
+__device__ double find_my_seed_device(int kn,
+		int np,
 		long nn,
 		double s,
 		double a);
 static void full_verify_gpu();
-__global__ void full_verify_gpu_kernel_1(INT_TYPE* key_array,
-		INT_TYPE* key_buff2,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void full_verify_gpu_kernel_2(INT_TYPE* key_buff2,
-		INT_TYPE* key_buff_ptr_global,
-		INT_TYPE* key_array,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void full_verify_gpu_kernel_3(INT_TYPE* key_array,
-		INT_TYPE* global_aux,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
+__global__ void full_verify_gpu_kernel_1(int* key_array,
+		int* key_buff2,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void full_verify_gpu_kernel_2(int* key_buff2,
+		int* key_buff_ptr_global,
+		int* key_array,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void full_verify_gpu_kernel_3(int* key_array,
+		int* global_aux,
+		int number_of_blocks,
+		int amount_of_work);
 __device__ double randlc_device(double* X,
 		double* A);
-static void rank_gpu(INT_TYPE iteration);
-__global__ void rank_gpu_kernel_1(INT_TYPE* key_array,
-		INT_TYPE* partial_verify_vals,
-		INT_TYPE* test_index_array,
-		INT_TYPE iteration,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void rank_gpu_kernel_2(INT_TYPE* key_buff1,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void rank_gpu_kernel_3(INT_TYPE* key_buff_ptr,
-		INT_TYPE* key_buff_ptr2,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void rank_gpu_kernel_4(INT_TYPE* source,
-		INT_TYPE* destiny,
-		INT_TYPE* sum,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void rank_gpu_kernel_5(INT_TYPE* source,
-		INT_TYPE* destiny,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void rank_gpu_kernel_6(INT_TYPE* source,
-		INT_TYPE* destiny,
-		INT_TYPE* offset,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
-__global__ void rank_gpu_kernel_7(INT_TYPE* partial_verify_vals,
-		INT_TYPE* key_buff_ptr,
-		INT_TYPE* test_rank_array,
-		INT_TYPE* passed_verification_device,
-		INT_TYPE iteration,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work);
+static void rank_gpu(int iteration);
+__global__ void rank_gpu_kernel_1(int* key_array,
+		int* partial_verify_vals,
+		int* test_index_array,
+		int iteration,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void rank_gpu_kernel_2(int* key_buff1,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void rank_gpu_kernel_3(int* key_buff_ptr,
+		int* key_buff_ptr2,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void rank_gpu_kernel_4(int* source,
+		int* destiny,
+		int* sum,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void rank_gpu_kernel_5(int* source,
+		int* destiny,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void rank_gpu_kernel_6(int* source,
+		int* destiny,
+		int* offset,
+		int number_of_blocks,
+		int amount_of_work);
+__global__ void rank_gpu_kernel_7(int* partial_verify_vals,
+		int* key_buff_ptr,
+		int* test_rank_array,
+		int* passed_verification_device,
+		int iteration,
+		int number_of_blocks,
+		int amount_of_work);
 static void release_gpu();
 static void setup_gpu();
 
@@ -336,11 +311,13 @@ int main(int argc, char** argv){
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
 	printf(" DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION mode on\n");
 #endif
+	int i, iteration;
+	double timecounter;
+	FILE* fp;
+
 #if defined(PROFILING)
 	printf(" PROFILING mode on\n");
 #endif
-	INT_TYPE i, iteration;
-	double timecounter;
 
 	timer_clear(PROFILING_TOTAL_TIME);
 #if defined(PROFILING)
@@ -420,17 +397,16 @@ int main(int argc, char** argv){
 #endif
 	/* this is the main iteration */
 	for(iteration=1; iteration<=MAX_ITERATIONS; iteration++){
-		if(CLASS != 'S')printf( "        %ld\n", (long)iteration);		
+		if(CLASS != 'S')printf( "        %d\n", iteration);
 		rank_gpu(iteration);
 	}
+	cudaMemcpy(&passed_verification, passed_verification_device, size_passed_verification_device, cudaMemcpyDeviceToHost);
 #if defined(PROFILING)
 	timer_stop(PROFILING_RANK);
 #else
 	timer_stop(PROFILING_TOTAL_TIME);
 #endif
 
-	cudaMemcpy(&passed_verification, passed_verification_device, size_passed_verification_device, cudaMemcpyDeviceToHost);	
-
 	/* 
 	 * this tests that keys are in sequence: sorting of last ranked key seq
 	 * occurs here, but is an untimed operation                             
@@ -442,6 +418,8 @@ int main(int argc, char** argv){
 #if defined(PROFILING)
 	timer_stop(PROFILING_VERIFY);
 #endif
+
+	/* end of timing, obtain maximum time of all processors */
 #if defined(PROFILING)
 	timer_stop(PROFILING_TOTAL_TIME);
 	timecounter = timer_read(PROFILING_RANK);
@@ -449,30 +427,6 @@ int main(int argc, char** argv){
 	timecounter = timer_read(PROFILING_TOTAL_TIME);
 #endif
 
-
-	char gpu_config[256];
-	char gpu_config_string[2048];
-
-#if defined(PROFILING)
-	sprintf(gpu_config, "%5s\t%25s\t%25s\t%25s\n", "GPU Kernel", "Threads Per Block", "Time in Seconds", "Time in Percentage");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25ld\t%25f\t%24.2f%%\n", " create", (long) threads_per_block_on_create_seq, timer_read(PROFILING_CREATE), (timer_read(PROFILING_CREATE)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25ld\t%25f\t%24.2f%%\n", " rank", (long) threads_per_block_on_rank, timer_read(PROFILING_RANK), (timer_read(PROFILING_RANK)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25ld\t%25f\t%24.2f%%\n", " verify", (long) threads_per_block_on_full_verify, timer_read(PROFILING_VERIFY), (timer_read(PROFILING_VERIFY)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-#else
-	sprintf(gpu_config, "%5s\t%25s\n", "GPU Kernel", "Threads Per Block");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25ld\n", " create", (long) threads_per_block_on_create_seq);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25ld\n", " rank", (long) threads_per_block_on_rank);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25ld\n", " verify", (long) threads_per_block_on_full_verify);
-	strcat(gpu_config_string, gpu_config);
-#endif
-
 	/* the final printout  */
 	if(passed_verification != 5*MAX_ITERATIONS+1){passed_verification = 0;}
 	c_print_results((char*)"IS",
@@ -484,14 +438,9 @@ int main(int argc, char** argv){
 			timecounter,
 			((double)(MAX_ITERATIONS*TOTAL_KEYS))/timecounter/1000000.0,
 			(char*)"keys ranked",
-			(int)passed_verification,
+			passed_verification,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			(char*)gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
@@ -500,33 +449,54 @@ int main(int argc, char** argv){
 			(char*)CS6,
 			(char*)CS7);
 
+	/* print additional timers */
+#if defined(PROFILING)
+	double t_total, t_percent;
+	t_total = timer_read(PROFILING_TOTAL_TIME);
+	printf("\nAdditional timers -\n");
+	printf(" Total execution: %8.3f\n", t_total);
+	if(t_total == 0.0)t_total = 1.0;
+	timecounter = timer_read(PROFILING_CREATE);
+	t_percent = timecounter/t_total * 100.;
+	printf(" Initialization : %8.3f (%5.2f%%)\n", timecounter, t_percent);
+	timecounter = timer_read(PROFILING_RANK);
+	t_percent = timecounter/t_total * 100.;
+	printf(" Benchmarking   : %8.3f (%5.2f%%)\n", timecounter, t_percent);
+	timecounter = timer_read(PROFILING_VERIFY);
+	t_percent = timecounter/t_total * 100.;
+	printf(" Sorting        : %8.3f (%5.2f%%)\n", timecounter, t_percent);
+#endif
+
 	release_gpu();
 
 	return 0;  
 }
 
 static void create_seq_gpu(double seed, double a){  
-	create_seq_gpu_kernel<<<blocks_per_grid_on_create_seq, 
-		threads_per_block_on_create_seq>>>(key_array_device,
-				seed,
-				a,
-				blocks_per_grid_on_create_seq,
-				amount_of_work_on_create_seq);
+	threads_per_block = THREADS_PER_BLOCK_AT_CREATE_SEQ_GPU_KERNEL;
+	amount_of_work = AMOUNT_OF_WORK_AT_CREATE_SEQ_GPU_KERNEL;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+	create_seq_gpu_kernel<<<blocks_per_grid, threads_per_block>>>(key_array_device,
+			seed,
+			a,
+			blocks_per_grid,
+			amount_of_work);
 	cudaDeviceSynchronize();
 }
 
-__global__ void create_seq_gpu_kernel(INT_TYPE* key_array,
+__global__ void create_seq_gpu_kernel(int* key_array,
 		double seed,
 		double a,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
+		int number_of_blocks,
+		int amount_of_work){
 	double x, s;
-	INT_TYPE i, k;
+	int i, k;
 
-	INT_TYPE k1, k2;
+	int k1, k2;
 	double an = a;
-	INT_TYPE myid, num_procs;
-	INT_TYPE mq;
+	int myid, num_procs;
+	int mq;
 
 	myid = blockIdx.x*blockDim.x+threadIdx.x;
 	num_procs = amount_of_work;
@@ -549,8 +519,8 @@ __global__ void create_seq_gpu_kernel(INT_TYPE* key_array,
 	}
 }
 
-__device__ double find_my_seed_device(INT_TYPE kn,
-		INT_TYPE np,
+__device__ double find_my_seed_device(int kn,
+		int np,
 		long nn,
 		double s,
 		double a){
@@ -581,41 +551,46 @@ __device__ double find_my_seed_device(INT_TYPE kn,
 }
 
 static void full_verify_gpu(){		
-	INT_TYPE* memory_aux_device;
-	size_t size_memory_aux=sizeof(INT_TYPE)*(amount_of_work_on_full_verify_3/threads_per_block_on_full_verify_3);		
+	int* memory_aux_device;
+	size_t size_memory_aux=sizeof(int)*(AMOUNT_OF_WORK_AT_FULL_VERIFY/THREADS_PER_BLOCK_AT_FULL_VERIFY);		
 	cudaMalloc(&memory_aux_device, size_memory_aux);	
 
 	/* full_verify_gpu_kernel_1 */
-	full_verify_gpu_kernel_1<<<blocks_per_grid_on_full_verify_1, 
-		threads_per_block_on_full_verify_1>>>(key_array_device,
-				key_buff2_device,
-				blocks_per_grid_on_full_verify_1,
-				amount_of_work_on_full_verify_1);
+	threads_per_block = THREADS_PER_BLOCK;
+	amount_of_work = NUM_KEYS;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	full_verify_gpu_kernel_1<<<blocks_per_grid, threads_per_block>>>(key_array_device,
+			key_buff2_device,
+			blocks_per_grid,
+			amount_of_work);
 	cudaDeviceSynchronize();
 
 	/* full_verify_gpu_kernel_2 */
-	full_verify_gpu_kernel_2<<<blocks_per_grid_on_full_verify_2, 
-		threads_per_block_on_full_verify_2>>>(key_buff2_device,
-				key_buff1_device,
-				key_array_device,
-				blocks_per_grid_on_full_verify_2,
-				amount_of_work_on_full_verify_2);
+	threads_per_block = THREADS_PER_BLOCK;
+	amount_of_work = NUM_KEYS;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	full_verify_gpu_kernel_2<<<blocks_per_grid, threads_per_block>>>(key_buff2_device,
+			key_buff1_device,
+			key_array_device,
+			blocks_per_grid,
+			amount_of_work);
 	cudaDeviceSynchronize();
 
 	/* full_verify_gpu_kernel_3 */
-	full_verify_gpu_kernel_3<<<blocks_per_grid_on_full_verify_3, 
-		threads_per_block_on_full_verify_3,
-		size_shared_data_on_full_verify_3>>>(key_array_device,
-				memory_aux_device,
-				blocks_per_grid_on_full_verify_3,
-				amount_of_work_on_full_verify_3);
+	threads_per_block = THREADS_PER_BLOCK_AT_FULL_VERIFY;
+	amount_of_work = AMOUNT_OF_WORK_AT_FULL_VERIFY;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	full_verify_gpu_kernel_3<<<blocks_per_grid, threads_per_block>>>(key_array_device,
+			memory_aux_device,
+			blocks_per_grid,
+			amount_of_work);
 	cudaDeviceSynchronize();
 
 	/* reduce on cpu */
-	INT_TYPE i, j = 0;
-	INT_TYPE* memory_aux_host=(INT_TYPE*)malloc(size_memory_aux);
+	int i, j = 0;
+	int* memory_aux_host=(int*)malloc(size_memory_aux);
 	cudaMemcpy(memory_aux_host, memory_aux_device, size_memory_aux, cudaMemcpyDeviceToHost);
-	for(i=0; i<size_memory_aux/sizeof(INT_TYPE); i++){
+	for(i=0; i<size_memory_aux/sizeof(int); i++){
 		j += memory_aux_host[i];
 	}	
 
@@ -629,37 +604,31 @@ static void full_verify_gpu(){
 	free(memory_aux_host);
 }
 
-__global__ void full_verify_gpu_kernel_1(INT_TYPE* key_array,
-		INT_TYPE* key_buff2,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
-	INT_TYPE i = blockIdx.x*blockDim.x+threadIdx.x;
+__global__ void full_verify_gpu_kernel_1(int* key_array,
+		int* key_buff2,
+		int number_of_blocks,
+		int amount_of_work){
+	int i = blockIdx.x*blockDim.x+threadIdx.x;
 	key_buff2[i] = key_array[i];
 }
 
-__global__ void full_verify_gpu_kernel_2(INT_TYPE* key_buff2,
-		INT_TYPE* key_buff_ptr_global,
-		INT_TYPE* key_array,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){		
-	INT_TYPE value = key_buff2[blockIdx.x*blockDim.x+threadIdx.x];
-
-	#if CLASS == 'D'
-		INT_TYPE index = atomicAdd( (unsigned long long int*) &key_buff_ptr_global[value], (unsigned long long int) -1) -1;
-	#else
-		INT_TYPE index = atomicAdd(&key_buff_ptr_global[value], -1) -1;
-	#endif
-
+__global__ void full_verify_gpu_kernel_2(int* key_buff2,
+		int* key_buff_ptr_global,
+		int* key_array,
+		int number_of_blocks,
+		int amount_of_work){		
+	int value = key_buff2[blockIdx.x*blockDim.x+threadIdx.x];
+	int index = atomicAdd(&key_buff_ptr_global[value], -1) - 1;
 	key_array[index] = value;
 }
 
-__global__ void full_verify_gpu_kernel_3(INT_TYPE* key_array,
-		INT_TYPE* global_aux,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
-	INT_TYPE* shared_aux = (INT_TYPE*)(extern_share_data);
+__global__ void full_verify_gpu_kernel_3(int* key_array,
+		int* global_aux,
+		int number_of_blocks,
+		int amount_of_work){
+	__shared__ int shared_aux[THREADS_PER_BLOCK];
 
-	INT_TYPE i = (blockIdx.x*blockDim.x+threadIdx.x) + 1;
+	int i = (blockIdx.x*blockDim.x+threadIdx.x) + 1;
 
 	if(i<NUM_KEYS){
 		if(key_array[i-1]>key_array[i]){shared_aux[threadIdx.x]=1;}
@@ -668,7 +637,7 @@ __global__ void full_verify_gpu_kernel_3(INT_TYPE* key_array,
 
 	__syncthreads();
 
-	for(i=blockDim.x/2; i>0; i>>=1){
+	for(i=THREADS_PER_BLOCK/2; i>0; i>>=1){
 		if(threadIdx.x<i){
 			shared_aux[threadIdx.x] += shared_aux[threadIdx.x+i];
 		}
@@ -686,7 +655,7 @@ __device__ double randlc_device(double* X,
 	double X1;
 	double X2;
 	double Z;
-	INT_TYPE j;
+	int j;
 
 	/*
 	 * --------------------------------------------------------------------
@@ -722,71 +691,92 @@ __device__ double randlc_device(double* X,
 	return(R46 * *X);
 } 
 
-static void rank_gpu(INT_TYPE iteration){
+static void rank_gpu(int iteration){
 	/* rank_gpu_kernel_1 */
-	rank_gpu_kernel_1<<<blocks_per_grid_on_rank_1, 
-		threads_per_block_on_rank_1>>>(key_array_device,
-				partial_verify_vals_device,
-				index_array_device,
-				iteration,
-				blocks_per_grid_on_rank_1,
-				amount_of_work_on_rank_1);
+	threads_per_block = 1;
+	amount_of_work = 1;
+	blocks_per_grid = 1;
+	rank_gpu_kernel_1<<<blocks_per_grid, threads_per_block>>>(key_array_device,
+			partial_verify_vals_device,
+			index_array_device,
+			iteration,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 
 	/* rank_gpu_kernel_2 */
-	rank_gpu_kernel_2<<<blocks_per_grid_on_rank_2, 
-		threads_per_block_on_rank_2>>>(key_buff1_device,
-				blocks_per_grid_on_rank_2,
-				amount_of_work_on_rank_2);
+	threads_per_block = THREADS_PER_BLOCK_AT_RANK_GPU_KERNEL_2;
+	amount_of_work = AMOUNT_OF_WORK_AT_RANK_GPU_KERNEL_2;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	rank_gpu_kernel_2<<<blocks_per_grid, threads_per_block>>>(key_buff1_device,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 
 	/* rank_gpu_kernel_3 */
-	rank_gpu_kernel_3<<<blocks_per_grid_on_rank_3, 
-		threads_per_block_on_rank_3>>>(key_buff1_device,
-				key_array_device,
-				blocks_per_grid_on_rank_3,
-				amount_of_work_on_rank_3);
+	threads_per_block = THREADS_PER_BLOCK_AT_RANK_GPU_KERNEL_3;
+	amount_of_work = AMOUNT_OF_WORK_AT_RANK_GPU_KERNEL_3;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	rank_gpu_kernel_3<<<blocks_per_grid, threads_per_block>>>(key_buff1_device,
+			key_array_device,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 
 	/* rank_gpu_kernel_4 */
-	rank_gpu_kernel_4<<<blocks_per_grid_on_rank_4, 
-		threads_per_block_on_rank_4,
-		size_shared_data_on_rank_4>>>(key_buff1_device,
-				key_buff1_device,
-				sum_device,
-				blocks_per_grid_on_rank_4,
-				amount_of_work_on_rank_4);
+	threads_per_block = THREADS_PER_BLOCK;
+	amount_of_work = THREADS_PER_BLOCK * THREADS_PER_BLOCK;
+	if(amount_of_work > MAX_KEY){amount_of_work = MAX_KEY;}
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	rank_gpu_kernel_4<<<blocks_per_grid, threads_per_block>>>(key_buff1_device,
+			key_buff1_device,
+			sum_device,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 
 	/* rank_gpu_kernel_5 */
-	rank_gpu_kernel_5<<<blocks_per_grid_on_rank_5, 
-		threads_per_block_on_rank_5,
-		size_shared_data_on_rank_5>>>(sum_device,
-				sum_device,
-				blocks_per_grid_on_rank_5,
-				amount_of_work_on_rank_5);
+	threads_per_block = THREADS_PER_BLOCK;
+	amount_of_work = THREADS_PER_BLOCK;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	rank_gpu_kernel_5<<<blocks_per_grid, threads_per_block>>>(sum_device,
+			sum_device,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 
 	/* rank_gpu_kernel_6 */
-	rank_gpu_kernel_6<<<blocks_per_grid_on_rank_6, 
-		threads_per_block_on_rank_6>>>(key_buff1_device,
-				key_buff1_device,
-				sum_device,
-				blocks_per_grid_on_rank_6,
-				amount_of_work_on_rank_6);
+	threads_per_block = THREADS_PER_BLOCK;
+	amount_of_work = THREADS_PER_BLOCK * THREADS_PER_BLOCK;
+	if(amount_of_work > MAX_KEY){amount_of_work = MAX_KEY;}
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	rank_gpu_kernel_6<<<blocks_per_grid, threads_per_block>>>(key_buff1_device,
+			key_buff1_device,
+			sum_device,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 
 	/* rank_gpu_kernel_7 */
-	rank_gpu_kernel_7<<<blocks_per_grid_on_rank_7, 
-		threads_per_block_on_rank_7>>>(partial_verify_vals_device,
-				key_buff1_device,
-				rank_array_device,
-				passed_verification_device,
-				iteration,
-				blocks_per_grid_on_rank_7,
-				amount_of_work_on_rank_7);
+	threads_per_block = 1;
+	amount_of_work = 1;
+	blocks_per_grid = 1;
+	rank_gpu_kernel_7<<<blocks_per_grid, threads_per_block>>>(partial_verify_vals_device,
+			key_buff1_device,
+			rank_array_device,
+			passed_verification_device,
+			iteration,
+			blocks_per_grid,
+			amount_of_work);
+	cudaDeviceSynchronize();
 }
 
-__global__ void rank_gpu_kernel_1(INT_TYPE* key_array,
-		INT_TYPE* partial_verify_vals,
-		INT_TYPE* test_index_array,
-		INT_TYPE iteration,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
+__global__ void rank_gpu_kernel_1(int* key_array,
+		int* partial_verify_vals,
+		int* test_index_array,
+		int iteration,
+		int number_of_blocks,
+		int amount_of_work){
 	key_array[iteration] = iteration;
 	key_array[iteration+MAX_ITERATIONS] = MAX_KEY - iteration;
 	/*
@@ -796,22 +786,21 @@ __global__ void rank_gpu_kernel_1(INT_TYPE* key_array,
 	 * load into top of array bucket_size  
 	 * --------------------------------------------------------------------
 	 */
-#pragma unroll
-	for(INT_TYPE i=0; i<TEST_ARRAY_SIZE; i++){
+	for(int i=0; i<TEST_ARRAY_SIZE; i++){
 		partial_verify_vals[i] = key_array[test_index_array[i]];
 	}
 }
 
-__global__ void rank_gpu_kernel_2(INT_TYPE* key_buff1,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
+__global__ void rank_gpu_kernel_2(int* key_buff1,
+		int number_of_blocks,
+		int amount_of_work){
 	key_buff1[blockIdx.x*blockDim.x+threadIdx.x] = 0;
 }
 
-__global__ void rank_gpu_kernel_3(INT_TYPE* key_buff_ptr,
-		INT_TYPE* key_buff_ptr2,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
+__global__ void rank_gpu_kernel_3(int* key_buff_ptr,
+		int* key_buff_ptr2,
+		int number_of_blocks,
+		int amount_of_work){
 	/*
 	 * --------------------------------------------------------------------
 	 * in this section, the keys themselves are used as their 
@@ -819,38 +808,35 @@ __global__ void rank_gpu_kernel_3(INT_TYPE* key_buff_ptr,
 	 * individual population  
 	 * --------------------------------------------------------------------
 	 */
-	#if CLASS == 'D'
-		atomicAdd( (unsigned long long int*) &key_buff_ptr[key_buff_ptr2[blockIdx.x*blockDim.x+threadIdx.x]], (unsigned long long int) 1);
-	#else
-		atomicAdd(&key_buff_ptr[key_buff_ptr2[blockIdx.x*blockDim.x+threadIdx.x]], 1);
-	#endif
+	int key = key_buff_ptr2[blockIdx.x*blockDim.x+threadIdx.x];
+	atomicAdd(&key_buff_ptr[key], 1);
 }
 
-__global__ void rank_gpu_kernel_4(INT_TYPE* source,
-		INT_TYPE* destiny,
-		INT_TYPE* sum,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
-	INT_TYPE* shared_data = (INT_TYPE*)(extern_share_data);
+__global__ void rank_gpu_kernel_4(int* source,
+		int* destiny,
+		int* sum,
+		int number_of_blocks,
+		int amount_of_work){
+	__shared__ int shared_data[SHARE_MEMORY_ON_RANK_GPU_KERNEL_4];
 
 	shared_data[threadIdx.x] = 0;
-	INT_TYPE position = blockDim.x + threadIdx.x;
+	int position = blockDim.x + threadIdx.x;
 
-	INT_TYPE factor = MAX_KEY / number_of_blocks;
-	INT_TYPE start = factor * blockIdx.x;
-	INT_TYPE end = start + factor;
+	int factor = MAX_KEY / number_of_blocks;
+	int start = factor * blockIdx.x;
+	int end = start + factor;
 
-	for(INT_TYPE i=start; i<end; i+=blockDim.x){
+	for(int i=start; i<end; i+=blockDim.x){
 		shared_data[position] = source[i + threadIdx.x];
 
-		for(INT_TYPE offset=1; offset<blockDim.x; offset<<=1){
+		for(uint offset=1; offset<blockDim.x; offset<<=1){
 			__syncthreads();
-			INT_TYPE t = shared_data[position] + shared_data[position - offset];
+			int t = shared_data[position] + shared_data[position - offset];
 			__syncthreads();
 			shared_data[position] = t;
 		}
 
-		INT_TYPE prv_val = (i == start) ? 0 : destiny[i - 1];
+		int prv_val = (i == start) ? 0 : destiny[i - 1];
 		destiny[i + threadIdx.x] = shared_data[position] + prv_val;
 	}
 
@@ -858,19 +844,19 @@ __global__ void rank_gpu_kernel_4(INT_TYPE* source,
 	if(threadIdx.x==0){sum[blockIdx.x]=destiny[end-1];}
 }
 
-__global__ void rank_gpu_kernel_5(INT_TYPE* source,
-		INT_TYPE* destiny,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
-	INT_TYPE* shared_data = (INT_TYPE*)(extern_share_data);
+__global__ void rank_gpu_kernel_5(int* source,
+		int* destiny,
+		int number_of_blocks,
+		int amount_of_work){
+	__shared__ int shared_data[SHARE_MEMORY_ON_RANK_GPU_KERNEL_5];
 
 	shared_data[threadIdx.x] = 0;
-	INT_TYPE position = blockDim.x + threadIdx.x;
+	int position = blockDim.x + threadIdx.x;
 	shared_data[position] = source[threadIdx.x];
 
-	for(INT_TYPE offset=1; offset<blockDim.x; offset<<=1){
+	for(uint offset=1; offset<blockDim.x; offset<<=1){
 		__syncthreads();
-		INT_TYPE t = shared_data[position] + shared_data[position - offset];
+		int t = shared_data[position] + shared_data[position - offset];
 		__syncthreads();
 		shared_data[position] = t;
 	}
@@ -880,27 +866,27 @@ __global__ void rank_gpu_kernel_5(INT_TYPE* source,
 	destiny[threadIdx.x] = shared_data[position - 1];
 }
 
-__global__ void rank_gpu_kernel_6(INT_TYPE* source,
-		INT_TYPE* destiny,
-		INT_TYPE* offset,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
-	INT_TYPE factor = MAX_KEY / number_of_blocks;
-	INT_TYPE start = factor * blockIdx.x;
-	INT_TYPE end = start + factor;
-	INT_TYPE sum = offset[blockIdx.x];
-	for(INT_TYPE i=start; i<end; i+=blockDim.x){
+__global__ void rank_gpu_kernel_6(int* source,
+		int* destiny,
+		int* offset,
+		int number_of_blocks,
+		int amount_of_work){
+	int factor = MAX_KEY / number_of_blocks;
+	int start = factor * blockIdx.x;
+	int end = start + factor;
+	int sum = offset[blockIdx.x];
+	for(int i=start; i<end; i+=blockDim.x){
 		destiny[i + threadIdx.x] = source[i + threadIdx.x] + sum;
 	}
 }		
 
-__global__ void rank_gpu_kernel_7(INT_TYPE* partial_verify_vals,
-		INT_TYPE* key_buff_ptr,
-		INT_TYPE* test_rank_array,
-		INT_TYPE* passed_verification_device,
-		INT_TYPE iteration,
-		INT_TYPE number_of_blocks,
-		INT_TYPE amount_of_work){
+__global__ void rank_gpu_kernel_7(int* partial_verify_vals,
+		int* key_buff_ptr,
+		int* test_rank_array,
+		int* passed_verification_device,
+		int iteration,
+		int number_of_blocks,
+		int amount_of_work){
 	/*
 	 * --------------------------------------------------------------------
 	 * this is the partial verify test section 
@@ -908,14 +894,14 @@ __global__ void rank_gpu_kernel_7(INT_TYPE* partial_verify_vals,
 	 * shifted differently for different cases
 	 * --------------------------------------------------------------------
 	 */
-	INT_TYPE i, k;
-	INT_TYPE passed_verification = 0;
+	int i, k;
+	int passed_verification = 0;
 	for(i=0; i<TEST_ARRAY_SIZE; i++){  
 		/* test vals were put here on partial_verify_vals */                                           
 		k = partial_verify_vals[i];          
 		if(0<k && k<=NUM_KEYS-1){
-			INT_TYPE key_rank = key_buff_ptr[k-1];
-			INT_TYPE failed = 0;
+			int key_rank = key_buff_ptr[k-1];
+			int failed = 0;
 			switch(CLASS){
 				case 'S':
 					if(i<=2){
@@ -998,7 +984,7 @@ __global__ void rank_gpu_kernel_7(INT_TYPE* partial_verify_vals,
 					break;
 			}
 			if(failed==1){
-				printf("Failed partial verification: iteration %ld, test key %ld\n", (long)iteration, (long)i);
+				printf("Failed partial verification: iteration %d, test key %d\n", iteration, (int)i);
 			}
 		}
 	}
@@ -1018,121 +1004,25 @@ static void release_gpu(){
 }
 
 static void setup_gpu(){
-	/*
-	 * struct cudaDeviceProp{
-	 *  char name[256];
-	 *  size_t totalGlobalMem;
-	 *  size_t sharedMemPerBlock;
-	 *  int regsPerBlock;
-	 *  int warpSize;
-	 *  size_t memPitch;
-	 *  int maxThreadsPerBlock;
-	 *  int maxThreadsDim[3];
-	 *  int maxGridSize[3];
-	 *  size_t totalConstMem;
-	 *  int major;
-	 *  int minor;
-	 *  int clockRate;
-	 *  size_t textureAlignment;
-	 *  int deviceOverlap;
-	 *  int multiProcessorCount;
-	 *  int kernelExecTimeoutEnabled;
-	 *  int integrated;
-	 *  int canMapHostMemory;
-	 *  int computeMode;
-	 *  int concurrentKernels;
-	 *  int ECCEnabled;
-	 *  int pciBusID;
-	 *  int pciDeviceID;
-	 *  int tccDriver;
-	 * }
-	 */
-	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);
-
-	/* define gpu_device */
-	if(total_devices==0){
-		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
-		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
-	}else{
-		gpu_device_id = 0;
-	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((IS_THREADS_PER_BLOCK_ON_CREATE_SEQ>=1)&&
-			(IS_THREADS_PER_BLOCK_ON_CREATE_SEQ<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_create_seq = IS_THREADS_PER_BLOCK_ON_CREATE_SEQ;
-	}else{
-		threads_per_block_on_create_seq = gpu_device_properties.warpSize;
-	}
-	if((IS_THREADS_PER_BLOCK_ON_RANK>=1)&&
-			(IS_THREADS_PER_BLOCK_ON_RANK<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_rank = IS_THREADS_PER_BLOCK_ON_RANK;
-	}else{
-		threads_per_block_on_rank = gpu_device_properties.warpSize;
-	}
-	if((IS_THREADS_PER_BLOCK_ON_FULL_VERIFY>=1)&&
-			(IS_THREADS_PER_BLOCK_ON_FULL_VERIFY<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_full_verify = IS_THREADS_PER_BLOCK_ON_FULL_VERIFY;
-	}else{
-		threads_per_block_on_full_verify = gpu_device_properties.warpSize;
-	}	
-
-	threads_per_block_on_rank_1=1;
-	threads_per_block_on_rank_2=threads_per_block_on_rank;
-	threads_per_block_on_rank_3=threads_per_block_on_rank;
-	threads_per_block_on_rank_4=threads_per_block_on_rank;
-	threads_per_block_on_rank_5=threads_per_block_on_rank;
-	threads_per_block_on_rank_6=threads_per_block_on_rank;
-	threads_per_block_on_rank_7=1;
-	threads_per_block_on_full_verify_1=threads_per_block_on_full_verify;
-	threads_per_block_on_full_verify_2=threads_per_block_on_full_verify;
-	threads_per_block_on_full_verify_3=threads_per_block_on_full_verify;
-
-	amount_of_work_on_create_seq=threads_per_block_on_create_seq*threads_per_block_on_create_seq;
-	amount_of_work_on_rank_1=1;
-	amount_of_work_on_rank_2=MAX_KEY;
-	amount_of_work_on_rank_3=NUM_KEYS;
-	amount_of_work_on_rank_4=threads_per_block_on_rank_4*threads_per_block_on_rank_4;
-	amount_of_work_on_rank_5=threads_per_block_on_rank_5;
-	amount_of_work_on_rank_6=threads_per_block_on_rank_6*threads_per_block_on_rank_6;
-	amount_of_work_on_rank_7=1;
-	amount_of_work_on_full_verify_1=NUM_KEYS;
-	amount_of_work_on_full_verify_2=NUM_KEYS;
-	amount_of_work_on_full_verify_3=NUM_KEYS;
-
-	blocks_per_grid_on_create_seq=(ceil((double)(amount_of_work_on_create_seq)/(double)(threads_per_block_on_create_seq)));
-	blocks_per_grid_on_rank_1=1;
-	blocks_per_grid_on_rank_2=(ceil((double)(amount_of_work_on_rank_2)/(double)(threads_per_block_on_rank_2)));
-	blocks_per_grid_on_rank_3=(ceil((double)(amount_of_work_on_rank_3)/(double)(threads_per_block_on_rank_3)));
-	if(amount_of_work_on_rank_4 > MAX_KEY){amount_of_work_on_rank_4=MAX_KEY;}
-	blocks_per_grid_on_rank_4=(ceil((double)(amount_of_work_on_rank_4)/(double)(threads_per_block_on_rank_4)));
-	blocks_per_grid_on_rank_5=1;
-	if(amount_of_work_on_rank_6 > MAX_KEY){amount_of_work_on_rank_6=MAX_KEY;}
-	blocks_per_grid_on_rank_6=(ceil((double)(amount_of_work_on_rank_6)/(double)(threads_per_block_on_rank_6)));
-	blocks_per_grid_on_rank_7=1;
-	blocks_per_grid_on_full_verify_1=(ceil((double)(amount_of_work_on_full_verify_1)/(double)(threads_per_block_on_full_verify_1)));
-	blocks_per_grid_on_full_verify_2=(ceil((double)(amount_of_work_on_full_verify_2)/(double)(threads_per_block_on_full_verify_2)));
-	blocks_per_grid_on_full_verify_3=(ceil((double)(amount_of_work_on_full_verify_3)/(double)(threads_per_block_on_full_verify_3)));
-
-	size_test_array_device=TEST_ARRAY_SIZE*sizeof(INT_TYPE);
-	size_key_array_device=SIZE_OF_BUFFERS*sizeof(INT_TYPE); 
-	size_key_buff1_device=MAX_KEY*sizeof(INT_TYPE); 
-	size_key_buff2_device=SIZE_OF_BUFFERS*sizeof(INT_TYPE);
-	size_index_array_device=TEST_ARRAY_SIZE*sizeof(INT_TYPE); 
-	size_rank_array_device=TEST_ARRAY_SIZE*sizeof(INT_TYPE);
-	size_partial_verify_vals_device=TEST_ARRAY_SIZE*sizeof(INT_TYPE);
-	size_passed_verification_device=1*sizeof(INT_TYPE);
-	size_key_scan_device=MAX_KEY*sizeof(INT_TYPE); 
-	size_sum_device=threads_per_block_on_rank*sizeof(INT_TYPE);
-	size_shared_data_on_rank_4=2*threads_per_block_on_rank_4*sizeof(INT_TYPE);
-	size_shared_data_on_rank_5=2*threads_per_block_on_rank_5*sizeof(INT_TYPE);
-	size_shared_data_on_full_verify_3=threads_per_block_on_full_verify_3*sizeof(INT_TYPE);
+	THREADS_PER_BLOCK_AT_CREATE_SEQ_GPU_KERNEL = 64;
+	AMOUNT_OF_WORK_AT_CREATE_SEQ_GPU_KERNEL = THREADS_PER_BLOCK_AT_CREATE_SEQ_GPU_KERNEL * 256;
+	THREADS_PER_BLOCK_AT_RANK_GPU_KERNEL_2 = THREADS_PER_BLOCK;
+	AMOUNT_OF_WORK_AT_RANK_GPU_KERNEL_2 = MAX_KEY;
+	THREADS_PER_BLOCK_AT_RANK_GPU_KERNEL_3 = THREADS_PER_BLOCK;
+	AMOUNT_OF_WORK_AT_RANK_GPU_KERNEL_3 = NUM_KEYS;
+	THREADS_PER_BLOCK_AT_FULL_VERIFY = THREADS_PER_BLOCK;
+	AMOUNT_OF_WORK_AT_FULL_VERIFY = NUM_KEYS;
+
+	size_test_array_device = sizeof(int) * TEST_ARRAY_SIZE;
+	size_key_array_device = sizeof(int) * SIZE_OF_BUFFERS; 
+	size_key_buff1_device = sizeof(int) * MAX_KEY; 
+	size_key_buff2_device = sizeof(int) * SIZE_OF_BUFFERS;
+	size_index_array_device = sizeof(int) * TEST_ARRAY_SIZE; 
+	size_rank_array_device = sizeof(int) * TEST_ARRAY_SIZE;
+	size_partial_verify_vals_device = sizeof(int) * TEST_ARRAY_SIZE;
+	size_passed_verification_device = sizeof(int) * 1;
+	size_key_scan_device = sizeof(int) * MAX_KEY; 
+	size_sum_device = sizeof(int) * THREADS_PER_BLOCK;
 
 	cudaMalloc(&key_array_device, size_key_array_device);
 	cudaMalloc(&key_buff1_device, size_key_buff1_device);
diff --git a/CUDA/LU/Makefile b/CUDA/LU/Makefile
index 4b6d93d..ec2f9a1 100644
--- a/CUDA/LU/Makefile
+++ b/CUDA/LU/Makefile
@@ -17,4 +17,4 @@ lu.o:		lu.cu npbparams.hpp
 
 clean:
 	- /bin/rm -f npbparams.hpp
-	- /bin/rm -f *.o *~
+	- /bin/rm -f *.o *.bc seq.txt *~
diff --git a/CUDA/LU/Makefile.all b/CUDA/LU/Makefile.all
new file mode 100644
index 0000000..f891b14
--- /dev/null
+++ b/CUDA/LU/Makefile.all
@@ -0,0 +1,15 @@
+SHELL=/bin/sh
+BENCHMARK=lu
+BENCHMARKU=LU
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+clang:
\ No newline at end of file
diff --git a/CUDA/LU/lu.cu b/CUDA/LU/lu.cu
index 237d223..0b9378b 100644
--- a/CUDA/LU/lu.cu
+++ b/CUDA/LU/lu.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -59,7 +60,7 @@
  * ------------------------------------------------------------------------------
  */
 
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -119,6 +120,30 @@
 #define PROFILING_SSOR_1 (22)
 #define PROFILING_SSOR_2 (23)
 
+#define THREADS_PER_BLOCK_ON_ERHS_1 (32)
+#define THREADS_PER_BLOCK_ON_ERHS_2 (32)
+#define THREADS_PER_BLOCK_ON_ERHS_3 (32)
+#define THREADS_PER_BLOCK_ON_ERHS_4 (32)
+#define THREADS_PER_BLOCK_ON_ERROR (32)
+#define THREADS_PER_BLOCK_ON_NORM (32)
+#define THREADS_PER_BLOCK_ON_JACLD_BLTS (32)
+#define THREADS_PER_BLOCK_ON_JACU_BUTS (32)
+#define THREADS_PER_BLOCK_ON_L2NORM (32)
+#define THREADS_PER_BLOCK_ON_PINTGR_1 (64)
+#define THREADS_PER_BLOCK_ON_PINTGR_2 (64)
+#define THREADS_PER_BLOCK_ON_PINTGR_3 (64)
+#define THREADS_PER_BLOCK_ON_PINTGR_4 (64)
+#define THREADS_PER_BLOCK_ON_RHS_1 (32)
+#define THREADS_PER_BLOCK_ON_RHS_2 (32)
+#define THREADS_PER_BLOCK_ON_RHS_3 (32)
+#define THREADS_PER_BLOCK_ON_RHS_4 (32)
+#define THREADS_PER_BLOCK_ON_SETBV_1 (32)
+#define THREADS_PER_BLOCK_ON_SETBV_2 (32)
+#define THREADS_PER_BLOCK_ON_SETBV_3 (32)
+#define THREADS_PER_BLOCK_ON_SETIV (32)
+#define THREADS_PER_BLOCK_ON_SSOR_1 (32)
+#define THREADS_PER_BLOCK_ON_SSOR_2 (32)
+
 /* gpu linear pattern */
 #define u(m,i,j,k) u[(m)+5*((i)+nx*((j)+ny*(k)))]
 #define v(m,i,j,k) v[(m)+5*((i)+nx*((j)+ny*(k)))]
@@ -176,32 +201,9 @@ static size_t size_norm_buffer_device;
 static int nx;
 static int ny;
 static int nz;
-static int THREADS_PER_BLOCK_ON_ERHS_1;
-static int THREADS_PER_BLOCK_ON_ERHS_2;
-static int THREADS_PER_BLOCK_ON_ERHS_3;
-static int THREADS_PER_BLOCK_ON_ERHS_4;
-static int THREADS_PER_BLOCK_ON_ERROR;
-static int THREADS_PER_BLOCK_ON_NORM;
-static int THREADS_PER_BLOCK_ON_JACLD_BLTS;
-static int THREADS_PER_BLOCK_ON_JACU_BUTS;
-static int THREADS_PER_BLOCK_ON_L2NORM;
-static int THREADS_PER_BLOCK_ON_PINTGR_1;
-static int THREADS_PER_BLOCK_ON_PINTGR_2;
-static int THREADS_PER_BLOCK_ON_PINTGR_3;
-static int THREADS_PER_BLOCK_ON_PINTGR_4;
-static int THREADS_PER_BLOCK_ON_RHS_1;
-static int THREADS_PER_BLOCK_ON_RHS_2;
-static int THREADS_PER_BLOCK_ON_RHS_3;
-static int THREADS_PER_BLOCK_ON_RHS_4;
-static int THREADS_PER_BLOCK_ON_SETBV_1;
-static int THREADS_PER_BLOCK_ON_SETBV_2;
-static int THREADS_PER_BLOCK_ON_SETBV_3;
-static int THREADS_PER_BLOCK_ON_SETIV;
-static int THREADS_PER_BLOCK_ON_SSOR_1;
-static int THREADS_PER_BLOCK_ON_SSOR_2;
 int gpu_device_id;
 int total_devices;
-cudaDeviceProp gpu_device_properties;
+hipDeviceProp_t gpu_device_properties;
 extern __shared__ double extern_share_data[];
 
 namespace constants_device{
@@ -572,11 +574,6 @@ int main(int argc, char** argv){
 			verified,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
@@ -1039,7 +1036,7 @@ static void error_gpu(){
 	timer_stop(PROFILING_NORM);
 #endif
 
-	cudaMemcpy(errnm, norm_buffer_device, 5*sizeof(double), cudaMemcpyDeviceToHost);
+	hipMemcpy(errnm, norm_buffer_device, 5*sizeof(double), hipMemcpyDeviceToHost);
 	for(int m=0;m<5;m++){errnm[m]=sqrt(errnm[m]/((double)(nz-2)*(double)(ny-2)*(double)(nx-2)));}
 }
 
@@ -1653,7 +1650,7 @@ static void l2norm_gpu(const double* v,
 	timer_stop(PROFILING_NORM);
 #endif
 
-	cudaMemcpy(sum, norm_buffer_device, 5*sizeof(double), cudaMemcpyDeviceToHost);
+	hipMemcpy(sum, norm_buffer_device, 5*sizeof(double), hipMemcpyDeviceToHost);
 	for(int m=0;m<5;m++){sum[m]=sqrt(sum[m]/((double)(nz-2)*(double)(ny-2)*(double)(nx-2)));}
 }
 
@@ -1838,7 +1835,7 @@ static void pintgr_gpu(){
 	timer_stop(PROFILING_PINTGR_4);
 #endif
 
-	cudaMemcpy(&frc, norm_buffer_device, sizeof(double), cudaMemcpyDeviceToHost);
+	hipMemcpy(&frc, norm_buffer_device, sizeof(double), hipMemcpyDeviceToHost);
 }
 
 __global__ static void pintgr_gpu_kernel_1(const double* u,
@@ -2078,12 +2075,12 @@ void read_input(){
 }
 
 static void release_gpu(){
-	cudaFree(u_device);
-	cudaFree(rsd_device);
-	cudaFree(frct_device);
-	cudaFree(rho_i_device);
-	cudaFree(qs_device);
-	cudaFree(norm_buffer_device);
+	hipFree(u_device);
+	hipFree(rsd_device);
+	hipFree(frct_device);
+	hipFree(rho_i_device);
+	hipFree(qs_device);
+	hipFree(norm_buffer_device);
 }
 
 static void rhs_gpu(){
@@ -2876,37 +2873,37 @@ static void setcoeff_gpu(){
 	ce[11][4]=0.3;
 	ce[12][4]=0.2;
 	/* */
-	cudaMemcpyToSymbol(constants_device::dx1, &dx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx2, &dx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx3, &dx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx4, &dx4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx5, &dx5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy1, &dy1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy2, &dy2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy3, &dy3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy4, &dy4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy5, &dy5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz1, &dz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz2, &dz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz3, &dz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz4, &dz4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz5, &dz5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dssp, &dssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dxi, &dxi, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::deta, &deta, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dzeta, &dzeta, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx1, &tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx2, &tx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx3, &tx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty1, &ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty2, &ty2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty3, &ty3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz1, &tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz2, &tz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz3, &tz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ce, &ce, 13*5*sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dt, &dt, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::omega, &omega, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx1), &dx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx2), &dx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx3), &dx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx4), &dx4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx5), &dx5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy1), &dy1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy2), &dy2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy3), &dy3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy4), &dy4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy5), &dy5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz1), &dz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz2), &dz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz3), &dz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz4), &dz4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz5), &dz5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dssp), &dssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dxi), &dxi, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::deta), &deta, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dzeta), &dzeta, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx1), &tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx2), &tx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx3), &tx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty1), &ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty2), &ty2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty3), &ty3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz1), &tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz2), &tz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz3), &tz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ce), &ce, 13*5*sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dt), &dt, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::omega), &omega, sizeof(double));
 }
 
 /*
@@ -2972,7 +2969,7 @@ __global__ static void setiv_gpu_kernel(double* u,
 
 static void setup_gpu(){
 	/*
-	 * struct cudaDeviceProp{
+	 * struct hipDeviceProp_t{
 	 *  char name[256];
 	 *  size_t totalGlobalMem;
 	 *  size_t sharedMemPerBlock;
@@ -3001,183 +2998,17 @@ static void setup_gpu(){
 	 * }
 	 */
 	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);
+	hipGetDeviceCount(&total_devices);
 
 	/* define gpu_device */
 	if(total_devices==0){
 		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
 		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
 	}else{
 		gpu_device_id = 0;
 	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((LU_THREADS_PER_BLOCK_ON_ERHS_1>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_ERHS_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERHS_1 = LU_THREADS_PER_BLOCK_ON_ERHS_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERHS_1 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_ERHS_2>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_ERHS_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERHS_2 = LU_THREADS_PER_BLOCK_ON_ERHS_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERHS_2 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_ERHS_3>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_ERHS_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERHS_3 = LU_THREADS_PER_BLOCK_ON_ERHS_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERHS_3 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_ERHS_4>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_ERHS_4<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERHS_4 = LU_THREADS_PER_BLOCK_ON_ERHS_4;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERHS_4 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_ERROR>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_ERROR<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERROR = LU_THREADS_PER_BLOCK_ON_ERROR;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERROR = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_NORM>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_NORM<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_NORM = LU_THREADS_PER_BLOCK_ON_NORM;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_NORM = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_JACLD_BLTS>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_JACLD_BLTS<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_JACLD_BLTS = LU_THREADS_PER_BLOCK_ON_JACLD_BLTS;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_JACLD_BLTS = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_JACU_BUTS>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_JACU_BUTS<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_JACU_BUTS = LU_THREADS_PER_BLOCK_ON_JACU_BUTS;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_JACU_BUTS = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_L2NORM>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_L2NORM<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_L2NORM = LU_THREADS_PER_BLOCK_ON_L2NORM;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_L2NORM=gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_PINTGR_1>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_PINTGR_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_PINTGR_1 = LU_THREADS_PER_BLOCK_ON_PINTGR_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_PINTGR_1=gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_PINTGR_2>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_PINTGR_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_PINTGR_2 = LU_THREADS_PER_BLOCK_ON_PINTGR_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_PINTGR_2 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_PINTGR_3>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_PINTGR_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_PINTGR_3 = LU_THREADS_PER_BLOCK_ON_PINTGR_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_PINTGR_3 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_PINTGR_4>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_PINTGR_4<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_PINTGR_4 = LU_THREADS_PER_BLOCK_ON_PINTGR_4;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_PINTGR_4 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_RHS_1>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_RHS_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_1 = LU_THREADS_PER_BLOCK_ON_RHS_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_1 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_RHS_2>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_RHS_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_2 = LU_THREADS_PER_BLOCK_ON_RHS_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_2 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_RHS_3>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_RHS_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_3 = LU_THREADS_PER_BLOCK_ON_RHS_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_3 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_RHS_4>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_RHS_4<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_4 = LU_THREADS_PER_BLOCK_ON_RHS_4;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_4 = gpu_device_properties.warpSize;
-	}	
-	if((LU_THREADS_PER_BLOCK_ON_SETBV_1>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_SETBV_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_SETBV_1 = LU_THREADS_PER_BLOCK_ON_SETBV_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_SETBV_1 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_SETBV_2>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_SETBV_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_SETBV_2 = LU_THREADS_PER_BLOCK_ON_SETBV_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_SETBV_2 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_SETBV_3>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_SETBV_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_SETBV_3 = LU_THREADS_PER_BLOCK_ON_SETBV_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_SETBV_3 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_SETIV>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_SETIV<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_SETIV = LU_THREADS_PER_BLOCK_ON_SETIV;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_SETIV = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_SSOR_1>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_SSOR_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_SSOR_1 = LU_THREADS_PER_BLOCK_ON_SSOR_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_SSOR_1 = gpu_device_properties.warpSize;
-	}
-	if((LU_THREADS_PER_BLOCK_ON_SSOR_2>=1)&&
-			(LU_THREADS_PER_BLOCK_ON_SSOR_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_SSOR_2 = LU_THREADS_PER_BLOCK_ON_SSOR_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_SSOR_2 = gpu_device_properties.warpSize;
-	}
+	hipSetDevice(gpu_device_id);	
+	hipGetDeviceProperties(&gpu_device_properties, gpu_device_id);
 
 	int gridsize=nx*ny*nz;
 	int norm_buf_size=max(5*(ny-2)*(nz-2), ((nx-3)*(ny-3)+(nx-3)*(nz-3)+(ny-3)*(nz-3))/((gpu_device_properties.maxThreadsPerBlock-1)*(gpu_device_properties.maxThreadsPerBlock-1))+3);
@@ -3187,12 +3018,12 @@ static void setup_gpu(){
 	size_rho_i_device=sizeof(double)*(gridsize);
 	size_qs_device=sizeof(double)*(gridsize);
 	size_norm_buffer_device=sizeof(double)*(norm_buf_size);
-	cudaMalloc(&u_device, size_u_device);
-	cudaMalloc(&rsd_device, size_rsd_device);
-	cudaMalloc(&frct_device, size_frct_device);
-	cudaMalloc(&rho_i_device, size_rho_i_device);
-	cudaMalloc(&qs_device, size_qs_device);
-	cudaMalloc(&norm_buffer_device, size_norm_buffer_device);
+	hipMalloc(&u_device, size_u_device);
+	hipMalloc(&rsd_device, size_rsd_device);
+	hipMalloc(&frct_device, size_frct_device);
+	hipMalloc(&rho_i_device, size_rho_i_device);
+	hipMalloc(&qs_device, size_qs_device);
+	hipMalloc(&norm_buffer_device, size_norm_buffer_device);
 }
 
 /*
diff --git a/CUDA/LU/setup.ini b/CUDA/LU/setup.ini
new file mode 100644
index 0000000..b84d59a
--- /dev/null
+++ b/CUDA/LU/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/lu.S;../bin/lu.W;../bin/lu.A;../bin/lu.B
+use_clang_plugin = true
+clang_convert = make CLASS=A INJECT_CODE_CLANG=1
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/MG/Makefile b/CUDA/MG/Makefile
index d9af442..5a4514d 100644
--- a/CUDA/MG/Makefile
+++ b/CUDA/MG/Makefile
@@ -16,5 +16,5 @@ mg.o:		mg.cu npbparams.hpp
 	${CCOMPILE} mg.cu
 
 clean:
-	- rm -f *.o *~ 
+	- rm -f *.o *.bc seq.txt *~ 
 	- rm -f npbparams.hpp core
diff --git a/CUDA/MG/Makefile.all b/CUDA/MG/Makefile.all
new file mode 100644
index 0000000..30c54dc
--- /dev/null
+++ b/CUDA/MG/Makefile.all
@@ -0,0 +1,23 @@
+SHELL=/bin/sh
+BENCHMARK=mg
+BENCHMARKU=MG
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+llvm:
+	$(MAKE) CLASS=S INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=W INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=A INJECT_CODE_LLVM=1
+	$(MAKE) clean
+	$(MAKE) CLASS=B INJECT_CODE_LLVM=1
+	$(MAKE) clean 
diff --git a/CUDA/MG/mg.cu b/CUDA/MG/mg.cu
index 2f99b88..7f24ef6 100644
--- a/CUDA/MG/mg.cu
+++ b/CUDA/MG/mg.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -59,7 +60,9 @@
  * ------------------------------------------------------------------------------
  */
 
-#include <cuda.h>
+ /* NO CAST VERSION 2 */
+
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -71,14 +74,24 @@
 #define MM (10)
 #define	A (pow(5.0,13.0))
 #define	X (314159265.0)
-#define PROFILING_TOTAL_TIME (0)
-#define PROFILING_COMM3 (1)
-#define PROFILING_INTERP (2)
-#define PROFILING_NORM2U3 (3)
-#define PROFILING_PSINV (4)
-#define PROFILING_RESID (5)
-#define PROFILING_RPRJ3 (6)
-#define PROFILING_ZERO3 (7)
+#define T_INIT (0)
+#define T_BENCH (1)
+#define T_MG3P (2)
+#define T_PSINV (3)
+#define T_RESID (4)
+#define T_RESID2 (5)
+#define T_RPRJ3 (6)
+#define T_INTERP (7)
+#define T_NORM2 (8)
+#define T_COMM3 (9)
+#define T_LAST (10)
+#define THREADS_PER_BLOCK (1024) //1024
+#define THREADS_PER_BLOCK_ON_NORM2U3 (128) //128
+#define THREADS_PER_BLOCK_ON_COMM3 (32) //32
+#define THREADS_PER_BLOCK_ON_ZERO3 (1024) //1024
+//#define SHARED_2_M (2*M*sizeof(double))
+//#define SHARED_3_M (3*M*sizeof(double))
+//#define SHARED_2_NORM (2*THREADS_PER_BLOCK_ON_NORM2U3*sizeof(double))
 
 /* global variables */
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
@@ -107,7 +120,11 @@ static double (*v)=(double*)malloc(sizeof(double)*(NV));
 static double (*r)=(double*)malloc(sizeof(double)*(NR));
 #endif
 static int is1, is2, is3, ie1, ie2, ie3, lt, lb;
+static boolean timeron;
 /* gpu variables */
+int threads_per_block;
+int blocks_per_grid;
+int amount_of_work;
 size_t size_a_device;
 size_t size_c_device;
 size_t size_u_device;
@@ -118,22 +135,7 @@ double* c_device;
 double* u_device;
 double* v_device;
 double* r_device;
-int threads_per_block_on_comm3;
-int threads_per_block_on_interp;
-int threads_per_block_on_norm2u3;
-int threads_per_block_on_psinv;
-int threads_per_block_on_resid;
-int threads_per_block_on_rprj3;
-int threads_per_block_on_zero3;
-size_t size_shared_data_on_interp;
-size_t size_shared_data_on_norm2u3;
-size_t size_shared_data_on_psinv;
-size_t size_shared_data_on_resid;
-size_t size_shared_data_on_rprj3;
-int gpu_device_id;
-int total_devices;
-cudaDeviceProp gpu_device_properties;
-extern __shared__ double extern_share_data[];
+//extern __shared__ double extern_share_data[];
 
 /* function prototypes */
 static void bubble(double ten[][MM], 
@@ -184,8 +186,8 @@ static void interp_gpu(double* z_device,
 		int n2, 
 		int n3, 
 		int k);
-__global__ void interp_gpu_kernel(double* z_device,
-		double* u_device,
+__global__ void interp_gpu_kernel(double* base_z,
+		double* base_u,
 		int mm1, 
 		int mm2, 
 		int mm3,
@@ -309,8 +311,8 @@ static void rprj3_gpu(double* r_device,
 		int m2j, 
 		int m3j, 
 		int k);
-__global__ void rprj3_gpu_kernel(double* r_device,
-		double* s_device,
+__global__ void rprj3_gpu_kernel(double* base_r,
+		double* base_s,
 		int m1k,
 		int m2k,
 		int m3k,
@@ -356,9 +358,6 @@ static void zran3(void* pointer_z,
 int main(int argc, char** argv){
 #if defined(DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION)
 	printf(" DO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION mode on\n");
-#endif
-#if defined(PROFILING)
-	printf(" PROFILING mode on\n");
 #endif
 	/*
 	 * -------------------------------------------------------------------------
@@ -367,7 +366,7 @@ int main(int argc, char** argv){
 	 * -------------------------------------------------------------------------
 	 */
 	int k, it;
-	double t, mflops;
+	double t, tinit, mflops;
 
 	double a[4], c[4];
 
@@ -378,20 +377,73 @@ int main(int argc, char** argv){
 	char class_npb;
 
 	int i;
+	char* t_names[T_LAST];
+	double tmax;
+
+	for(i=T_INIT; i<T_LAST; i++){
+		timer_clear(i);
+	}
+
+	timer_start(T_INIT);	
 
 	/*
 	 * ----------------------------------------------------------------------
 	 * read in and broadcast input data
 	 * ----------------------------------------------------------------------
 	 */
-	lt = LT_DEFAULT;
-	nit = NIT_DEFAULT;
-	nx[lt] = NX_DEFAULT;
-	ny[lt] = NY_DEFAULT;
-	nz[lt] = NZ_DEFAULT;
-	for(i = 0; i <= 7; i++){
-		debug_vec[i] = DEBUG_DEFAULT;
+	FILE* fp;
+	if((fp = fopen("timer.flag", "r")) != NULL){
+		timeron = TRUE;
+		t_names[T_INIT] = (char*) "init";
+		t_names[T_BENCH] = (char*) "benchmk";
+		t_names[T_MG3P] = (char*) "mg3P";
+		t_names[T_PSINV] = (char*) "psinv";
+		t_names[T_RESID] = (char*) "resid";
+		t_names[T_RPRJ3] = (char*) "rprj3";
+		t_names[T_INTERP] = (char*) "interp";
+		t_names[T_NORM2] = (char*) "norm2";
+		t_names[T_COMM3] = (char*) "comm3";
+		fclose(fp);
+	}else{
+		timeron = FALSE;
+	}
+	fp = fopen("mg.input", "r");
+	if(fp != NULL){
+		printf(" Reading from input file mg.input\n");
+		if(fscanf(fp, "%d", &lt) != 1){
+			printf(" Error in reading elements\n");
+			exit(1);
+		}
+		while(fgetc(fp) != '\n');
+		if(fscanf(fp, "%d%d%d", &nx[lt], &ny[lt], &nz[lt]) != 3){
+			printf(" Error in reading elements\n");
+			exit(1);
+		}
+		while(fgetc(fp) != '\n');
+		if(fscanf(fp, "%d", &nit) != 1){
+			printf(" Error in reading elements\n");
+			exit(1);
+		}
+		while(fgetc(fp) != '\n');
+		for(i = 0; i <= 7; i++) {
+			if(fscanf(fp, "%d", &debug_vec[i]) != 1){
+				printf(" Error in reading elements\n");
+				exit(1);
+			}
+		}
+		fclose(fp);
+	}else{
+		printf(" No input file. Using compiled defaults\n");
+		lt = LT_DEFAULT;
+		nit = NIT_DEFAULT;
+		nx[lt] = NX_DEFAULT;
+		ny[lt] = NY_DEFAULT;
+		nz[lt] = NZ_DEFAULT;
+		for(i = 0; i <= 7; i++){
+			debug_vec[i] = DEBUG_DEFAULT;
+		}
 	}
+
 	if((nx[lt] != ny[lt]) || (nx[lt] != nz[lt])){
 		class_npb = 'U';
 	}else if(nx[lt] == 32 && nit == 4){
@@ -476,31 +528,36 @@ int main(int argc, char** argv){
 	zero3(u,n1,n2,n3);
 	zran3(v,n1,n2,n3,nx[lt],ny[lt],k);
 
-	setup_gpu(a,c);
+	timer_stop(T_INIT);
+	tinit = timer_read(T_INIT);
+	printf(" Initialization time: %15.3f seconds\n", tinit);
 
-	timer_clear(PROFILING_TOTAL_TIME);
-#if defined(PROFILING)
-	timer_clear(PROFILING_COMM3);
-	timer_clear(PROFILING_INTERP);
-	timer_clear(PROFILING_NORM2U3);
-	timer_clear(PROFILING_PSINV);
-	timer_clear(PROFILING_RESID);
-	timer_clear(PROFILING_RPRJ3);
-	timer_clear(PROFILING_ZERO3);
-#endif
+	for(i=T_BENCH; i<T_LAST; i++){
+		timer_clear(i);
+	} 
 
-	timer_start(PROFILING_TOTAL_TIME);
+	setup_gpu(a,c);
+
+	timer_start(T_BENCH);
 
-	resid_gpu(u_device,v_device,r_device,n1,n2,n3,a_device,k);	
+	if(timeron){timer_start(T_RESID2);}
+	resid_gpu(u_device,v_device,r_device,n1,n2,n3,a_device,k);
+	if(timeron){timer_stop(T_RESID2);}
 	norm2u3_gpu(r_device,n1,n2,n3,&rnm2,&rnmu,nx[lt],ny[lt],nz[lt]);
+
 	for(it = 1; it <= nit; it++){
+		//if((it==1)||(it==nit)||((it%5)==0)){printf("  iter %3d\n",it);}
+		if(timeron){timer_start(T_MG3P);}
 		mg3P_gpu(u_device,v_device,r_device,a_device,c_device,n1,n2,n3,k);
+		if(timeron){timer_stop(T_MG3P);}
+		if(timeron){timer_start(T_RESID2);}
 		resid_gpu(u_device,v_device,r_device,n1,n2,n3,a_device,k);
+		if(timeron){timer_stop(T_RESID2);}
 	}
 	norm2u3_gpu(r_device,n1,n2,n3,&rnm2,&rnmu,nx[lt],ny[lt],nz[lt]);
 
-	timer_stop(PROFILING_TOTAL_TIME);
-	t = timer_read(PROFILING_TOTAL_TIME);  	
+	timer_stop(T_BENCH);
+	t = timer_read(T_BENCH);  	
 
 	verified = FALSE;
 	verify_value = 0.0;	
@@ -550,44 +607,6 @@ int main(int argc, char** argv){
 		mflops = 0.0;
 	}
 
-	char gpu_config[256];
-	char gpu_config_string[2048];
-#if defined(PROFILING)
-	sprintf(gpu_config, "%5s\t%25s\t%25s\t%25s\n", "GPU Kernel", "Threads Per Block", "Time in Seconds", "Time in Percentage");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " comm3", threads_per_block_on_comm3, timer_read(PROFILING_COMM3), (timer_read(PROFILING_COMM3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " interp", threads_per_block_on_interp, timer_read(PROFILING_INTERP), (timer_read(PROFILING_INTERP)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " norm2u3", threads_per_block_on_norm2u3, timer_read(PROFILING_NORM2U3), (timer_read(PROFILING_NORM2U3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " psinv", threads_per_block_on_psinv, timer_read(PROFILING_PSINV), (timer_read(PROFILING_PSINV)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " resid", threads_per_block_on_resid, timer_read(PROFILING_RESID), (timer_read(PROFILING_RESID)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " rprj3", threads_per_block_on_rprj3, timer_read(PROFILING_RPRJ3), (timer_read(PROFILING_RPRJ3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\t%25f\t%24.2f%%\n", " zero3", threads_per_block_on_zero3, timer_read(PROFILING_ZERO3), (timer_read(PROFILING_ZERO3)*100/timer_read(PROFILING_TOTAL_TIME)));
-	strcat(gpu_config_string, gpu_config);
-#else
-	sprintf(gpu_config, "%5s\t%25s\n", "GPU Kernel", "Threads Per Block");
-	strcpy(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " comm3", threads_per_block_on_comm3);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " interp", threads_per_block_on_interp);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " norm2u3", threads_per_block_on_norm2u3);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " psinv", threads_per_block_on_psinv);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " resid", threads_per_block_on_resid);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " rprj3", threads_per_block_on_rprj3);
-	strcat(gpu_config_string, gpu_config);
-	sprintf(gpu_config, "%29s\t%25d\n", " zero3", threads_per_block_on_zero3);
-	strcat(gpu_config_string, gpu_config);
-#endif
-
 	c_print_results((char*)"MG",
 			class_npb,
 			nx[lt],
@@ -600,11 +619,6 @@ int main(int argc, char** argv){
 			verified,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			(char*)gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
@@ -613,6 +627,26 @@ int main(int argc, char** argv){
 			(char*)CS6,
 			(char*)CS7);
 
+	/*
+	 * ---------------------------------------------------------------------
+	 * more timers
+	 * ---------------------------------------------------------------------
+	 */
+	if(timeron){
+		tmax = timer_read(T_BENCH);
+		if(tmax==0.0){tmax=1.0;}
+		printf("  SECTION   Time (secs)\n");
+		for(i=T_BENCH; i<T_LAST; i++){
+			t = timer_read(i);
+			if(i==T_RESID2){
+				t = timer_read(T_RESID) - t;
+				printf("    --> %8s:%9.3f  (%6.2f%%)\n", "mg-resid", t, t*100.0/tmax);
+			}else{
+				printf("  %-8s:%9.3f  (%6.2f%%)\n", t_names[i], t, t*100.0/tmax);
+			}
+		}
+	}
+
 	release_gpu();
 
 	return 0;
@@ -689,30 +723,42 @@ static void comm3(void* pointer_u,
 		int n2, 
 		int n3, 
 		int kk){
-	double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;
+	//double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;
+	double* pointer_aux_u = (double*)pointer_u;
 
 	int i1, i2, i3;
+	if(timeron){timer_start(T_COMM3);}
 	/* axis = 1 */
 	for(i3 = 1; i3 < n3-1; i3++){
 		for(i2 = 1; i2 < n2-1; i2++){
-			u[i3][i2][0] = u[i3][i2][n1-2];
-			u[i3][i2][n1-1] = u[i3][i2][1];			
+			//u[i3][i2][0] = u[i3][i2][n1-2];
+			pointer_aux_u[(i3)*n1*n2 + (i2)*n1 + (0)] = u[(i3)*n1*n2 + (i2)*n1 + (n1-2)];
+
+			//u[i3][i2][n1-1] = u[i3][i2][1];
+			pointer_aux_u[(i3)*n1*n2 + (i2)*n1 + (n1-1)] = pointer_aux_u[(i3)*n1*n2 + (i2)*n1 + (1)];
 		}
 	}
 	/* axis = 2 */
 	for(i3 = 1; i3 < n3-1; i3++){
 		for(i1 = 0; i1 < n1; i1++){
-			u[i3][0][i1] = u[i3][n2-2][i1];
-			u[i3][n2-1][i1] = u[i3][1][i1];			
+			//u[i3][0][i1] = u[i3][n2-2][i1];
+			pointer_aux_u[(i3)*n1*n2 + (0)*n1 + (i1)] = pointer_aux_u[(i3)*n1*n2 + (n2-2)*n1 + (i1)];
+
+			//u[i3][n2-1][i1] = u[i3][1][i1];
+			pointer_aux_u[(i3)*n1*n2 + (n2-1)*n1 + (i1)] = pointer_aux_u[(i3)*n1*n2 + (1)*n1 + (i1)];
 		}
 	}
 	/* axis = 3 */
 	for(i2 = 0; i2 < n2; i2++){
 		for(i1 = 0; i1 < n1; i1++){
-			u[0][i2][i1] = u[n3-2][i2][i1];
-			u[n3-1][i2][i1] = u[1][i2][i1];			
+			//u[0][i2][i1] = u[n3-2][i2][i1];
+			pointer_aux_u[(0)*n1*n2 + (i2)*n1 + (i1)] = pointer_aux_u[(n3-2)*n1*n2 + (i2)*n1 + (i1)];
+
+			//u[n3-1][i2][i1] = u[1][i2][i1];	
+			pointer_aux_u[(n3-1)*n1*n2 + (i2)*n1 + (i1)] = pointer_aux_u[(1)*n1*n2 + (i2)*n1 + (i1)];
 		}
 	}
+	if(timeron){timer_stop(T_COMM3);}
 }
 
 static void comm3_gpu(double* u_device, 
@@ -720,63 +766,42 @@ static void comm3_gpu(double* u_device,
 		int n2, 
 		int n3, 
 		int kk){
-#if defined(PROFILING)
-	timer_start(PROFILING_COMM3);
-#endif
+	if(timeron){timer_start(T_COMM3);}
 
-	/* axis = 1 */
-	int threads_per_block_x=threads_per_block_on_comm3;
-	int threads_per_block_y=1;
-	int amount_of_work_x=(ceil(double(n2-2)/double(threads_per_block_x)))*threads_per_block_x;
-	int amount_of_work_y=n3-2;
-	dim3 threadsPerBlock(threads_per_block_x,
-			threads_per_block_y,
-			1);
-	dim3 blocksPerGrid(ceil(double(amount_of_work_x)/double(threadsPerBlock.x)),
-			ceil(double(amount_of_work_y)/double(threadsPerBlock.y)),
-			1);
-	comm3_gpu_kernel_1<<<blocksPerGrid,
-		threadsPerBlock>>>(u_device,
-				n1,
-				n2,
-				n3,
-				amount_of_work_x);
+	int threads_per_block = THREADS_PER_BLOCK_ON_COMM3;
+	int amount_of_work = (n3-2) * THREADS_PER_BLOCK_ON_COMM3;
+	int blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
 
-	/* axis = 2 */
-	threads_per_block_x=threads_per_block_on_comm3;
-	threads_per_block_y=1;
-	amount_of_work_x=(ceil(double(n1)/double(threads_per_block_x)))*threads_per_block_x;
-	amount_of_work_y=n3-2;
-	threadsPerBlock.x=threads_per_block_x;
-	threadsPerBlock.y=threads_per_block_y;
-	blocksPerGrid.x=ceil(double(amount_of_work_x)/double(threadsPerBlock.x));
-	blocksPerGrid.y=ceil(double(amount_of_work_y)/double(threadsPerBlock.y));
-	comm3_gpu_kernel_2<<<blocksPerGrid,	
-		threadsPerBlock>>>(u_device,
-				n1,
-				n2,
-				n3,
-				amount_of_work_x);
+	comm3_gpu_kernel_1<<<blocks_per_grid,threads_per_block>>>(u_device,
+			n1,
+			n2,
+			n3,
+			amount_of_work);
+	hipDeviceSynchronize();
 
-	/* axis = 3 */
-	threads_per_block_x=threads_per_block_on_comm3;
-	threads_per_block_y=1;
-	amount_of_work_x=(ceil(double(n1)/double(threads_per_block_x)))*threads_per_block_x;
-	amount_of_work_y=n2;
-	threadsPerBlock.x=threads_per_block_x;
-	threadsPerBlock.y=threads_per_block_y;
-	blocksPerGrid.x=ceil(double(amount_of_work_x) / double(threadsPerBlock.x));
-	blocksPerGrid.y=ceil(double(amount_of_work_y) / double(threadsPerBlock.y));
-	comm3_gpu_kernel_3<<<blocksPerGrid,
-		threadsPerBlock>>>(u_device,
-				n1,
-				n2,
-				n3,
-				amount_of_work_x);
-
-#if defined(PROFILING)
-	timer_stop(PROFILING_COMM3);
-#endif
+	threads_per_block = THREADS_PER_BLOCK_ON_COMM3;
+	amount_of_work = (n3-2) * THREADS_PER_BLOCK_ON_COMM3;	
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+	comm3_gpu_kernel_2<<<blocks_per_grid,threads_per_block>>>(u_device,
+			n1,
+			n2,
+			n3,
+			amount_of_work);
+	hipDeviceSynchronize();
+
+	threads_per_block = THREADS_PER_BLOCK_ON_COMM3;
+	amount_of_work = n2 * THREADS_PER_BLOCK_ON_COMM3;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+	comm3_gpu_kernel_3<<<blocks_per_grid,threads_per_block>>>(u_device,
+			n1,
+			n2,
+			n3,
+			amount_of_work);
+	hipDeviceSynchronize();
+
+	if(timeron){timer_stop(T_COMM3);}
 }
 
 __global__ void comm3_gpu_kernel_1(double* u, 
@@ -784,13 +809,17 @@ __global__ void comm3_gpu_kernel_1(double* u,
 		int n2, 
 		int n3, 
 		int amount_of_work){
-	int i3=blockIdx.y*blockDim.y+threadIdx.y+1;
-	int i2=blockIdx.x*blockDim.x+threadIdx.x+1;
+	int check=blockIdx.x*blockDim.x+threadIdx.x;
+	if(check>=amount_of_work){return;}
 
-	if(i2>=n2-1){return;}
+	int i3=blockIdx.x+1;
+	int i2=threadIdx.x+1;
 
-	u[i3*n2*n1+i2*n1+0]=u[i3*n2*n1+i2*n1+n1-2];
-	u[i3*n2*n1+i2*n1+n1-1]=u[i3*n2*n1+i2*n1+1];
+	while(i2<n2-1){
+		u[i3*n2*n1+i2*n1+0]=u[i3*n2*n1+i2*n1+n1-2];
+		u[i3*n2*n1+i2*n1+n1-1]=u[i3*n2*n1+i2*n1+1];
+		i2+=THREADS_PER_BLOCK_ON_COMM3;
+	}
 }
 
 __global__ void comm3_gpu_kernel_2(double* u,
@@ -798,13 +827,17 @@ __global__ void comm3_gpu_kernel_2(double* u,
 		int n2,
 		int n3,
 		int amount_of_work){
-	int i3=blockIdx.y*blockDim.y+threadIdx.y+1;
-	int i1=blockIdx.x*blockDim.x+threadIdx.x;
+	int check=blockIdx.x*blockDim.x+threadIdx.x;
+	if(check>=amount_of_work){return;}
 
-	if(i1>=n1){return;}
+	int i3=blockIdx.x + 1;
+	int i1=threadIdx.x;
 
-	u[i3*n2*n1+0*n1+i1]=u[i3*n2*n1+(n2-2)*n1+i1];
-	u[i3*n2*n1+(n2-1)*n1+i1]=u[i3*n2*n1+1*n1+i1];
+	while(i1<n1){
+		u[i3*n2*n1+0*n1+i1]=u[i3*n2*n1+(n2-2)*n1+i1];
+		u[i3*n2*n1+(n2-1)*n1+i1]=u[i3*n2*n1+1*n1+i1];
+		i1+=THREADS_PER_BLOCK_ON_COMM3;
+	}
 }
 
 __global__ void comm3_gpu_kernel_3(double* u,
@@ -812,13 +845,17 @@ __global__ void comm3_gpu_kernel_3(double* u,
 		int n2, 
 		int n3, 
 		int amount_of_work){
-	int i2=blockIdx.y*blockDim.y+threadIdx.y;
-	int i1=blockIdx.x*blockDim.x+threadIdx.x;
+	int check=blockIdx.x*blockDim.x+threadIdx.x;
+	if(check>=amount_of_work){return;}
 
-	if(i1>=n1){return;}
+	int i2=blockIdx.x;
+	int i1=threadIdx.x;
 
-	u[0*n2*n1+i2*n1+i1]=u[(n3-2)*n2*n1+i2*n1+i1];
-	u[(n3-1)*n2*n1+i2*n1+i1]=u[1*n2*n1+i2*n1+i1];
+	while(i1<n1){
+		u[0*n2*n1+i2*n1+i1]=u[(n3-2)*n2*n1+i2*n1+i1];
+		u[(n3-1)*n2*n1+i2*n1+i1]=u[1*n2*n1+i2*n1+i1];
+		i1+=THREADS_PER_BLOCK_ON_COMM3;
+	}
 }
 
 /*
@@ -841,9 +878,11 @@ static void interp(void* pointer_z,
 		int n1, 
 		int n2, 
 		int n3, 
-		int k){
-	double (*z)[mm2][mm1] = (double (*)[mm2][mm1])pointer_z;
-	double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;	
+		int k){	
+	//double (*z)[mm2][mm1] = (double (*)[mm2][mm1])pointer_z;
+	//double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;
+	double* pointer_aux_z = (double*)pointer_z;
+	double* pointer_aux_u = (double*)pointer_u;
 
 	int i3, i2, i1, d1, d2, d3, t1, t2, t3;
 
@@ -857,37 +896,71 @@ static void interp(void* pointer_z,
 	 */
 	double z1[M], z2[M], z3[M];
 
+	if(timeron){timer_start(T_INTERP);}
 	if(n1 != 3 && n2 != 3 && n3 != 3){
 		for(i3 = 0; i3 < mm3-1; i3++){
 			for(i2 = 0; i2 < mm2-1; i2++){
 				for(i1 = 0; i1 < mm1; i1++){
-					z1[i1] = z[i3][i2+1][i1] + z[i3][i2][i1];
-					z2[i1] = z[i3+1][i2][i1] + z[i3][i2][i1];
-					z3[i1] = z[i3+1][i2+1][i1] + z[i3+1][i2][i1] + z1[i1];
+					//z1[i1] = z[i3][i2+1][i1] + z[i3][i2][i1];
+					z1[i1] = pointer_aux_z[(i3)*mm1*mm2 + (i2+1)*mm1 + (i1)] + pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1)];
+
+					//z2[i1] = z[i3+1][i2][i1] + z[i3][i2][i1];
+					z2[i1] = pointer_aux_z[(i3+1)*mm1*mm2 + (i2)*mm1 + (i1)] + pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1)];
+
+					//z3[i1] = z[i3+1][i2+1][i1] + z[i3+1][i2][i1] + z1[i1];
+					z3[i1] = pointer_aux_z[(i3+1)*mm1*mm2 + (i2+1)*mm1 + (i1)] + pointer_aux_z[(i3+1)*mm1*mm2 + (i2)*mm1 + (i1)] + z1[i1];
 				}
 				for(i1 = 0; i1 < mm1-1; i1++){
-					u[2*i3][2*i2][2*i1] = u[2*i3][2*i2][2*i1]
-						+z[i3][i2][i1];
-					u[2*i3][2*i2][2*i1+1] = u[2*i3][2*i2][2*i1+1]
-						+0.5*(z[i3][i2][i1+1]+z[i3][i2][i1]);
+					//u[2*i3][2*i2][2*i1] = u[2*i3][2*i2][2*i1] + z[i3][i2][i1];
+					pointer_aux_u[(2*i3)*n1*n2 +(2*i2)*n1 + (2*i1)] = 
+						pointer_aux_u[(2*i3)*n1*n2 +(2*i2)*n1 + (2*i1)] + 
+						pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1)];
+
+					//u[2*i3][2*i2][2*i1+1] = u[2*i3][2*i2][2*i1+1] + 0.5 * (z[i3][i2][i1+1] + z[i3][i2][i1]);
+					pointer_aux_u[(2*i3)*n1*n2 + (2*i2)*n1 + (2*i1+1)] = 
+						pointer_aux_u[(2*i3)*n1*n2 + (2*i2)*n1 + (2*i1+1)] + 
+						0.5 * 
+						(pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1+1)] + 
+						 pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1)]);
 				}
 				for(i1 = 0; i1 < mm1-1; i1++){
-					u[2*i3][2*i2+1][2*i1] = u[2*i3][2*i2+1][2*i1]
-						+0.5 * z1[i1];
-					u[2*i3][2*i2+1][2*i1+1] = u[2*i3][2*i2+1][2*i1+1]
-						+0.25*( z1[i1] + z1[i1+1] );
+					//u[2*i3][2*i2+1][2*i1] = u[2*i3][2*i2+1][2*i1] + 0.5 * z1[i1];
+					pointer_aux_u[(2*i3)*n1*n2 + (2*i2+1)*n1 + (2*i1)] = 
+						pointer_aux_u[(2*i3)*n1*n2 + (2*i2+1)*n1 + (2*i1)] + 
+						0.5 * 
+						z1[i1];
+
+					//u[2*i3][2*i2+1][2*i1+1] = u[2*i3][2*i2+1][2*i1+1] + 0.25 * ( z1[i1] + z1[i1+1] );
+					pointer_aux_u[(2*i3)*n1*n2 + (2*i2+1)*n1 + (2*i1+1)] = 
+						pointer_aux_u[(2*i3)*n1*n2 + (2*i2+1)*n1 + (2*i1+1)] + 
+						0.25 * 
+						( z1[i1] + z1[i1+1] );
 				}
 				for(i1 = 0; i1 < mm1-1; i1++){
-					u[2*i3+1][2*i2][2*i1] = u[2*i3+1][2*i2][2*i1]
-						+0.5 * z2[i1];
-					u[2*i3+1][2*i2][2*i1+1] = u[2*i3+1][2*i2][2*i1+1]
-						+0.25*( z2[i1] + z2[i1+1] );
+					//u[2*i3+1][2*i2][2*i1] = u[2*i3+1][2*i2][2*i1] + 0.5 * z2[i1];
+					pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2)*n1 + (2*i1)] = 
+						pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2)*n1 + (2*i1)] + 
+						0.5 * 
+						z2[i1];
+
+					//u[2*i3+1][2*i2][2*i1+1] = u[2*i3+1][2*i2][2*i1+1] + 0.25 * ( z2[i1] + z2[i1+1] );
+					pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2)*n1 + (2*i1+1)] = 
+						pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2)*n1 + (2*i1+1)] + 
+						0.25 * 
+						( z2[i1] + z2[i1+1] );
 				}
 				for(i1 = 0; i1 < mm1-1; i1++){
-					u[2*i3+1][2*i2+1][2*i1] = u[2*i3+1][2*i2+1][2*i1]
-						+0.25* z3[i1];
-					u[2*i3+1][2*i2+1][2*i1+1] = u[2*i3+1][2*i2+1][2*i1+1]
-						+0.125*( z3[i1] + z3[i1+1] );
+					//u[2*i3+1][2*i2+1][2*i1] = u[2*i3+1][2*i2+1][2*i1] + 0.25 * z3[i1];
+					pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2+1)*n1 + (2*i1)] = 
+						pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2+1)*n1 + (2*i1)] + 
+						0.25 * 
+						z3[i1];
+
+					//u[2*i3+1][2*i2+1][2*i1+1] = u[2*i3+1][2*i2+1][2*i1+1] + 0.125 * ( z3[i1] + z3[i1+1] );
+					pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2+1)*n1 + (2*i1+1)] = 
+						pointer_aux_u[(2*i3+1)*n1*n2 + (2*i2+1)*n1 + (2*i1+1)] + 
+						0.125 * 
+						( z3[i1] + z3[i1+1] );
 				}
 			}
 		}
@@ -916,70 +989,99 @@ static void interp(void* pointer_z,
 		for(i3 = d3; i3 <= mm3-1; i3++){
 			for(i2 = d2; i2 <= mm2-1; i2++){
 				for(i1 = d1; i1 <= mm1-1; i1++){
-					u[2*i3-d3-1][2*i2-d2-1][2*i1-d1-1] =
-						u[2*i3-d3-1][2*i2-d2-1][2*i1-d1-1]
-						+z[i3-1][i2-1][i1-1];
+					//u[2*i3-d3-1][2*i2-d2-1][2*i1-d1-1] = u[2*i3-d3-1][2*i2-d2-1][2*i1-d1-1] + z[i3-1][i2-1][i1-1];
+					pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-d1-1)] = 
+						pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-d1-1)] + 
+						pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)];
 				}
 				for(i1 = 1; i1 <= mm1-1; i1++){
-					u[2*i3-d3-1][2*i2-d2-1][2*i1-t1-1] =
-						u[2*i3-d3-1][2*i2-d2-1][2*i1-t1-1]
-						+0.5*(z[i3-1][i2-1][i1]+z[i3-1][i2-1][i1-1]);
+					//u[2*i3-d3-1][2*i2-d2-1][2*i1-t1-1] = u[2*i3-d3-1][2*i2-d2-1][2*i1-t1-1] + 0.5 * (z[i3-1][i2-1][i1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-t1-1)] = 
+						pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-t1-1)] + 
+						0.5 * 
+						(pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 			}
 			for(i2 = 1; i2 <= mm2-1; i2++){
-				for ( i1 = d1; i1 <= mm1-1; i1++) {
-					u[2*i3-d3-1][2*i2-t2-1][2*i1-d1-1] =
-						u[2*i3-d3-1][2*i2-t2-1][2*i1-d1-1]
-						+0.5*(z[i3-1][i2][i1-1]+z[i3-1][i2-1][i1-1]);
+				for( i1 = d1; i1 <= mm1-1; i1++){
+					//u[2*i3-d3-1][2*i2-t2-1][2*i1-d1-1] = u[2*i3-d3-1][2*i2-t2-1][2*i1-d1-1] + 0.5 * (z[i3-1][i2][i1-1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-d1-1)] = 
+						pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-d1-1)] + 
+						0.5 * 
+						(pointer_aux_z[(i3-1)*mm1*mm2 + (i2)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 				for(i1 = 1; i1 <= mm1-1; i1++){
-					u[2*i3-d3-1][2*i2-t2-1][2*i1-t1-1] =
-						u[2*i3-d3-1][2*i2-t2-1][2*i1-t1-1]
-						+0.25*(z[i3-1][i2][i1]+z[i3-1][i2-1][i1]
-								+z[i3-1][i2][i1-1]+z[i3-1][i2-1][i1-1]);
+					//u[2*i3-d3-1][2*i2-t2-1][2*i1-t1-1] = u[2*i3-d3-1][2*i2-t2-1][2*i1-t1-1] + 0.25 * (z[i3-1][i2][i1] + z[i3-1][i2-1][i1] + z[i3-1][i2][i1-1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-t1-1)] = 
+						pointer_aux_u[(2*i3-d3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-t1-1)] + 
+						0.25 * 
+						(pointer_aux_z[(i3-1)*mm1*mm2 + (i2)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 			}
 		}
 		for(i3 = 1; i3 <= mm3-1; i3++){
 			for(i2 = d2; i2 <= mm2-1; i2++){
 				for(i1 = d1; i1 <= mm1-1; i1++){
-					u[2*i3-t3-1][2*i2-d2-1][2*i1-d1-1] =
-						u[2*i3-t3-1][2*i2-d2-1][2*i1-d1-1]
-						+0.5*(z[i3][i2-1][i1-1]+z[i3-1][i2-1][i1-1]);
+					//u[2*i3-t3-1][2*i2-d2-1][2*i1-d1-1] = u[2*i3-t3-1][2*i2-d2-1][2*i1-d1-1] + 0.5 * (z[i3][i2-1][i1-1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-d1-1)] = 
+						pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-d1-1)] + 
+						0.5 * 
+						(pointer_aux_z[(i3)*mm1*mm2 + (i2-1)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 				for(i1 = 1; i1 <= mm1-1; i1++){
-					u[2*i3-t3-1][2*i2-d2-1][2*i1-t1-1] =
-						u[2*i3-t3-1][2*i2-d2-1][2*i1-t1-1]
-						+0.25*(z[i3][i2-1][i1]+z[i3][i2-1][i1-1]
-								+z[i3-1][i2-1][i1]+z[i3-1][i2-1][i1-1]);
+					//u[2*i3-t3-1][2*i2-d2-1][2*i1-t1-1] = u[2*i3-t3-1][2*i2-d2-1][2*i1-t1-1] + 0.25 * (z[i3][i2-1][i1] + z[i3][i2-1][i1-1] + z[i3-1][i2-1][i1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-t1-1)] = 
+						pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-d2-1)*n1 + (2*i1-t1-1)] + 
+						0.25 * 
+						(pointer_aux_z[(i3)*mm1*mm2 + (i2-1)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3)*mm1*mm2 + (i2-1)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 			}
 			for(i2 = 1; i2 <= mm2-1; i2++){
 				for (i1 = d1; i1 <= mm1-1; i1++){
-					u[2*i3-t3-1][2*i2-t2-1][2*i1-d1-1] =
-						u[2*i3-t3-1][2*i2-t2-1][2*i1-d1-1]
-						+0.25*(z[i3][i2][i1-1]+z[i3][i2-1][i1-1]
-								+z[i3-1][i2][i1-1]+z[i3-1][i2-1][i1-1]);
+					//u[2*i3-t3-1][2*i2-t2-1][2*i1-d1-1] = u[2*i3-t3-1][2*i2-t2-1][2*i1-d1-1] + 0.25 * (z[i3][i2][i1-1] + z[i3][i2-1][i1-1] + z[i3-1][i2][i1-1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-d1-1)] = 
+						pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-d1-1)] + 
+						0.25 * 
+						(pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3)*mm1*mm2 + (i2-1)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 				for(i1 = 1; i1 <= mm1-1; i1++){
-					u[2*i3-t3-1][2*i2-t2-1][2*i1-t1-1] =
-						u[2*i3-t3-1][2*i2-t2-1][2*i1-t1-1]
-						+0.125*(z[i3][i2][i1]+z[i3][i2-1][i1]
-								+z[i3][i2][i1-1]+z[i3][i2-1][i1-1]
-								+z[i3-1][i2][i1]+z[i3-1][i2-1][i1]
-								+z[i3-1][i2][i1-1]+z[i3-1][i2-1][i1-1]);
+					//u[2*i3-t3-1][2*i2-t2-1][2*i1-t1-1] = u[2*i3-t3-1][2*i2-t2-1][2*i1-t1-1] + 0.125 * (z[i3][i2][i1] + z[i3][i2-1][i1] + z[i3][i2][i1-1] + z[i3][i2-1][i1-1] + z[i3-1][i2][i1] + z[i3-1][i2-1][i1] + z[i3-1][i2][i1-1] + z[i3-1][i2-1][i1-1]);
+					pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-t1-1)] = 
+						pointer_aux_u[(2*i3-t3-1)*n1*n2 + (2*i2-t2-1)*n1 + (2*i1-t1-1)] + 
+						0.125 * 
+						(pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3)*mm1*mm2 + (i2-1)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3)*mm1*mm2 + (i2)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3)*mm1*mm2 + (i2-1)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2)*mm1 + (i1-1)] + 
+						 pointer_aux_z[(i3-1)*mm1*mm2 + (i2-1)*mm1 + (i1-1)]);
 				}
 			}
 		}
 	}
+	if(timeron){timer_stop(T_INTERP);}
 
 	if(debug_vec[0] >= 1){
-		rep_nrm(z,mm1,mm2,mm3,(char*)"z: inter",k-1);
-		rep_nrm(u,n1,n2,n3,(char*)"u: inter",k);
+		rep_nrm(pointer_aux_z,mm1,mm2,mm3,(char*)"z: inter",k-1);
+		rep_nrm(pointer_aux_u,n1,n2,n3,(char*)"u: inter",k);
 	}
 	if(debug_vec[5] >= k){
-		showall(z,mm1,mm2,mm3);
-		showall(u,n1,n2,n3);
+		showall(pointer_aux_z,mm1,mm2,mm3);
+		showall(pointer_aux_u,n1,n2,n3);
 	}
 }
 
@@ -992,48 +1094,32 @@ static void interp_gpu(double* z_device,
 		int n2, 
 		int n3, 
 		int k){
-#if defined(PROFILING)
-	timer_start(PROFILING_INTERP);
-#endif
-
+	if(timeron){timer_start(T_INTERP);}
 	if(n1 != 3 && n2 != 3 && n3 != 3){
-		int amount_of_work_x=(mm2-1)*mm1;
-		int amount_of_work_y=mm3-1;
-		int threads_per_block_x;
-		int threads_per_block_y=1;
-		if(threads_per_block_on_interp != mm1){
-			threads_per_block_x = mm1;
-		}
-		else{
-			threads_per_block_x = threads_per_block_on_interp;
-		}
-		dim3 threadsPerBlock(threads_per_block_x,
-				threads_per_block_y,
-				1);
-		dim3 blocksPerGrid(ceil(double(amount_of_work_x)/double(threadsPerBlock.x)),
-				ceil(double(amount_of_work_y)/double(threadsPerBlock.y)),
-				1);
-		interp_gpu_kernel<<<blocksPerGrid, 
-			threadsPerBlock,
-			size_shared_data_on_interp>>>(
-					z_device,
-					u_device,
-					mm1,
-					mm2,
-					mm3,
-					n1,
-					n2,
-					n3,
-					amount_of_work_x);		
+		threads_per_block = mm1;
+		amount_of_work = (mm3-1) * (mm2-1) * mm1;	
+		blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+		interp_gpu_kernel<<<blocks_per_grid, 
+			threads_per_block
+				//,SHARED_3_M
+				>>>(
+						z_device,
+						u_device,
+						mm1,
+						mm2,
+						mm3,
+						n1,
+						n2,
+						n3,
+						amount_of_work);
+		hipDeviceSynchronize();
 	}
-
-#if defined(PROFILING)
-	timer_stop(PROFILING_INTERP);
-#endif
+	if(timeron){timer_stop(T_INTERP);}
 }
 
-__global__ void interp_gpu_kernel(double* z_device,
-		double* u_device,
+__global__ void interp_gpu_kernel(double* base_z,
+		double* base_u,
 		int mm1, 
 		int mm2, 
 		int mm3,
@@ -1042,31 +1128,38 @@ __global__ void interp_gpu_kernel(double* z_device,
 		int n3,
 		int amount_of_work){
 	int check=blockIdx.x*blockDim.x+threadIdx.x;
-	if(check>=amount_of_work){return;}
+	if(check>=amount_of_work){return;}	
 
-	double* z1 = (double*)(extern_share_data);
-	double* z2 = (double*)(z1+M);
-	double* z3 = (double*)(z2+M);
+	int i3,i2,i1;
 
-	int i3=blockIdx.y*blockDim.y+threadIdx.y;
-	int i2=blockIdx.x;
-	int i1=threadIdx.x;
+	__shared__ double z1[M],z2[M],z3[M];
+	//double* z1 = (double*)(extern_share_data);
+	//double* z2 = (double*)(&z1[M]);
+	//double* z3 = (double*)(&z2[M]);
 
-	z1[i1]=z_device[i3*mm2*mm1+(i2+1)*mm1+i1]+z_device[i3*mm2*mm1+i2*mm1+i1];
-	z2[i1]=z_device[(i3+1)*mm2*mm1+i2*mm1+i1]+z_device[i3*mm2*mm1+i2*mm1+i1];
-	z3[i1]=z_device[(i3+1)*mm2*mm1+(i2+1)*mm1+i1]+z_device[(i3+1)*mm2*mm1+i2*mm1+i1]+z1[i1];
+	double (*z)=base_z;
+	double (*u)=base_u;
+
+	i3=blockIdx.x/(mm2-1);
+	i2=blockIdx.x%(mm2-1);
+	i1=threadIdx.x;
+
+	z1[i1]=z[i3*mm2*mm1+(i2+1)*mm1+i1]+z[i3*mm2*mm1+i2*mm1+i1];
+	z2[i1]=z[(i3+1)*mm2*mm1+i2*mm1+i1]+z[i3*mm2*mm1+i2*mm1+i1];
+	z3[i1]=z[(i3+1)*mm2*mm1+(i2+1)*mm1+i1] 
+		+z[(i3+1)*mm2*mm1+i2*mm1+i1]+z1[i1];
 
 	__syncthreads();
 	if(i1<mm1-1){
-		double z321=z_device[i3*mm2*mm1+i2*mm1+i1];
-		u_device[2*i3*n2*n1+2*i2*n1+2*i1]+=z321;
-		u_device[2*i3*n2*n1+2*i2*n1+2*i1+1]+=0.5*(z_device[i3*mm2*mm1+i2*mm1+i1+1]+z321);
-		u_device[2*i3*n2*n1+(2*i2+1)*n1+2*i1]+=0.5*z1[i1];
-		u_device[2*i3*n2*n1+(2*i2+1)*n1+2*i1+1]+=0.25*(z1[i1]+z1[i1+1]);
-		u_device[(2*i3+1)*n2*n1+2*i2*n1+2*i1]+=0.5*z2[i1];
-		u_device[(2*i3+1)*n2*n1+2*i2*n1+2*i1+1]+=0.25*(z2[i1]+z2[i1+1]);
-		u_device[(2*i3+1)*n2*n1+(2*i2+1)*n1+2*i1]+=0.25*z3[i1];
-		u_device[(2*i3+1)*n2*n1+(2*i2+1)*n1+2*i1+1]+=0.125*(z3[i1]+z3[i1+1]);
+		double z321=z[i3*mm2*mm1+i2*mm1+i1];
+		u[2*i3*n2*n1+2*i2*n1+2*i1]+=z321;
+		u[2*i3*n2*n1+2*i2*n1+2*i1+1]+=0.5*(z[i3*mm2*mm1+i2*mm1+i1+1]+z321);
+		u[2*i3*n2*n1+(2*i2+1)*n1+2*i1]+=0.5*z1[i1];
+		u[2*i3*n2*n1+(2*i2+1)*n1+2*i1+1]+=0.25*(z1[i1]+z1[i1+1]);
+		u[(2*i3+1)*n2*n1+2*i2*n1+2*i1]+=0.5*z2[i1];
+		u[(2*i3+1)*n2*n1+2*i2*n1+2*i1+1]+=0.25*(z2[i1]+z2[i1+1]);
+		u[(2*i3+1)*n2*n1+(2*i2+1)*n1+2*i1]+=0.25*z3[i1];
+		u[(2*i3+1)*n2*n1+(2*i2+1)*n1+2*i1+1]+=0.125*(z3[i1]+z3[i1+1]);
 	}
 }
 
@@ -1207,13 +1300,15 @@ static void norm2u3(void* pointer_r,
 		int nx, 
 		int ny, 
 		int nz){
-	double (*r)[n2][n1] = (double (*)[n2][n1])pointer_r;
+	//double (*r)[n2][n1] = (double (*)[n2][n1])pointer_r;
+	double* pointer_aux_r = (double*)pointer_r;
 
 	double s, a;
 	int i3, i2, i1;
 
 	double dn;
 
+	if(timeron){timer_start(T_NORM2);}
 	dn = 1.0*nx*ny*nz;
 
 	s = 0.0;
@@ -1221,14 +1316,17 @@ static void norm2u3(void* pointer_r,
 	for(i3 = 1; i3 < n3-1; i3++){
 		for(i2 = 1; i2 < n2-1; i2++){
 			for(i1 = 1; i1 < n1-1; i1++){
-				s = s + r[i3][i2][i1] * r[i3][i2][i1];
-				a = fabs(r[i3][i2][i1]);
+				//s = s + r[i3][i2][i1] * r[i3][i2][i1];
+				s = s + pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1)] * pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1)];
+				//a = fabs(r[i3][i2][i1]);
+				a = fabs(pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1)]);
 				if(a > *rnmu){*rnmu = a;}
 			}
 		}
 	}
 
 	*rnm2 = sqrt(s/dn);
+	if(timeron){timer_stop(T_NORM2);}
 }
 
 static void norm2u3_gpu(double* r_device, 
@@ -1240,69 +1338,60 @@ static void norm2u3_gpu(double* r_device,
 		int nx, 
 		int ny, 
 		int nz){
-#if defined(PROFILING)
-	timer_start(PROFILING_NORM2U3);
-#endif
+	if(timeron){timer_start(T_NORM2);}
+
+	double s;
+	double dn, max_rnmu;
+	int temp_size, j;
+
+	dn=1.0*nx*ny*nz;
+	s=0.0;
+	max_rnmu=0.0;
+
+	threads_per_block = THREADS_PER_BLOCK_ON_NORM2U3;
+	amount_of_work = (n2-2) * (n3-2) * threads_per_block;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
 
-	int temp_size;
-	double* data_host;
-	double* data_device;
-	double* sum_host;
+	temp_size = amount_of_work / threads_per_block;
+
+	double (*sum_host)=(double*)malloc(temp_size*sizeof(double));
+	double (*max_host)=(double*)malloc(temp_size*sizeof(double));
 	double* sum_device;
-	double* max_host;
-	double* max_device;	
+	double* max_device;
+	hipMalloc(&sum_device,temp_size*sizeof(double));
+	hipMalloc(&max_device,temp_size*sizeof(double));
+
+	norm2u3_gpu_kernel<<<blocks_per_grid, 
+		threads_per_block
+			//,SHARED_2_NORM
+			>>>(
+					r_device,
+					n1,
+					n2,
+					n3,
+					sum_device,
+					max_device,
+					blocks_per_grid,
+					amount_of_work);
+	hipDeviceSynchronize();
 
-	double dn=1.0*nx*ny*nz;
-	double s=0.0;
-	double max_rnmu=0.0;
-
-	int threads_per_block_x=threads_per_block_on_norm2u3;
-	int threads_per_block_y=1;
-	int amount_of_work_x=(n2-2)*threads_per_block_x;
-	int amount_of_work_y=n3-2;	
-	dim3 threadsPerBlock(threads_per_block_x,
-			threads_per_block_y,
-			1);
-	dim3 blocksPerGrid(ceil(double(amount_of_work_x)/double(threadsPerBlock.x)),
-			ceil(double(amount_of_work_y)/double(threadsPerBlock.y)),
-			1);
-
-	temp_size=(amount_of_work_x*amount_of_work_y)/(threads_per_block_x*threads_per_block_y);	
-	cudaMalloc(&data_device,2*temp_size*sizeof(double));
-	sum_device=data_device;
-	max_device=data_device+temp_size;
-
-	norm2u3_gpu_kernel<<<blocksPerGrid, 
-		threadsPerBlock,
-		size_shared_data_on_norm2u3>>>(
-				r_device,
-				n1,
-				n2,
-				n3,
-				sum_device,
-				max_device,
-				blocksPerGrid.x,
-				amount_of_work_x);
-
-	data_host=(double*)malloc(2*temp_size*sizeof(double));
-	cudaMemcpy(data_host, data_device, 2*temp_size*sizeof(double), cudaMemcpyDeviceToHost);
-	sum_host=data_host;
-	max_host=(data_host+temp_size);
-
-	for(int j=0; j<temp_size; j++){
+	hipMemcpy(sum_host, sum_device, temp_size*sizeof(double), hipMemcpyDeviceToHost);
+	hipMemcpy(max_host, max_device, temp_size*sizeof(double), hipMemcpyDeviceToHost);
+
+	for(j=0; j<temp_size; j++){
 		s=s+sum_host[j];
 		if(max_rnmu<max_host[j]){max_rnmu=max_host[j];}
 	}
 
-	cudaFree(data_device);
-	free(data_host);
+	hipFree(sum_device);
+	hipFree(max_device);
+	free(sum_host);
+	free(max_host);
 
 	*rnmu=max_rnmu;
 	*rnm2=sqrt(s/dn);
 
-#if defined(PROFILING)
-	timer_stop(PROFILING_NORM2U3);
-#endif
+	if(timeron){timer_stop(T_NORM2);}
 }
 
 __global__ void norm2u3_gpu_kernel(double* r,
@@ -1311,16 +1400,18 @@ __global__ void norm2u3_gpu_kernel(double* r,
 		const int n3,
 		double* res_sum,
 		double* res_max,
-		int number_of_blocks_on_x_axis,
+		int number_of_blocks,
 		int amount_of_work){
 	int check=blockIdx.x*blockDim.x+threadIdx.x;
 	if(check>=amount_of_work){return;}
 
-	double* scratch_sum = (double*)(extern_share_data);
-	double* scratch_max = (double*)(scratch_sum+blockDim.x);
+	__shared__ double scratch_sum[THREADS_PER_BLOCK_ON_NORM2U3];
+	__shared__ double scratch_max[THREADS_PER_BLOCK_ON_NORM2U3];
+	//double* scratch_sum = (double*)(extern_share_data);
+	//double* scratch_max = (double*)(&scratch_sum[THREADS_PER_BLOCK_ON_NORM2U3]);
 
-	int i3=blockIdx.y*blockDim.y+threadIdx.y+1;
-	int i2=blockIdx.x+1;
+	int i3=blockIdx.x/(n2-2)+1;
+	int i2=blockIdx.x%(n2-2)+1;
 	int i1=threadIdx.x+1;
 
 	double s=0.0;
@@ -1332,7 +1423,7 @@ __global__ void norm2u3_gpu_kernel(double* r,
 		s=s+r321*r321;
 		a=fabs(r321);
 		my_rnmu=(a>my_rnmu)?a:my_rnmu;
-		i1+=blockDim.x;
+		i1+=THREADS_PER_BLOCK_ON_NORM2U3;
 	}
 
 	int lid=threadIdx.x;
@@ -1340,7 +1431,7 @@ __global__ void norm2u3_gpu_kernel(double* r,
 	scratch_max[lid]=my_rnmu;
 
 	__syncthreads();
-	for(int i=blockDim.x/2; i>0; i>>=1){
+	for(int i=THREADS_PER_BLOCK_ON_NORM2U3/2; i>0; i>>=1){
 		if(lid<i){
 			scratch_sum[lid]+=scratch_sum[lid+i];
 			scratch_max[lid]=(scratch_max[lid]>scratch_max[lid+i])?scratch_max[lid]:scratch_max[lid+i];
@@ -1348,7 +1439,7 @@ __global__ void norm2u3_gpu_kernel(double* r,
 		__syncthreads();
 	}
 	if(lid == 0){
-		int idx=blockIdx.y*number_of_blocks_on_x_axis+blockIdx.x;
+		int idx=blockIdx.x;
 		res_sum[idx]=scratch_sum[0];
 		res_max[idx]=scratch_max[0];
 	}
@@ -1399,26 +1490,44 @@ static void psinv(void* pointer_r,
 		int n3, 
 		double c[4], 
 		int k){
-	double (*r)[n2][n1] = (double (*)[n2][n1])pointer_r;
-	double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;	
+	//double (*r)[n2][n1] = (double (*)[n2][n1])pointer_r;
+	//double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;	
+	double* pointer_aux_r = (double*)pointer_r;
+	double* pointer_aux_u = (double*)pointer_u;
 
 	int i3, i2, i1;
 	double r1[M], r2[M];
 
+	if(timeron){timer_start(T_PSINV);}
 	for(i3 = 1; i3 < n3-1; i3++){
 		for(i2 = 1; i2 < n2-1; i2++){
 			for(i1 = 0; i1 < n1; i1++){
-				r1[i1] = r[i3][i2-1][i1] + r[i3][i2+1][i1]
-					+ r[i3-1][i2][i1] + r[i3+1][i2][i1];
-				r2[i1] = r[i3-1][i2-1][i1] + r[i3-1][i2+1][i1]
-					+ r[i3+1][i2-1][i1] + r[i3+1][i2+1][i1];
+				//r1[i1] = r[i3][i2-1][i1] + r[i3][i2+1][i1] + r[i3-1][i2][i1] + r[i3+1][i2][i1];
+				r1[i1] = pointer_aux_r[(i3)*n1*n2 + (i2-1)*n1 + (i1)] + 
+					pointer_aux_r[(i3)*n1*n2 + (i2+1)*n1 + (i1)] + 
+					pointer_aux_r[(i3-1)*n1*n2 + (i2)*n1 + (i1)] + 
+					pointer_aux_r[(i3+1)*n1*n2 + (i2)*n1 + (i1)];
+
+				//r2[i1] = r[i3-1][i2-1][i1] + r[i3-1][i2+1][i1] + r[i3+1][i2-1][i1] + r[i3+1][i2+1][i1];
+				r2[i1] = pointer_aux_r[(i3-1)*n1*n2 + (i2-1)*n1 + (i1)] + 
+					pointer_aux_r[(i3-1)*n1*n2 + (i2+1)*n1 + (i1)] + 
+					pointer_aux_r[(i3+1)*n1*n2 + (i2-1)*n1 + (i1)] + 
+					pointer_aux_r[(i3+1)*n1*n2 + (i2+1)*n1 + (i1)];
 			}
 			for(i1 = 1; i1 < n1-1; i1++){
-				u[i3][i2][i1] = u[i3][i2][i1]
-					+ c[0] * r[i3][i2][i1]
-					+ c[1] * ( r[i3][i2][i1-1] + r[i3][i2][i1+1]
-							+ r1[i1] )
-					+ c[2] * ( r2[i1] + r1[i1-1] + r1[i1+1] );
+				//u[i3][i2][i1] = u[i3][i2][i1] + c[0] * r[i3][i2][i1] + c[1] * ( r[i3][i2][i1-1] + r[i3][i2][i1+1] + r1[i1] ) + c[2] * ( r2[i1] + r1[i1-1] + r1[i1+1] );
+				pointer_aux_u[(i3)*n1*n2 + (i2)*n1 + (i1)] = 
+					pointer_aux_u[(i3)*n1*n2 + (i2)*n1 + (i1)] + 
+					c[0] * 
+					pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1)] + 
+					c[1] * 
+					( pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1-1)] + 
+					  pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1+1)] + 
+					  r1[i1] ) + 
+					c[2] * ( 
+							r2[i1] + 
+							r1[i1-1] + 
+							r1[i1+1] );
 				/*
 				 * --------------------------------------------------------------------
 				 * assume c(3) = 0    (enable line below if c(3) not= 0)
@@ -1429,20 +1538,21 @@ static void psinv(void* pointer_r,
 			}
 		}
 	}
+	if(timeron){timer_stop(T_PSINV);}
 
 	/*
 	 * --------------------------------------------------------------------
 	 * exchange boundary points
 	 * --------------------------------------------------------------------
 	 */
-	comm3(u,n1,n2,n3,k);
+	comm3(pointer_aux_u,n1,n2,n3,k);
 
 	if(debug_vec[0] >= 1){
-		rep_nrm(u,n1,n2,n3,(char*)"   psinv",k);
+		rep_nrm(pointer_aux_u,n1,n2,n3,(char*)"   psinv",k);
 	}
 
 	if(debug_vec[3] >= k){
-		showall(u,n1,n2,n3);
+		showall(pointer_aux_u,n1,n2,n3);
 	}
 }
 
@@ -1453,36 +1563,24 @@ static void psinv_gpu(double* r_device,
 		int n3, 
 		double* c_device, 
 		int k){
-#if defined(PROFILING)
-	timer_start(PROFILING_PSINV);
-#endif
-
-	int threads_per_block = n1 > threads_per_block_on_psinv ? threads_per_block_on_psinv : n1;
-	int amount_of_work_x=(n2-2)*threads_per_block;
-	int amount_of_work_y=n3-2;
-	int threads_per_block_x=threads_per_block;
-	int threads_per_block_y=1;
-	dim3 threadsPerBlock(threads_per_block_x,
-			threads_per_block_y,
-			1);
-	dim3 blocksPerGrid(ceil(double(amount_of_work_x) / double(threadsPerBlock.x)),
-			ceil(double(amount_of_work_y) / double(threadsPerBlock.y)),
-			1);
-
-	psinv_gpu_kernel<<<blocksPerGrid, 
-		threadsPerBlock,
-		size_shared_data_on_psinv>>>(
-				r_device,
-				u_device,
-				c_device,
-				n1,
-				n2,
-				n3,
-				amount_of_work_x);
-
-#if defined(PROFILING)
-	timer_stop(PROFILING_PSINV);
-#endif
+	threads_per_block = n1 > THREADS_PER_BLOCK ? THREADS_PER_BLOCK : n1;
+	amount_of_work = (n3-2) * (n2-2) * threads_per_block;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+	if(timeron){timer_start(T_PSINV);}
+	psinv_gpu_kernel<<<blocks_per_grid, 
+		threads_per_block
+			//,SHARED_2_M
+			>>>(
+					r_device,
+					u_device,
+					c_device,
+					n1,
+					n2,
+					n3,
+					amount_of_work);
+	hipDeviceSynchronize();
+	if(timeron){timer_stop(T_PSINV);}
 
 	/*
 	 * --------------------------------------------------------------------
@@ -1502,15 +1600,16 @@ __global__ void psinv_gpu_kernel(double* r,
 	int check=blockIdx.x*blockDim.x+threadIdx.x;
 	if(check>=amount_of_work){return;}
 
-	double* r1 = (double*)(extern_share_data);
-	double* r2 = (double*)(r1+M);
+	__shared__ double r1[M],r2[M];
+	//double* r1 = (double*)(extern_share_data);
+	//double* r2 = (double*)(&r1[M]);
 
-	int i3=blockIdx.y*blockDim.y+threadIdx.y+1;
-	int i2=blockIdx.x+1;
+	int i3=blockIdx.x/(n2-2)+1;
+	int i2=blockIdx.x%(n2-2)+1;
 	int lid=threadIdx.x;
 	int i1;	
 
-	for(i1=lid; i1<n1; i1+=blockDim.x){
+	for(i1=lid; i1<n1; i1+=THREADS_PER_BLOCK){
 		r1[i1]=r[i3*n2*n1+(i2-1)*n2+i1]
 			+r[i3*n2*n1+(i2+1)*n1+i1]
 			+r[(i3-1)*n2*n1+i2*n1+i1]
@@ -1520,7 +1619,7 @@ __global__ void psinv_gpu_kernel(double* r,
 			+r[(i3+1)*n2*n1+(i2-1)*n1+i1]
 			+r[(i3+1)*n2*n1+(i2+1)*n1+i1];
 	} __syncthreads();
-	for(i1=lid+1; i1<n1-1; i1+=blockDim.x){
+	for(i1=lid+1; i1<n1-1; i1+=THREADS_PER_BLOCK){
 		u[i3*n2*n1+i2*n1+i1]=u[i3*n2*n1+i2*n1+i1]
 			+c[0]*r[i3*n2*n1+i2*n1+i1]
 			+c[1]*(r[i3*n2*n1+i2*n1+i1-1]
@@ -1531,11 +1630,11 @@ __global__ void psinv_gpu_kernel(double* r,
 }
 
 static void release_gpu(){
-	cudaFree(a_device);
-	cudaFree(c_device);
-	cudaFree(u_device);
-	cudaFree(v_device);
-	cudaFree(r_device);
+	hipFree(a_device);
+	hipFree(c_device);
+	hipFree(u_device);
+	hipFree(v_device);
+	hipFree(r_device);
 }
 
 /*
@@ -1577,51 +1676,71 @@ static void resid(void* pointer_u,
 		int n3, 
 		double a[4], 
 		int k){
-	double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;
-	double (*v)[n2][n1] = (double (*)[n2][n1])pointer_v;
-	double (*r)[n2][n1] = (double (*)[n2][n1])pointer_r;	
+	//double (*u)[n2][n1] = (double (*)[n2][n1])pointer_u;
+	//double (*v)[n2][n1] = (double (*)[n2][n1])pointer_v;
+	//double (*r)[n2][n1] = (double (*)[n2][n1])pointer_r;	
+	double* pointer_aux_u = (double*)pointer_u;
+	double* pointer_aux_v = (double*)pointer_v;
+	double* pointer_aux_r = (double*)pointer_r;
 
 	int i3, i2, i1;
 	double u1[M], u2[M];
 
+	if(timeron){timer_start(T_RESID);}
 	for(i3 = 1; i3 < n3-1; i3++){
 		for(i2 = 1; i2 < n2-1; i2++){
 			for(i1 = 0; i1 < n1; i1++){
-				u1[i1] = u[i3][i2-1][i1] + u[i3][i2+1][i1]
-					+ u[i3-1][i2][i1] + u[i3+1][i2][i1];
-				u2[i1] = u[i3-1][i2-1][i1] + u[i3-1][i2+1][i1]
-					+ u[i3+1][i2-1][i1] + u[i3+1][i2+1][i1];
+				//u1[i1] = u[i3][i2-1][i1] + u[i3][i2+1][i1] + u[i3-1][i2][i1] + u[i3+1][i2][i1];
+				u1[i1] = pointer_aux_u[(i3)*n1*n2 + (i2-1)*n1 + (i1)] + 
+					pointer_aux_u[(i3)*n1*n2 + (i2+1)*n1 + (i1)] + 
+					pointer_aux_u[(i3-1)*n1*n2 + (i2)*n1 + (i1)] + 
+					pointer_aux_u[(i3+1)*n1*n2 + (i2)*n1 + (i1)];
+
+				//u2[i1] = u[i3-1][i2-1][i1] + u[i3-1][i2+1][i1] + u[i3+1][i2-1][i1] + u[i3+1][i2+1][i1];
+				u2[i1] = pointer_aux_u[(i3-1)*n1*n2 + (i2-1)*n1 + (i1)] + 
+					pointer_aux_u[(i3-1)*n1*n2 + (i2+1)*n1 + (i1)] + 
+					pointer_aux_u[(i3+1)*n1*n2 + (i2-1)*n1 + (i1)] + 
+					pointer_aux_u[(i3+1)*n1*n2 + (i2+1)*n1 + (i1)];
 			}
 			for(i1 = 1; i1 < n1-1; i1++){
-				r[i3][i2][i1] = v[i3][i2][i1]
-					- a[0] * u[i3][i2][i1]
-					/*
-					 * ---------------------------------------------------------------------
-					 * assume a(1) = 0 (enable 2 lines below if a(1) not= 0)
-					 * ---------------------------------------------------------------------
-					 * > - a(1) * ( u(i1-1,i2,i3) + u(i1+1,i2,i3)
-					 * > + u1(i1) )
-					 * ---------------------------------------------------------------------
-					 */
-					- a[2] * ( u2[i1] + u1[i1-1] + u1[i1+1] )
-					- a[3] * ( u2[i1-1] + u2[i1+1] );
+				/*
+				 * ---------------------------------------------------------------------
+				 * assume a(1) = 0 (enable 2 lines below if a(1) not= 0)
+				 * ---------------------------------------------------------------------
+				 * > - a(1) * ( u(i1-1,i2,i3) + u(i1+1,i2,i3)
+				 * > + u1(i1) )
+				 * ---------------------------------------------------------------------
+				 */
+				//r[i3][i2][i1] = v[i3][i2][i1] - a[0] * u[i3][i2][i1] - a[2] * ( u2[i1] + u1[i1-1] + u1[i1+1] ) - a[3] * ( u2[i1-1] + u2[i1+1] );
+				pointer_aux_r[(i3)*n1*n2 + (i2)*n1 + (i1)] = 
+					pointer_aux_v[(i3)*n1*n2 + (i2)*n1 + (i1)] - 
+					a[0] * 
+					pointer_aux_u[(i3)*n1*n2 + (i2)*n1 + (i1)] - 
+					a[2] * ( 
+							u2[i1] + 
+							u1[i1-1] + 
+							u1[i1+1] ) - 
+					a[3] * ( 
+							u2[i1-1] + 
+							u2[i1+1] );
 			}
 		}
 	}
+	if(timeron){timer_stop(T_RESID);}
 
 	/*
 	 * --------------------------------------------------------------------
 	 * exchange boundary data
 	 * --------------------------------------------------------------------
 	 */
-	comm3(r,n1,n2,n3,k);
+	comm3(pointer_aux_r,n1,n2,n3,k);
 
 	if(debug_vec[0] >= 1){
-		rep_nrm(r,n1,n2,n3,(char*)"   resid",k);
+		rep_nrm(pointer_aux_r,n1,n2,n3,(char*)"   resid",k);
 	}
 
 	if(debug_vec[2] >= k){
-		showall(r,n1,n2,n3);
+		showall(pointer_aux_r,n1,n2,n3);
 	}
 }
 
@@ -1633,37 +1752,25 @@ static void resid_gpu(double* u_device,
 		int n3,
 		double* a_device,
 		int k){
-#if defined(PROFILING)
-	timer_start(PROFILING_RESID);
-#endif
-
-	int threads_per_block = n1 > threads_per_block_on_resid ? threads_per_block_on_resid : n1;
-	int amount_of_work_x=(n2-2)*threads_per_block;
-	int amount_of_work_y=n3-2;
-	int threads_per_block_x=threads_per_block;
-	int threads_per_block_y=1;
-	dim3 threadsPerBlock(threads_per_block_x,
-			threads_per_block_y,
-			1);
-	dim3 blocksPerGrid(ceil(double(amount_of_work_x) / double(threadsPerBlock.x)),
-			ceil(double(amount_of_work_y) / double(threadsPerBlock.y)),
-			1);
-
-	resid_gpu_kernel<<<blocksPerGrid, 
-		threadsPerBlock,
-		size_shared_data_on_resid>>>(
-				u_device,
-				v_device,
-				r_device,
-				a_device,
-				n1,
-				n2,
-				n3,
-				amount_of_work_x);
-
-#if defined(PROFILING)
-	timer_stop(PROFILING_RESID);
-#endif
+	threads_per_block = n1 > THREADS_PER_BLOCK ? THREADS_PER_BLOCK : n1;
+	amount_of_work = (n3-2) * (n2-2) * threads_per_block;
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+	if(timeron){timer_start(T_RESID);}
+	resid_gpu_kernel<<<blocks_per_grid, 
+		threads_per_block
+			//,SHARED_2_M
+			>>>(
+					u_device,
+					v_device,
+					r_device,
+					a_device,
+					n1,
+					n2,
+					n3,
+					amount_of_work);
+	hipDeviceSynchronize();
+	if(timeron){timer_stop(T_RESID);}
 
 	/*
 	 * --------------------------------------------------------------------
@@ -1684,15 +1791,16 @@ __global__ void resid_gpu_kernel(double* u,
 	int check=blockIdx.x*blockDim.x+threadIdx.x;
 	if(check>=amount_of_work){return;}
 
-	double* u1=(double*)(extern_share_data);
-	double* u2=(double*)(u1+M);
+	__shared__ double u1[M], u2[M];
+	//double* u1 = (double*)(extern_share_data);
+	//double* u2 = (double*)(&u1[M]);
 
-	int i3=blockIdx.y*blockDim.y+threadIdx.y+1;
-	int i2=blockIdx.x+1;
+	int i3=blockIdx.x/(n2-2)+1;
+	int i2=blockIdx.x%(n2-2)+1;
 	int lid=threadIdx.x;
-	int i1;	
+	int i1;
 
-	for(i1=lid; i1<n1; i1+=blockDim.x){
+	for(i1=lid; i1<n1; i1+=THREADS_PER_BLOCK){
 		u1[i1]=u[i3*n2*n1+(i2-1)*n1+i1]
 			+u[i3*n2*n1+(i2+1)*n1+i1]
 			+u[(i3-1)*n2*n1+i2*n1+i1]
@@ -1702,7 +1810,7 @@ __global__ void resid_gpu_kernel(double* u,
 			+u[(i3+1)*n2*n1+(i2-1)*n1+i1]
 			+u[(i3+1)*n2*n1+(i2+1)*n1+i1];
 	} __syncthreads();
-	for(i1=lid+1; i1<n1-1; i1+=blockDim.x){
+	for(i1=lid+1; i1<n1-1; i1+=THREADS_PER_BLOCK){
 		r[i3*n2*n1+i2*n1+i1]=v[i3*n2*n1+i2*n1+i1]
 			-a[0]*u[i3*n2*n1+i2*n1+i1]
 			-a[2]*(u2[i1]+u1[i1-1]+u1[i1+1])
@@ -1730,13 +1838,16 @@ static void rprj3(void* pointer_r,
 		int m2j, 
 		int m3j, 
 		int k){
-	double (*r)[m2k][m1k] = (double (*)[m2k][m1k])pointer_r;
-	double (*s)[m2j][m1j] = (double (*)[m2j][m1j])pointer_s;	
+	//double (*r)[m2k][m1k] = (double (*)[m2k][m1k])pointer_r;
+	//double (*s)[m2j][m1j] = (double (*)[m2j][m1j])pointer_s;	
+	double* pointer_aux_r = (double*)pointer_r;
+	double* pointer_aux_s = (double*)pointer_s;	
 
 	int j3, j2, j1, i3, i2, i1, d1, d2, d3, j;
 
 	double x1[M], y1[M], x2, y2;
 
+	if(timeron){timer_start(T_RPRJ3);}
 	if(m1k == 3){
 		d1 = 2;
 	}else{
@@ -1757,36 +1868,64 @@ static void rprj3(void* pointer_r,
 		for(j2 = 1; j2 < m2j-1; j2++){
 			i2 = 2*j2-d2;
 			for(j1 = 1; j1 < m1j; j1++){
-				i1 = 2*j1-d1;				
-				x1[i1] = r[i3+1][i2][i1] + r[i3+1][i2+2][i1]
-					+ r[i3][i2+1][i1] + r[i3+2][i2+1][i1];
-				y1[i1] = r[i3][i2][i1] + r[i3+2][i2][i1]
-					+ r[i3][i2+2][i1] + r[i3+2][i2+2][i1];
+				i1 = 2*j1-d1;	
+
+				//x1[i1] = r[i3+1][i2][i1] + r[i3+1][i2+2][i1] + r[i3][i2+1][i1] + r[i3+2][i2+1][i1];
+				x1[i1] = pointer_aux_r[(i3+1)*m1k*m2k + (i2)*m1k + (i1)] + 
+					pointer_aux_r[(i3+1)*m1k*m2k + (i2+2)*m1k + (i1)] + 
+					pointer_aux_r[(i3)*m1k*m2k + (i2+1)*m1k + (i1)] + 
+					pointer_aux_r[(i3+2)*m1k*m2k + (i2+1)*m1k + (i1)];
+
+				//y1[i1] = r[i3][i2][i1] + r[i3+2][i2][i1] + r[i3][i2+2][i1] + r[i3+2][i2+2][i1];
+				y1[i1] = pointer_aux_r[(i3)*m1k*m2k + (i2)*m1k + (i1)] + 
+					pointer_aux_r[(i3+2)*m1k*m2k + (i2)*m1k + (i1)] + 
+					pointer_aux_r[(i3)*m1k*m2k + (i2+2)*m1k + (i1)] + 
+					pointer_aux_r[(i3+2)*m1k*m2k + (i2+2)*m1k + (i1)];
+
 			}
 			for(j1 = 1; j1 < m1j-1; j1++){
-				i1 = 2*j1-d1;				
-				y2 = r[i3][i2][i1+1] + r[i3+2][i2][i1+1]
-					+ r[i3][i2+2][i1+1] + r[i3+2][i2+2][i1+1];
-				x2 = r[i3+1][i2][i1+1] + r[i3+1][i2+2][i1+1]
-					+ r[i3][i2+1][i1+1] + r[i3+2][i2+1][i1+1];
-				s[j3][j2][j1] =
-					0.5 * r[i3+1][i2+1][i1+1]
-					+ 0.25 * ( r[i3+1][i2+1][i1] + r[i3+1][i2+1][i1+2] + x2)
-					+ 0.125 * ( x1[i1] + x1[i1+2] + y2)
-					+ 0.0625 * ( y1[i1] + y1[i1+2] );
+				i1 = 2*j1-d1;	
+
+				//y2 = r[i3][i2][i1+1] + r[i3+2][i2][i1+1] + r[i3][i2+2][i1+1] + r[i3+2][i2+2][i1+1];
+				y2 = pointer_aux_r[(i3)*m1k*m2k + (i2)*m1k + (i1+1)] + 
+					pointer_aux_r[(i3+2)*m1k*m2k + (i2)*m1k + (i1+1)] + 
+					pointer_aux_r[(i3)*m1k*m2k + (i2+2)*m1k + (i1+1)] + 
+					pointer_aux_r[(i3+2)*m1k*m2k + (i2+2)*m1k + (i1+1)];
+
+				//x2 = r[i3+1][i2][i1+1] + r[i3+1][i2+2][i1+1] + r[i3][i2+1][i1+1] + r[i3+2][i2+1][i1+1];
+				x2 = pointer_aux_r[(i3+1)*m1k*m2k + (i2)*m1k + (i1+1)] + 
+					pointer_aux_r[(i3+1)*m1k*m2k + (i2+2)*m1k + (i1+1)] + 
+					pointer_aux_r[(i3)*m1k*m2k + (i2+1)*m1k + (i1+1)] + 
+					pointer_aux_r[(i3+2)*m1k*m2k + (i2+1)*m1k + (i1+1)];
+
+				//s[j3][j2][j1] = 0.5 * r[i3+1][i2+1][i1+1] + 0.25 * ( r[i3+1][i2+1][i1] + r[i3+1][i2+1][i1+2] + x2) + 0.125 * ( x1[i1] + x1[i1+2] + y2) + 0.0625 * ( y1[i1] + y1[i1+2] );
+				pointer_aux_s[(j3)*m1j*m2j + (j2)*m1j + (j1)] = 0.5 * 
+					pointer_aux_r[(i3+1)*m1k*m2k + (i2+1)*m1k + (i1+1)] + 
+					0.25 * ( 
+							pointer_aux_r[(i3+1)*m1k*m2k + (i2+1)*m1k + (i1)] + 
+							pointer_aux_r[(i3+1)*m1k*m2k + (i2+1)*m1k + (i1+2)] + 
+							x2) + 
+					0.125 * ( 
+							x1[i1] + 
+							x1[i1+2] + 
+							y2) + 
+					0.0625 * ( 
+							y1[i1] + 
+							y1[i1+2] );
 			}
 		}
 	}
+	if(timeron){timer_stop(T_RPRJ3);}
 
 	j=k-1;
-	comm3(s,m1j,m2j,m3j,j);
+	comm3(pointer_aux_s,m1j,m2j,m3j,j);
 
 	if(debug_vec[0] >= 1){
-		rep_nrm(s,m1j,m2j,m3j,(char*)"   rprj3",k-1);	
+		rep_nrm(pointer_aux_s,m1j,m2j,m3j,(char*)"   rprj3",k-1);	
 	}
 
 	if(debug_vec[4] >= k){
-		showall(s,m1j,m2j,m3j);
+		showall(pointer_aux_s,m1j,m2j,m3j);
 	}
 }
 
@@ -1799,10 +1938,6 @@ static void rprj3_gpu(double* r_device,
 		int m2j, 
 		int m3j, 
 		int k){
-#if defined(PROFILING)
-	timer_start(PROFILING_RPRJ3);
-#endif
-
 	int d1,d2,d3,j;
 
 	if(m1k==3){
@@ -1821,49 +1956,36 @@ static void rprj3_gpu(double* r_device,
 		d3=1;
 	}
 
-	int amount_of_work_x=(m2j-2)*(m1j-1);
-	int amount_of_work_y=m3j-2;
-	int threads_per_block_x;
-	int threads_per_block_y=1;
-	if(threads_per_block_on_rprj3 != m1j-1){
-		threads_per_block_x = m1j-1;
-	}
-	else{
-		threads_per_block_x = threads_per_block_on_interp;
-	}
-	dim3 threadsPerBlock(threads_per_block_x,
-			threads_per_block_y,
-			1);
-	dim3 blocksPerGrid(ceil(double(amount_of_work_x) / double(threadsPerBlock.x)),
-			ceil(double(amount_of_work_y) / double(threadsPerBlock.y)),
-			1);
-
-	rprj3_gpu_kernel<<<blocksPerGrid, 
-		threadsPerBlock,
-		size_shared_data_on_rprj3>>>(
-				r_device,
-				s_device,
-				m1k,
-				m2k,
-				m3k,
-				m1j,
-				m2j,
-				m3j,
-				d1,
-				d2,
-				d3,
-				amount_of_work_x);
-
-#if defined(PROFILING)
-	timer_stop(PROFILING_RPRJ3);
-#endif
+	threads_per_block = m1j-1;
+	amount_of_work = (m3j-2) * (m2j-2) * (m1j-1);
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+
+	if(timeron){timer_start(T_RPRJ3);}
+	rprj3_gpu_kernel<<<blocks_per_grid, 
+		threads_per_block
+			//,SHARED_2_M
+			>>>(
+					r_device,
+					s_device,
+					m1k,
+					m2k,
+					m3k,
+					m1j,
+					m2j,
+					m3j,
+					d1,
+					d2,
+					d3,
+					amount_of_work);
+	hipDeviceSynchronize();
+	if(timeron){timer_stop(T_RPRJ3);}
 
 	j=k-1;
 	comm3_gpu(s_device,m1j,m2j,m3j,j);
 }
 
-__global__ void rprj3_gpu_kernel(double* r_device,
-		double* s_device,
+__global__ void rprj3_gpu_kernel(double* base_r,
+		double* base_s,
 		int m1k,
 		int m2k,
 		int m3k,
@@ -1880,39 +2002,43 @@ __global__ void rprj3_gpu_kernel(double* r_device,
 	int j3,j2,j1,i3,i2,i1;
 	double x2,y2;
 
-	double* x1 = (double*)(extern_share_data);
-	double* y1 = (double*)(x1+M);
+	__shared__ double x1[M],y1[M];
+	//double* x1 = (double*)(extern_share_data);
+	//double* y1 = (double*)(&x1[M]);
+
+	double (*r)=base_r;
+	double (*s)=base_s;
 
-	j3=blockIdx.y*blockDim.y+threadIdx.y+1;
-	j2=blockIdx.x+1;
+	j3=blockIdx.x/(m2j-2)+1;
+	j2=blockIdx.x%(m2j-2)+1;
 	j1=threadIdx.x+1;
 
 	i3=2*j3-d3;
 	i2=2*j2-d2;
 	i1=2*j1-d1;
-	x1[i1]=r_device[(i3+1)*m2k*m1k+i2*m1k+i1]
-		+r_device[(i3+1)*m2k*m1k+(i2+2)*m1k+i1]
-		+r_device[i3*m2k*m1k+(i2+1)*m1k+i1]
-		+r_device[(i3+2)*m2k*m1k+(i2+1)*m1k+i1];
-	y1[i1]=r_device[i3*m2k*m1k+i2*m1k+i1]
-		+r_device[(i3+2)*m2k*m1k+i2*m1k+i1]
-		+r_device[i3*m2k*m1k+(i2+2)*m1k+i1]
-		+r_device[(i3+2)*m2k*m1k+(i2+2)*m1k+i1];		
+	x1[i1]=r[(i3+1)*m2k*m1k+i2*m1k+i1]
+		+r[(i3+1)*m2k*m1k+(i2+2)*m1k+i1]
+		+r[i3*m2k*m1k+(i2+1)*m1k+i1]
+		+r[(i3+2)*m2k*m1k+(i2+1)*m1k+i1];
+	y1[i1]=r[i3*m2k*m1k+i2*m1k+i1]
+		+r[(i3+2)*m2k*m1k+i2*m1k+i1]
+		+r[i3*m2k*m1k+(i2+2)*m1k+i1]
+		+r[(i3+2)*m2k*m1k+(i2+2)*m1k+i1];		
 	__syncthreads();
 	if(j1<m1j-1){
 		i1=2*j1-d1;
-		y2=r_device[i3*m2k*m1k+i2*m1k+i1+1]
-			+r_device[(i3+2)*m2k*m1k+i2*m1k+i1+1]
-			+r_device[i3*m2k*m1k+(i2+2)*m1k+i1+1]
-			+r_device[(i3+2)*m2k*m1k+(i2+2)*m1k+i1+1];
-		x2=r_device[(i3+1)*m2k*m1k+i2*m1k+i1+1]
-			+r_device[(i3+1)*m2k*m1k+(i2+2)*m1k+i1+1]
-			+r_device[i3*m2k*m1k+(i2+1)*m1k+i1+1]
-			+r_device[(i3+2)*m2k*m1k+(i2+1)*m1k+i1+1];
-		s_device[j3*m2j*m1j+j2*m1j+j1]=
-			0.5*r_device[(i3+1)*m2k*m1k+(i2+1)*m1k+i1+1]
-			+0.25*(r_device[(i3+1)*m2k*m1k+(i2+1)*m1k+i1]
-					+r_device[(i3+1)*m2k*m1k+(i2+1)*m1k+i1+2]+x2)
+		y2=r[i3*m2k*m1k+i2*m1k+i1+1]
+			+r[(i3+2)*m2k*m1k+i2*m1k+i1+1]
+			+r[i3*m2k*m1k+(i2+2)*m1k+i1+1]
+			+r[(i3+2)*m2k*m1k+(i2+2)*m1k+i1+1];
+		x2=r[(i3+1)*m2k*m1k+i2*m1k+i1+1]
+			+r[(i3+1)*m2k*m1k+(i2+2)*m1k+i1+1]
+			+r[i3*m2k*m1k+(i2+1)*m1k+i1+1]
+			+r[(i3+2)*m2k*m1k+(i2+1)*m1k+i1+1];
+		s[j3*m2j*m1j+j2*m1j+j1]=
+			0.5*r[(i3+1)*m2k*m1k+(i2+1)*m1k+i1+1]
+			+0.25*(r[(i3+1)*m2k*m1k+(i2+1)*m1k+i1]
+					+r[(i3+1)*m2k*m1k+(i2+1)*m1k+i1+2]+x2)
 			+0.125*(x1[i1]+x1[i1+2]+y2)
 			+0.0625*(y1[i1]+y1[i1+2]);
 	}
@@ -1977,123 +2103,29 @@ static void setup(int* n1,
 
 static void setup_gpu(double* a, 
 		double* c){
-	/*
-	 * struct cudaDeviceProp{
-	 *  char name[256];
-	 *  size_t totalGlobalMem;
-	 *  size_t sharedMemPerBlock;
-	 *  int regsPerBlock;
-	 *  int warpSize;
-	 *  size_t memPitch;
-	 *  int maxThreadsPerBlock;
-	 *  int maxThreadsDim[3];
-	 *  int maxGridSize[3];
-	 *  size_t totalConstMem;
-	 *  int major;
-	 *  int minor;
-	 *  int clockRate;
-	 *  size_t textureAlignment;
-	 *  int deviceOverlap;
-	 *  int multiProcessorCount;
-	 *  int kernelExecTimeoutEnabled;
-	 *  int integrated;
-	 *  int canMapHostMemory;
-	 *  int computeMode;
-	 *  int concurrentKernels;
-	 *  int ECCEnabled;
-	 *  int pciBusID;
-	 *  int pciDeviceID;
-	 *  int tccDriver;
-	 * }
-	 */
-	/* define gpu_device */
-	cudaGetDeviceCount(&total_devices);
-	if(total_devices==0){
-		printf("\n\n\n No Nvidia GPU found!!! \n\n\n");
-		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
-	}else{
-		gpu_device_id = 0;
-	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((MG_THREADS_PER_BLOCK_ON_COMM3>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_COMM3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_comm3 = MG_THREADS_PER_BLOCK_ON_COMM3;
-	}else{
-		threads_per_block_on_comm3 = gpu_device_properties.warpSize;
-	}	
-
-	if((MG_THREADS_PER_BLOCK_ON_INTERP>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_INTERP<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_interp = MG_THREADS_PER_BLOCK_ON_INTERP;
-	}else{
-		threads_per_block_on_interp = gpu_device_properties.warpSize;
-	}	
-
-	if((MG_THREADS_PER_BLOCK_ON_NORM2U3>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_NORM2U3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_norm2u3 = MG_THREADS_PER_BLOCK_ON_NORM2U3;
-	}else{
-		threads_per_block_on_norm2u3 = gpu_device_properties.warpSize;
-	}
-	if((MG_THREADS_PER_BLOCK_ON_PSINV>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_PSINV<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_psinv = MG_THREADS_PER_BLOCK_ON_PSINV;
-	}else{
-		threads_per_block_on_psinv = gpu_device_properties.warpSize;
-	}	
-	if((MG_THREADS_PER_BLOCK_ON_RESID>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_RESID<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_resid = MG_THREADS_PER_BLOCK_ON_RESID;
-	}else{
-		threads_per_block_on_resid = gpu_device_properties.warpSize;
-	}
-	if((MG_THREADS_PER_BLOCK_ON_RPRJ3>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_RPRJ3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_rprj3 = MG_THREADS_PER_BLOCK_ON_RPRJ3;
-	}else{
-		threads_per_block_on_rprj3 = gpu_device_properties.warpSize;
-	}
-	if((MG_THREADS_PER_BLOCK_ON_ZERO3>=1)&&
-			(MG_THREADS_PER_BLOCK_ON_ZERO3<=gpu_device_properties.maxThreadsPerBlock)){
-		threads_per_block_on_zero3 = MG_THREADS_PER_BLOCK_ON_ZERO3;
-	}else{
-		threads_per_block_on_zero3 = gpu_device_properties.warpSize;
-	}	
-
 	size_a_device=sizeof(double)*(4);
 	size_c_device=sizeof(double)*(4);
 	size_u_device=sizeof(double)*(NR);
 	size_v_device=sizeof(double)*(NV);
 	size_r_device=sizeof(double)*(NR);
-	size_shared_data_on_interp=3*M*sizeof(double);
-	size_shared_data_on_norm2u3=2*threads_per_block_on_norm2u3*sizeof(double);
-	size_shared_data_on_psinv=2*M*sizeof(double);
-	size_shared_data_on_resid=2*M*sizeof(double);
-	size_shared_data_on_rprj3=2*M*sizeof(double);
-
-	cudaMalloc(&a_device, size_a_device);
-	cudaMalloc(&c_device, size_c_device);
-	cudaMalloc(&u_device, size_u_device);
-	cudaMalloc(&v_device, size_v_device);
-	cudaMalloc(&r_device, size_r_device);
-	cudaMemcpy(a_device, a, size_a_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(c_device, c, size_c_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(u_device, u, size_u_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(v_device, v, size_v_device, cudaMemcpyHostToDevice);
-	cudaMemcpy(r_device, r, size_r_device, cudaMemcpyHostToDevice);		
+	hipMalloc(&a_device, size_a_device);
+	hipMalloc(&c_device, size_c_device);
+	hipMalloc(&u_device, size_u_device);
+	hipMalloc(&v_device, size_v_device);
+	hipMalloc(&r_device, size_r_device);
+	hipMemcpy(a_device, a, size_a_device, hipMemcpyHostToDevice);
+	hipMemcpy(c_device, c, size_c_device, hipMemcpyHostToDevice);
+	hipMemcpy(u_device, u, size_u_device, hipMemcpyHostToDevice);
+	hipMemcpy(v_device, v, size_v_device, hipMemcpyHostToDevice);
+	hipMemcpy(r_device, r, size_r_device, hipMemcpyHostToDevice);		
 }
 
 static void showall(void* pointer_z, 
 		int n1, 
 		int n2, 
 		int n3){
-	double (*z)[n2][n1] = (double (*)[n2][n1])pointer_z;
+	//double (*z)[n2][n1] = (double (*)[n2][n1])pointer_z;
+	double* pointer_aux_z = (double*)pointer_z;
 
 	int i1,i2,i3;
 	int m1, m2, m3;
@@ -2106,7 +2138,8 @@ static void showall(void* pointer_z,
 	for(i3 = 0; i3 < m3; i3++){
 		for(i2 = 0; i2 < m2; i2++){
 			for(i1 = 0; i1 < m1; i1++){			
-				printf("%6.3f", z[i3][i2][i1]);
+				//printf("%6.3f", z[i3][i2][i1]);
+				printf("%6.3f", pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)]);
 			}
 			printf("\n");
 		}
@@ -2119,13 +2152,15 @@ static void zero3(void* pointer_z,
 		int n1, 
 		int n2, 
 		int n3){
-	double (*z)[n2][n1] = (double (*)[n2][n1])pointer_z;
+	//double (*z)[n2][n1] = (double (*)[n2][n1])pointer_z;
+	double* pointer_aux_z = (double*)pointer_z;
 
 	int i1, i2, i3;
 	for(i3 = 0;i3 < n3; i3++){
 		for(i2 = 0; i2 < n2; i2++){
 			for(i1 = 0; i1 < n1; i1++){
-				z[i3][i2][i1] = 0.0;
+				//z[i3][i2][i1] = 0.0;
+				pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)] = 0.0;
 			}
 		}
 	}
@@ -2135,23 +2170,15 @@ static void zero3_gpu(double* z_device,
 		int n1, 
 		int n2, 
 		int n3){
-#if defined(PROFILING)
-	timer_start(PROFILING_ZERO3);
-#endif
-
-	int threads_per_block = threads_per_block_on_zero3;
-	int amount_of_work = n1*n2*n3;	
-	int blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
+	threads_per_block = THREADS_PER_BLOCK_ON_ZERO3;
+	amount_of_work = n1*n2*n3;	
+	blocks_per_grid = (ceil((double)(amount_of_work)/(double)(threads_per_block)));
 
 	zero3_gpu_kernel<<<blocks_per_grid, threads_per_block>>>(z_device,
 			n1,
 			n2,
 			n3,
 			amount_of_work);
-
-#if defined(PROFILING)
-	timer_stop(PROFILING_ZERO3);
-#endif
 }
 
 __global__ void zero3_gpu_kernel(double* z, 
@@ -2178,7 +2205,8 @@ static void zran3(void* pointer_z,
 		int nx, 
 		int ny, 
 		int k){
-	double (*z)[n2][n1] = (double (*)[n2][n1])pointer_z;
+	//double (*z)[n2][n1] = (double (*)[n2][n1])pointer_z;
+	double* pointer_aux_z = (double*)pointer_z;
 
 	int i0, m0, m1;
 
@@ -2192,7 +2220,8 @@ static void zran3(void* pointer_z,
 	a1 = power(A, nx);
 	a2 = power(A, nx*ny);
 
-	zero3(z, n1, n2, n3);
+	//zero3(z, n1, n2, n3);
+	zero3(pointer_aux_z, n1, n2, n3);
 
 	i = is1-2+nx*(is2-2+ny*(is3-2));
 
@@ -2206,7 +2235,8 @@ static void zran3(void* pointer_z,
 		x1 = x0;
 		for(i2 = 1; i2 < e2; i2++){
 			xx = x1;
-			vranlc(d1, &xx, A, &(z[i3][i2][1]));
+			//vranlc(d1, &xx, A, &(z[i3][i2][1]));
+			vranlc(d1, &xx, A, &(pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (1)]));
 			randlc(&x1,a1);
 		}
 		randlc(&x0, a2);
@@ -2230,15 +2260,19 @@ static void zran3(void* pointer_z,
 	for(i3 = 1; i3 < n3-1; i3++){
 		for(i2 = 1; i2 < n2-1; i2++){
 			for(i1 = 1; i1 < n1-1; i1++){
-				if(z[i3][i2][i1] > ten[1][0]){
-					ten[1][0] = z[i3][i2][i1];
+				//if(z[i3][i2][i1] > ten[1][0]){
+				if(pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)] > ten[1][0]){
+					//ten[1][0] = z[i3][i2][i1];
+					ten[1][0] = pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)];
 					j1[1][0] = i1;
 					j2[1][0] = i2;
 					j3[1][0] = i3;
 					bubble(ten, j1, j2, j3, MM, 1);
 				}
-				if(z[i3][i2][i1] < ten[0][0]){
-					ten[0][0] = z[i3][i2][i1];
+				//if(z[i3][i2][i1] < ten[0][0]){
+				if(pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)] < ten[0][0]){
+					//ten[0][0] = z[i3][i2][i1];
+					ten[0][0] = pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)];
 					j1[0][0] = i1;
 					j2[0][0] = i2;
 					j3[0][0] = i3;
@@ -2248,56 +2282,59 @@ static void zran3(void* pointer_z,
 		}
 	}
 
-	/*
-	 * ---------------------------------------------------------------------
-	 * now which of these are globally best?
-	 * ---------------------------------------------------------------------
-	 */	
-	i1 = MM - 1;
-	i0 = MM - 1; 
-	for(i = MM - 1; i >= 0; i--){
-		best = 0.0;
-		if(best < ten[1][i1]){
-			jg[1][i][0] = 0;
-			jg[1][i][1] = is1 - 2 + j1[1][i1];
-			jg[1][i][2] = is2 - 2 + j2[1][i1];
-			jg[1][i][3] = is3 - 2 + j3[1][i1];
-			i1 = i1-1;
-		}else{
-			jg[1][i][0] = 0;
-			jg[1][i][1] = 0;
-			jg[1][i][2] = 0;
-			jg[1][i][3] = 0;
-		}
-		best = 1.0;
-		if(best > ten[0][i0]){
-			jg[0][i][0] = 0;
-			jg[0][i][1] = is1 - 2 + j1[0][i0];
-			jg[0][i][2] = is2 - 2 + j2[0][i0];
-			jg[0][i][3] = is3 - 2 + j3[0][i0];
-			i0 = i0-1;
-		}else{
-			jg[0][i][0] = 0;
-			jg[0][i][1] = 0;
-			jg[0][i][2] = 0;
-			jg[0][i][3] = 0;
-		}
-	}
-	m1 = 0;
-	m0 = 0;
-
-	for(i3 = 0; i3 < n3; i3++){
-		for(i2 = 0; i2 < n2; i2++){
-			for(i1 = 0; i1 < n1; i1++){
-				z[i3][i2][i1] = 0.0;
+			/*
+			 * ---------------------------------------------------------------------
+			 * now which of these are globally best?
+			 * ---------------------------------------------------------------------
+			 */	
+			i1 = MM - 1;
+			i0 = MM - 1; 
+			for(i = MM - 1; i >= 0; i--){
+				best = 0.0;
+				if(best < ten[1][i1]){
+					jg[1][i][0] = 0;
+					jg[1][i][1] = is1 - 2 + j1[1][i1];
+					jg[1][i][2] = is2 - 2 + j2[1][i1];
+					jg[1][i][3] = is3 - 2 + j3[1][i1];
+					i1 = i1-1;
+				}else{
+					jg[1][i][0] = 0;
+					jg[1][i][1] = 0;
+					jg[1][i][2] = 0;
+					jg[1][i][3] = 0;
+				}
+				best = 1.0;
+				if(best > ten[0][i0]){
+					jg[0][i][0] = 0;
+					jg[0][i][1] = is1 - 2 + j1[0][i0];
+					jg[0][i][2] = is2 - 2 + j2[0][i0];
+					jg[0][i][3] = is3 - 2 + j3[0][i0];
+					i0 = i0-1;
+				}else{
+					jg[0][i][0] = 0;
+					jg[0][i][1] = 0;
+					jg[0][i][2] = 0;
+					jg[0][i][3] = 0;
+				}
 			}
-		}
-	}
-	for (i = MM-1; i >= m0; i--){
-		z[jg[0][i][3]][jg[0][i][2]][jg[0][i][1]] = -1.0;
-	}
-	for(i = MM-1; i >= m1; i--){
-		z[jg[1][i][3]][jg[1][i][2]][jg[1][i][1]] = +1.0;
-	}
-	comm3(z, n1, n2, n3, k);
+			m1 = 0;
+			m0 = 0;
+
+			for(i3 = 0; i3 < n3; i3++){
+				for(i2 = 0; i2 < n2; i2++){
+					for(i1 = 0; i1 < n1; i1++){
+						//z[i3][i2][i1] = 0.0;
+						pointer_aux_z[(i3)*n1*n2 + (i2)*n1 + (i1)] = 0.0;
+					}
+				}
+			}
+			for (i = MM-1; i >= m0; i--){
+				//z[jg[0][i][3]][jg[0][i][2]][jg[0][i][1]] = -1.0;
+				pointer_aux_z[(jg[0][i][3])*n1*n2 + (jg[0][i][2])*n1 + (jg[0][i][1])] = -1.0;
+			}
+			for(i = MM-1; i >= m1; i--){
+				//z[jg[1][i][3]][jg[1][i][2]][jg[1][i][1]] = +1.0;
+				pointer_aux_z[(jg[1][i][3])*n1*n2 + (jg[1][i][2])*n1 + (jg[1][i][1])] = +1.0;
+			}
+			comm3(pointer_aux_z, n1, n2, n3, k);
 }
diff --git a/CUDA/MG/setup.ini b/CUDA/MG/setup.ini
new file mode 100644
index 0000000..309d8fe
--- /dev/null
+++ b/CUDA/MG/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/mg.S;../bin/mg.W;../bin/mg.A;../bin/mg.B
+use_clang_plugin = false
+llvm_pass = make -f Makefile.all llvm
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/Makefile b/CUDA/Makefile
index b4ccd54..d53f65f 100644
--- a/CUDA/Makefile
+++ b/CUDA/Makefile
@@ -51,7 +51,7 @@ suite:
 # because those makefiles need config/make.def
 clean:
 	- rm -f core 
-	- rm -f *~ */core */*~ */*.o */npbparams.hpp */*.obj */*.exe
+	- rm -f *~ */core */*~ */*.o */*.bc */seq.txt */npbparams.hpp */*.obj */*.exe
 	- rm -f sys/setparams sys/makesuite sys/setparams.hpp
 
 cleanall: clean
diff --git a/CUDA/README.md b/CUDA/README.md
index 0f2d705..34589a3 100644
--- a/CUDA/README.md
+++ b/CUDA/README.md
@@ -3,18 +3,11 @@
 This is a repository aimed at providing GPU parallel codes with different parallel APIs for the NAS Parallel Benchmarks ([NPB](https://www.nas.nasa.gov/publications/npb.html)) from a C/C++ version ([NPB-CPP](https://github.com/GMAP/NPB-CPP)). You can also contribute with this project, writing issues and pull requests. :smile:
 
 
-:sound:*News:* Parametrization support for configuring number of threads per block and CUDA parallelism optimizations. :date:25/Jul/2021
-
 :sound:*News:* CUDA versions for pseudo-applications added and IS improved. :date:11/Feb/2021
 
-:sound:*News:* Paper published in the journal Software: Practice and Experience (SPE). :date:29/Nov/2021
-
-
 ## How to cite our work :+1:
 
-[DOI](https://doi.org/10.1002/spe.3056) - Araujo, G.; Griebler, D.; Rockenbach, D. A.; Danelutto, M.; Fernandes, L. G.; **NAS Parallel Benchmarks with CUDA and beyond**, Software: Practice and Experience (SPE), 2021.
-
-[DOI](https://doi.org/10.1109/PDP50117.2020.00009) - Araujo, G.; Griebler, D.; Danelutto, M.; Fernandes, L. G.; **Efficient NAS Benchmark Kernels with CUDA**. *28th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)*, Vsters, 2020. 
+[DOI](https://doi.org/10.1109/PDP50117.2020.00009) - Araujo, G. A.; Griebler, D.; Danelutto, M.; Fernandes, L. G. **Efficient NAS Benchmark Kernels with CUDA**. *28th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)*, Vsters, 2020. 
   
 ## The NPB with CUDA
 
@@ -24,9 +17,9 @@ The parallel CUDA version was implemented from the serial version of [NPB-CPP](h
 
 NAS Parallel Benchmarks code contributors with CUDA are:
 
-Dalvan Griebler: dalvan.griebler@pucrs.br
+Dalvan Griebler: dalvan.griebler@acad.pucrs.br
 
-Gabriell Araujo: gabriell.araujo@edu.pucrs.br
+Gabriell Araujo: gabriell.araujo@acad.pucrs.br
 
 ==================================================================
 
@@ -34,11 +27,11 @@ Each directory is independent and contains its own implemented version:
 
 *Five kernels*
 
-+ **IS** - Integer Sort
++ **IS** - Integer Sort, random memory access
 + **EP** - Embarrassingly Parallel
-+ **CG** - Conjugate Gradient
-+ **MG** - Multi-Grid
-+ **FT** - discrete 3D fast Fourier Transform
++ **CG** - Conjugate Gradient, irregular memory access and communication
++ **MG** - Multi-Grid on a sequence of meshes, long- and short-distance communication, memory intensive
++ **FT** - discrete 3D fast Fourier Transform, all-to-all communication
 
 *Three pseudo-application*
 
@@ -63,7 +56,7 @@ make _BENCHMARK CLASS=_VERSION
 `_BENCHMARKs` are:
 
 
-CG, EP, FT, IS, MG, BT, LU, and SP 
+CG, EP, FT, IS, MG, SP, BT, and LU 
 
 
 `_VERSIONs` are:
@@ -86,36 +79,42 @@ make ep CLASS=B
 
 ## Activating the additional timers
 
-NPB-GPU has additional timers for profiling purpose. To activate these timers, create a dummy file 'timer.flag' in the main directory of the NPB version (e.g. CUDA/timer.flag).
+NPB3.3.1 includes additional timers for profiling purpose. To activate these timers, create a dummy file 'timer.flag' in the main directory of the NPB.
 
-## Configuring the number of threads per block
+## Notes about the Fortran to CPP convertion
 
-NPB-GPU allows configuring the number of threads per block of each GPU kernel in the benchmarks. The user can specify the number of threads per block by editing the file gpu.config in the directory <API>/config/. If no file is specified, all GPU kernels are executed using the warp size of the GPU as the number of threads per block.
+The following information are also written and keep updated in [NPB-CPP](https://github.com/GMAP/NPB-CPP):
 
-Syntax of the gpu.config file: 
++ Memory conventions adopted on NPB-CPP turn better the performance of the C++ code, lowering the execution time and memory consumption (on some applications, these conventions turn the performance of the NPB-CPP even better than the original Fortran NPB3.3.1, as for example on BT pseudo-application).  
 
-```
-<benchmark-name>_THREADS_PER_BLOCK_<gpu-kernel-name> = <interger-value>
-```
+  - Any global array is allocated with dynamic memory and as one single dimension.
 
-Configuring CG benchmark as example:
+  - On kernels, a cast is made in the functions, so is possible to work using multi-dimension accesses with the arrays, so, for example a function can receive an array like `matrix_aux[NY*NY]`, and work with accesses like `matrix_aux[j][i]`, instead of one single dimension access (actually, the cast in the functions follows the original NPB3.3.1 way).
 
-```
-CG_THREADS_PER_BLOCK_ON_KERNEL_ONE = 32
-CG_THREADS_PER_BLOCK_ON_KERNEL_TWO = 128
-CG_THREADS_PER_BLOCK_ON_KERNEL_THREE = 64
-CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR = 256
-CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE = 32
-CG_THREADS_PER_BLOCK_ON_KERNEL_SIX = 64
-CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN = 128
-CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT = 64
-CG_THREADS_PER_BLOCK_ON_KERNEL_NINE = 512
-CG_THREADS_PER_BLOCK_ON_KERNEL_TEN = 512
-CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN = 1024
-```
+  - On pseudo-applications, the cast is done already in the array declarations (NPB3.3.1 does not use one single dimension on pseudo-applications, so we cast the arrays directly on declarations, because this way, changes in the structure of the functions are not necessary).
 
-The NPB-GPU also allows changing the GPU device by providing the following syntax in the gpu.config file:
+  - To disable this convention (dynamic memory and on single dimension) is necessary set the flag `-DDO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION` on compilation.
 
-```
-GPU_DEVICE = <interger-value>
-```
\ No newline at end of file
+  - Also, we keep every single structure of the loops as the same as Fortran original, but as Fortran memory behavior is different from C/C++, we invert the dimensions of the arrays and consequently, any array access. It is like change for example from an access `array[i][j][k]` to `array[k][j][i]` (but keeping the same organization of the loops), and change array dimensions from `array[NX][NY][NZ]` to `array[NZ][NY][NX]`.   
+ 
++ The original NPB has a file for print results of IS (`c_print_results.c`) and another file for print results of the other Benchmarks (`print_results.f`), it means, one file for Fortran code and one file for C code (IS is the only Benchmark that was written using C language). As the entire NPB-CPP is in C++, we keep only a single file to print results of all Benchmarks, we merged these two files and created `c_print_results.cpp`.
+
++ Any `goto` in the applications was replaced with an equivalent code using loops, keeping the same logic.
+
++ There are some little differences on indexes and ranges of loops, that are inherent to conversions from Fortran to C/C++.
+
++ In the file `common/npb-CPP.hpp` we define additional stuff like the structure and operations for complex numbers.
+
++ FT
+
+	- Instead convert code directly from serial FT 3.3.1, we convert Fortran code from FT OpenMP version, where the format is equal to the FT serial versions before NPB 3.0.
+	In no version of NPB the OpenMP parallel code is based on the serial code presented in 3.0 and 3.3.1 versions.
+	The parallel code OpenMP in all versions of NPB is based on the sequential format before to the versions 3.0 and 3.3.1.
+	In addition, in version 3.4, the most recent, they state that the sequential code will no longer be available. The sequential code will be the OpenMP code without compiling with OpenMP.
+	Faced with these facts, we conclude that this refactored FT serial code (3.3.1) has no utility, as it was not used for parallel versions and from NPB 3.4 it will be completely forgotten.
+  - In the global.hpp, historically, the constants `FFTBLOCK_DEFAULT` and `FFTBLOCKPAD_DEFAULT` receive values that change the cache behavior of the applications and the performance can be better or worse for each processor according which values are choosed. We define these constants with the value 1 (DEFAULT_BEHAVIOR), that determines a default behavior independently of the processor where the application is running.
+  - The size of the matrixes on the original NPB is `[NZ][NY][NX+1]`, but we changed to `[NZ][NY][NX]`, because the additional positions generated by `NX+1` are not used on the application, they only spend more memory.
+  - On the original NPB, the auxiliary matrixes `y1`, `y2` and `u` have the size as `NX`. But only in cffts1 the size is NX, on the cffts2 the correct size is NY and cffts3 the size is NZ. It is a problem when NX is not the bigger dimension. To fix this, we assign the size of these matrixes as `MAXDIM` that is the size of the bigger dimension. Consequently `MAXDIM` is also used as argument in the function `fft_init` that initializes the values of the matrix `u`.
+
++ IS
+	- On the original NPB, IS has on its own source functions like `randlc`. On our version of NPB we does not need it, because we already have these functions implemented. On original NPB these functions are needed in the IS code, because IS is in C and these functions were written in Fortran for the other applications.
diff --git a/CUDA/SP/Makefile b/CUDA/SP/Makefile
index 6a66fe0..c388c4d 100644
--- a/CUDA/SP/Makefile
+++ b/CUDA/SP/Makefile
@@ -16,5 +16,5 @@ sp.o:		sp.cu npbparams.hpp
 	${CCOMPILE} sp.cu
 
 clean:
-	- rm -f *.o *~ mputil*
+	- rm -f *.o *.bc seq.txt *~ mputil*
 	- rm -f npbparams.hpp core
diff --git a/CUDA/SP/Makefile.all b/CUDA/SP/Makefile.all
new file mode 100644
index 0000000..3a05842
--- /dev/null
+++ b/CUDA/SP/Makefile.all
@@ -0,0 +1,15 @@
+SHELL=/bin/sh
+BENCHMARK=sp
+BENCHMARKU=SP
+
+all:
+	$(MAKE) CLASS=S
+	$(MAKE) clean
+	$(MAKE) CLASS=W
+	$(MAKE) clean
+	$(MAKE) CLASS=A
+	$(MAKE) clean
+	$(MAKE) CLASS=B
+	$(MAKE) clean    
+
+clang:
\ No newline at end of file
diff --git a/CUDA/SP/setup.ini b/CUDA/SP/setup.ini
new file mode 100644
index 0000000..bc7b66e
--- /dev/null
+++ b/CUDA/SP/setup.ini
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make -f Makefile.all
+run = ../bin/sp.S;../bin/sp.W;../bin/sp.A;../bin/sp.B
+use_clang_plugin = true
+clang_convert = make CLASS=A INJECT_CODE_CLANG=1
+clean = cd .. && make cleanall
\ No newline at end of file
diff --git a/CUDA/SP/sp.cu b/CUDA/SP/sp.cu
index baca72c..7ac1564 100644
--- a/CUDA/SP/sp.cu
+++ b/CUDA/SP/sp.cu
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h"
 /* 
  * ------------------------------------------------------------------------------
  *
@@ -58,7 +59,7 @@
  * ------------------------------------------------------------------------------
  */
 
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 #include "../common/npb-CPP.hpp"
 #include "npbparams.hpp"
 
@@ -86,6 +87,23 @@
 #define PROFILING_Y_SOLVE (15)
 #define PROFILING_Z_SOLVE (16)
 
+#define THREADS_PER_BLOCK_ON_ADD (32)
+#define THREADS_PER_BLOCK_ON_COMPUTE_RHS_1 (32)
+#define THREADS_PER_BLOCK_ON_COMPUTE_RHS_2 (32)
+#define THREADS_PER_BLOCK_ON_ERROR_NORM_1 (64)
+#define THREADS_PER_BLOCK_ON_ERROR_NORM_2 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_1 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_2 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_3 (32)
+#define THREADS_PER_BLOCK_ON_EXACT_RHS_4 (32)
+#define THREADS_PER_BLOCK_ON_INITIALIZE (32)
+#define THREADS_PER_BLOCK_ON_RHS_NORM_1 (64)
+#define THREADS_PER_BLOCK_ON_RHS_NORM_2 (32)
+#define THREADS_PER_BLOCK_ON_TXINVR (32)
+#define THREADS_PER_BLOCK_ON_X_SOLVE (32)
+#define THREADS_PER_BLOCK_ON_Y_SOLVE (32)
+#define THREADS_PER_BLOCK_ON_Z_SOLVE (32)
+
 /* gpu linear pattern */
 #define u(m,i,j,k) u[(i)+nx*((j)+ny*((k)+nz*(m)))]
 #define forcing(m,i,j,k) forcing[(i)+nx*((j)+ny*((k)+nz*(m)))]
@@ -178,25 +196,9 @@ static size_t size_rms_buffer_device;
 static int nx;
 static int ny;
 static int nz;
-static int THREADS_PER_BLOCK_ON_ADD;
-static int THREADS_PER_BLOCK_ON_COMPUTE_RHS_1;
-static int THREADS_PER_BLOCK_ON_COMPUTE_RHS_2;
-static int THREADS_PER_BLOCK_ON_ERROR_NORM_1;
-static int THREADS_PER_BLOCK_ON_ERROR_NORM_2;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_1;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_2;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_3;
-static int THREADS_PER_BLOCK_ON_EXACT_RHS_4;
-static int THREADS_PER_BLOCK_ON_INITIALIZE;
-static int THREADS_PER_BLOCK_ON_RHS_NORM_1;
-static int THREADS_PER_BLOCK_ON_RHS_NORM_2;
-static int THREADS_PER_BLOCK_ON_TXINVR;
-static int THREADS_PER_BLOCK_ON_X_SOLVE;
-static int THREADS_PER_BLOCK_ON_Y_SOLVE;
-static int THREADS_PER_BLOCK_ON_Z_SOLVE;
 int gpu_device_id;
 int total_devices;
-cudaDeviceProp gpu_device_properties;
+hipDeviceProp_t gpu_device_properties;
 extern __shared__ double extern_share_data[];
 
 namespace constants_device{
@@ -548,11 +550,6 @@ int main(int argc, char** argv){
 			verified,
 			(char*)NPBVERSION,
 			(char*)COMPILETIME,
-			(char*)COMPILERVERSION,
-			(char*)LIBVERSION,
-			(char*)CPU_MODEL,
-			(char*)gpu_device_properties.name,
-			gpu_config_string,
 			(char*)CS1,
 			(char*)CS2,
 			(char*)CS3,
@@ -902,7 +899,7 @@ static void error_norm_gpu(double rms[]){
 	timer_stop(PROFILING_ERROR_NORM_2);
 #endif
 
-	cudaMemcpy(rms, rms_buffer_device, 5*sizeof(double), cudaMemcpyDeviceToHost);
+	hipMemcpy(rms, rms_buffer_device, 5*sizeof(double), hipMemcpyDeviceToHost);
 }
 
 __global__ static void error_norm_gpu_kernel_1(double* rms,
@@ -1519,19 +1516,19 @@ __global__ static void initialize_gpu_kernel(double* u,
 }
 
 static void release_gpu(){
-	cudaFree(u_device);
-	cudaFree(forcing_device);
-	cudaFree(rhs_device);
-	cudaFree(rho_i_device);
-	cudaFree(us_device);
-	cudaFree(vs_device);
-	cudaFree(ws_device);
-	cudaFree(qs_device);
-	cudaFree(speed_device);
-	cudaFree(square_device);
-	cudaFree(lhs_device);
-	cudaFree(rhs_buffer_device);
-	cudaFree(rms_buffer_device);
+	hipFree(u_device);
+	hipFree(forcing_device);
+	hipFree(rhs_device);
+	hipFree(rho_i_device);
+	hipFree(us_device);
+	hipFree(vs_device);
+	hipFree(ws_device);
+	hipFree(qs_device);
+	hipFree(speed_device);
+	hipFree(square_device);
+	hipFree(lhs_device);
+	hipFree(rhs_buffer_device);
+	hipFree(rms_buffer_device);
 }
 
 static void rhs_norm_gpu(double rms[]){
@@ -1573,7 +1570,7 @@ static void rhs_norm_gpu(double rms[]){
 	timer_stop(PROFILING_RHS_NORM_2);
 #endif
 
-	cudaMemcpy(rms, rms_buffer_device, 5*sizeof(double), cudaMemcpyDeviceToHost);
+	hipMemcpy(rms, rms_buffer_device, 5*sizeof(double), hipMemcpyDeviceToHost);
 }
 
 __global__ static void rhs_norm_gpu_kernel_1(double* rms,
@@ -1815,107 +1812,107 @@ static void set_constants(){
 	zzcon4=c3c4tz3*con16*tz3;
 	zzcon5=c3c4tz3*c1c5*tz3;
 	/* */
-	cudaMemcpyToSymbol(constants_device::ce, &ce, 13*5*sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dt, &dt, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::bt, &bt, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1, &c1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2, &c2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3, &c3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c4, &c4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c5, &c5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dnxm1, &dnxm1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dnym1, &dnym1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dnzm1, &dnzm1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1c2, &c1c2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1c5, &c1c5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4, &c3c4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c1345, &c1345, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::conz1, &conz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx1, &tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx2, &tx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tx3, &tx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty1, &ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty2, &ty2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::ty3, &ty3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz1, &tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz2, &tz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::tz3, &tz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx1, &dx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx2, &dx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx3, &dx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx4, &dx4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx5, &dx5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy1, &dy1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy2, &dy2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy3, &dy3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy4, &dy4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy5, &dy5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz1, &dz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz2, &dz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz3, &dz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz4, &dz4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz5, &dz5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dxmax, &dxmax, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dymax, &dymax, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dzmax, &dzmax, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dssp, &dssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c4dssp, &c4dssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c5dssp, &c5dssp, sizeof(double));	
-	cudaMemcpyToSymbol(constants_device::dttx1, &dttx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dttx2, &dttx2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dtty1, &dtty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dtty2, &dtty2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dttz1, &dttz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dttz2, &dttz2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2dttx1, &c2dttx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2dtty1, &c2dtty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2dttz1, &c2dttz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dtdssp, &dtdssp, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz1, &comz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz4, &comz4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz5, &comz5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::comz6, &comz6, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4tx3, &c3c4tx3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4ty3, &c3c4ty3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c3c4tz3, &c3c4tz3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx1tx1, &dx1tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx2tx1, &dx2tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx3tx1, &dx3tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx4tx1, &dx4tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dx5tx1, &dx5tx1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy1ty1, &dy1ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy2ty1, &dy2ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy3ty1, &dy3ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy4ty1, &dy4ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dy5ty1, &dy5ty1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz1tz1, &dz1tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz2tz1, &dz2tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz3tz1, &dz3tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz4tz1, &dz4tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::dz5tz1, &dz5tz1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::c2iv, &c2iv, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::con43, &con43, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::con16, &con16, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon1, &xxcon1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon2, &xxcon2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon3, &xxcon3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon4, &xxcon4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::xxcon5, &xxcon5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon1, &yycon1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon2, &yycon2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon3, &yycon3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon4, &yycon4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::yycon5, &yycon5, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon1, &zzcon1, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon2, &zzcon2, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon3, &zzcon3, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon4, &zzcon4, sizeof(double));
-	cudaMemcpyToSymbol(constants_device::zzcon5, &zzcon5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ce), &ce, 13*5*sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dt), &dt, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::bt), &bt, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1), &c1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2), &c2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3), &c3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c4), &c4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c5), &c5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dnxm1), &dnxm1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dnym1), &dnym1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dnzm1), &dnzm1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1c2), &c1c2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1c5), &c1c5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4), &c3c4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c1345), &c1345, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::conz1), &conz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx1), &tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx2), &tx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tx3), &tx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty1), &ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty2), &ty2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::ty3), &ty3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz1), &tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz2), &tz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::tz3), &tz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx1), &dx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx2), &dx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx3), &dx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx4), &dx4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx5), &dx5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy1), &dy1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy2), &dy2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy3), &dy3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy4), &dy4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy5), &dy5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz1), &dz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz2), &dz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz3), &dz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz4), &dz4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz5), &dz5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dxmax), &dxmax, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dymax), &dymax, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dzmax), &dzmax, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dssp), &dssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c4dssp), &c4dssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c5dssp), &c5dssp, sizeof(double));	
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttx1), &dttx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttx2), &dttx2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dtty1), &dtty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dtty2), &dtty2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttz1), &dttz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dttz2), &dttz2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2dttx1), &c2dttx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2dtty1), &c2dtty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2dttz1), &c2dttz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dtdssp), &dtdssp, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz1), &comz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz4), &comz4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz5), &comz5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::comz6), &comz6, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4tx3), &c3c4tx3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4ty3), &c3c4ty3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c3c4tz3), &c3c4tz3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx1tx1), &dx1tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx2tx1), &dx2tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx3tx1), &dx3tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx4tx1), &dx4tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dx5tx1), &dx5tx1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy1ty1), &dy1ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy2ty1), &dy2ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy3ty1), &dy3ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy4ty1), &dy4ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dy5ty1), &dy5ty1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz1tz1), &dz1tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz2tz1), &dz2tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz3tz1), &dz3tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz4tz1), &dz4tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::dz5tz1), &dz5tz1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::c2iv), &c2iv, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::con43), &con43, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::con16), &con16, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon1), &xxcon1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon2), &xxcon2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon3), &xxcon3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon4), &xxcon4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::xxcon5), &xxcon5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon1), &yycon1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon2), &yycon2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon3), &yycon3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon4), &yycon4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::yycon5), &yycon5, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon1), &zzcon1, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon2), &zzcon2, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon3), &zzcon3, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon4), &zzcon4, sizeof(double));
+	hipMemcpyToSymbol(HIP_SYMBOL(constants_device::zzcon5), &zzcon5, sizeof(double));
 }
 
 static void setup_gpu(){
 	/*
-	 * struct cudaDeviceProp{
+	 * struct hipDeviceProp_t{
 	 *  char name[256];
 	 *  size_t totalGlobalMem;
 	 *  size_t sharedMemPerBlock;
@@ -1944,134 +1941,17 @@ static void setup_gpu(){
 	 * }
 	 */
 	/* amount of available devices */ 
-	cudaGetDeviceCount(&total_devices);
+	hipGetDeviceCount(&total_devices);
 
 	/* define gpu_device */
 	if(total_devices==0){
 		printf("\n\n\nNo Nvidia GPU found!\n\n\n");
 		exit(-1);
-	}else if((GPU_DEVICE>=0)&&
-			(GPU_DEVICE<total_devices)){
-		gpu_device_id = GPU_DEVICE;
 	}else{
 		gpu_device_id = 0;
 	}
-	cudaSetDevice(gpu_device_id);	
-	cudaGetDeviceProperties(&gpu_device_properties, gpu_device_id);
-
-	/* define threads_per_block */
-	if((SP_THREADS_PER_BLOCK_ON_ADD>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_ADD<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ADD = SP_THREADS_PER_BLOCK_ON_ADD;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ADD = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_1>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_COMPUTE_RHS_1 = SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_COMPUTE_RHS_1 = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_2>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_COMPUTE_RHS_2 = SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_COMPUTE_RHS_2 = gpu_device_properties.warpSize;
-	}	
-	if((SP_THREADS_PER_BLOCK_ON_ERROR_NORM_1>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_ERROR_NORM_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERROR_NORM_1 = SP_THREADS_PER_BLOCK_ON_ERROR_NORM_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERROR_NORM_1 = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_ERROR_NORM_2>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_ERROR_NORM_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_ERROR_NORM_2 = SP_THREADS_PER_BLOCK_ON_ERROR_NORM_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_ERROR_NORM_2 = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_EXACT_RHS_1>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_EXACT_RHS_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_1 = SP_THREADS_PER_BLOCK_ON_EXACT_RHS_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_1 = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_EXACT_RHS_2>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_EXACT_RHS_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_2 = SP_THREADS_PER_BLOCK_ON_EXACT_RHS_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_2 = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_EXACT_RHS_3>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_EXACT_RHS_3<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_3 = SP_THREADS_PER_BLOCK_ON_EXACT_RHS_3;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_3 = gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_EXACT_RHS_4>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_EXACT_RHS_4<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_EXACT_RHS_4 = SP_THREADS_PER_BLOCK_ON_EXACT_RHS_4;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_EXACT_RHS_4=gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_INITIALIZE>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_INITIALIZE<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_INITIALIZE = SP_THREADS_PER_BLOCK_ON_INITIALIZE;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_INITIALIZE=gpu_device_properties.warpSize;
-	}
-	if((SP_THREADS_PER_BLOCK_ON_RHS_NORM_1>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_RHS_NORM_1<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_NORM_1 = SP_THREADS_PER_BLOCK_ON_RHS_NORM_1;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_NORM_1 = gpu_device_properties.warpSize;
-	}	
-	if((SP_THREADS_PER_BLOCK_ON_RHS_NORM_2>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_RHS_NORM_2<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_RHS_NORM_2 = SP_THREADS_PER_BLOCK_ON_RHS_NORM_2;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_RHS_NORM_2 = gpu_device_properties.warpSize;
-	}	
-	if((SP_THREADS_PER_BLOCK_ON_TXINVR>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_TXINVR<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_TXINVR = SP_THREADS_PER_BLOCK_ON_TXINVR;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_TXINVR = gpu_device_properties.warpSize;
-	}	
-	if((SP_THREADS_PER_BLOCK_ON_X_SOLVE>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_X_SOLVE<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_X_SOLVE = SP_THREADS_PER_BLOCK_ON_X_SOLVE;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_X_SOLVE = gpu_device_properties.warpSize;
-	}	
-	if((SP_THREADS_PER_BLOCK_ON_Y_SOLVE>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_Y_SOLVE<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Y_SOLVE = SP_THREADS_PER_BLOCK_ON_Y_SOLVE;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Y_SOLVE = gpu_device_properties.warpSize;
-	}	
-	if((SP_THREADS_PER_BLOCK_ON_Z_SOLVE>=1)&&
-			(SP_THREADS_PER_BLOCK_ON_Z_SOLVE<=gpu_device_properties.maxThreadsPerBlock)){
-		THREADS_PER_BLOCK_ON_Z_SOLVE = SP_THREADS_PER_BLOCK_ON_Z_SOLVE;
-	}
-	else{
-		THREADS_PER_BLOCK_ON_Z_SOLVE = gpu_device_properties.warpSize;
-	}	
+	hipSetDevice(gpu_device_id);	
+	hipGetDeviceProperties(&gpu_device_properties, gpu_device_id);
 
 	int gridsize=nx*ny*nz;
 	int facesize=max(max(nx*ny, nx*nz), ny*nz);
@@ -2088,19 +1968,19 @@ static void setup_gpu(){
 	size_lhs_device=sizeof(double)*(9*gridsize);
 	size_rhs_buffer_device=sizeof(double)*(5*gridsize);
 	size_rms_buffer_device=sizeof(double)*(5*facesize);
-	cudaMalloc(&u_device, size_u_device);
-	cudaMalloc(&forcing_device, size_forcing_device);
-	cudaMalloc(&rhs_device, size_rhs_device);
-	cudaMalloc(&rho_i_device, size_rho_i_device);
-	cudaMalloc(&us_device, size_us_device);
-	cudaMalloc(&vs_device, size_vs_device);
-	cudaMalloc(&ws_device, size_ws_device);
-	cudaMalloc(&qs_device, size_qs_device);
-	cudaMalloc(&speed_device, size_speed_device);
-	cudaMalloc(&square_device, size_square_device);
-	cudaMalloc(&lhs_device, size_lhs_device);
-	cudaMalloc(&rhs_buffer_device, size_rhs_buffer_device);
-	cudaMalloc(&rms_buffer_device, size_rms_buffer_device);
+	hipMalloc(&u_device, size_u_device);
+	hipMalloc(&forcing_device, size_forcing_device);
+	hipMalloc(&rhs_device, size_rhs_device);
+	hipMalloc(&rho_i_device, size_rho_i_device);
+	hipMalloc(&us_device, size_us_device);
+	hipMalloc(&vs_device, size_vs_device);
+	hipMalloc(&ws_device, size_ws_device);
+	hipMalloc(&qs_device, size_qs_device);
+	hipMalloc(&speed_device, size_speed_device);
+	hipMalloc(&square_device, size_square_device);
+	hipMalloc(&lhs_device, size_lhs_device);
+	hipMalloc(&rhs_buffer_device, size_rhs_buffer_device);
+	hipMalloc(&rms_buffer_device, size_rms_buffer_device);
 }
 
 /*
diff --git a/CUDA/common/c_print_results.cpp b/CUDA/common/c_print_results.cpp
index 5ba682b..9d302ac 100644
--- a/CUDA/common/c_print_results.cpp
+++ b/CUDA/common/c_print_results.cpp
@@ -64,11 +64,6 @@ void c_print_results(char* name,
 		int passed_verification,
 		char* npbversion,
 		char* compiletime,
-		char* compilerversion,
-		char* libversion,
-		char* cpu_device,
-		char* gpu_device,
-		char* gpu_config,
 		char* cc,
 		char* clink,
 		char* c_lib,
@@ -76,86 +71,79 @@ void c_print_results(char* name,
 		char* cflags,
 		char* clinkflags,
 		char* rand){
-			printf("\n\n %s Benchmark Completed\n", name);
-			printf(" class_npb       =                        %c\n", class_npb);
-			if((name[0]=='I')&&(name[1]=='S')){
-				if(n3==0){
-					long nn = n1;
-					if(n2!=0){nn*=n2;}
-					printf(" Size            =             %12ld\n", nn); /* as in IS */
-				}else{
-					printf(" Size            =             %4dx%4dx%4d\n", n1,n2,n3);
-				}
-			}else{
-				char size[16];
-				int j;
-				if((n2==0) && (n3==0)){
-					if((name[0]=='E')&&(name[1]=='P')){
-						sprintf(size, "%15.0lf", pow(2.0, n1));
-						j = 14;
-						if(size[j] == '.'){
-							size[j] = ' '; 
-							j--;
-						}
-						size[j+1] = '\0';
-						printf(" Size            =          %15s\n", size);
-					}else{
-						printf(" Size            =             %12d\n", n1);
-					}
-				}else{
-					printf(" Size            =           %4dx%4dx%4d\n", n1, n2, n3);
+	printf("\n\n %s Benchmark Completed\n", name);
+	printf(" class_npb       =                        %c\n", class_npb);
+	if((name[0]=='I')&&(name[1]=='S')){
+		if(n3==0){
+			long nn = n1;
+			if(n2!=0){nn*=n2;}
+			printf(" Size            =             %12ld\n", nn); /* as in IS */
+		}else{
+			printf(" Size            =             %4dx%4dx%4d\n", n1,n2,n3);
+		}
+	}else{
+		char size[16];
+		int j;
+		if((n2==0) && (n3==0)){
+			if((name[0]=='E')&&(name[1]=='P')){
+				sprintf(size, "%15.0lf", pow(2.0, n1));
+				j = 14;
+				if(size[j] == '.'){
+					size[j] = ' '; 
+					j--;
 				}
-			}	
-			printf(" Iterations      =             %12d\n", niter); 
-			printf(" Time in seconds =             %12.2f\n", t);
-			printf(" Mop/s total     =             %12.2f\n", mops);
-			printf(" Operation type  = %24s\n", optype);
-			if(passed_verification < 0){
-				printf( " Verification    =            NOT PERFORMED\n");
-			}else if(passed_verification){
-				printf(" Verification    =               SUCCESSFUL\n");
+				size[j+1] = '\0';
+				printf(" Size            =          %15s\n", size);
 			}else{
-				printf(" Verification    =             UNSUCCESSFUL\n");
+				printf(" Size            =             %12d\n", n1);
 			}
-			printf(" Version         =             %12s\n", npbversion);
-			printf(" Compile date    =             %12s\n", compiletime);
-			printf(" NVCC version    =             %12s\n", compilerversion);
-			printf(" CUDA version    =             %12s\n", libversion);
-			printf("\n Compile options:\n");
-			printf("    CC           = %s\n", cc);
-			printf("    CLINK        = %s\n", clink);
-			printf("    C_LIB        = %s\n", c_lib);
-			printf("    C_INC        = %s\n", c_inc);
-			printf("    CFLAGS       = %s\n", cflags);
-			printf("    CLINKFLAGS   = %s\n", clinkflags);
-			printf("    RAND         = %s\n", rand);
-			printf("\n Hardware:\n");
-			printf("    CPU device   = %s\n", cpu_device);
-			printf("    GPU device   = %s\n", gpu_device);
-			printf("\n Software:\n");
-			printf("    Parameters   = %s\n", gpu_config);
+		}else{
+			printf(" Size            =           %4dx%4dx%4d\n", n1, n2, n3);
+		}
+	}	
+	printf(" Iterations      =             %12d\n", niter); 
+	printf(" Time in seconds =             %12.2f\n", t);
+	printf(" Mop/s total     =             %12.2f\n", mops);
+	printf(" Operation type  = %24s\n", optype);
+	if(passed_verification < 0){
+		printf( " Verification    =            NOT PERFORMED\n");
+	}else if(passed_verification){
+		printf(" Verification    =               SUCCESSFUL\n");
+	}else{
+		printf(" Verification    =             UNSUCCESSFUL\n");
+	}
+	printf(" Version         =             %12s\n", npbversion);
+	printf(" Compile date    =             %12s\n", compiletime);
+	printf("\n Compile options:\n");
+	printf("    CC           = %s\n", cc);
+	printf("    CLINK        = %s\n", clink);
+	printf("    C_LIB        = %s\n", c_lib);
+	printf("    C_INC        = %s\n", c_inc);
+	printf("    CFLAGS       = %s\n", cflags);
+	printf("    CLINKFLAGS   = %s\n", clinkflags);
+	printf("    RAND         = %s\n", rand);
 #ifdef SMP
-			evalue = getenv("MP_SET_NUMTHREADS");
-			printf("   MULTICPUS = %s\n", evalue);
+	evalue = getenv("MP_SET_NUMTHREADS");
+	printf("   MULTICPUS = %s\n", evalue);
 #endif    
-			/* 
-			 * printf(" Please send the results of this run to:\n\n");
-			 * printf(" NPB Development Team\n");
-			 * printf(" Internet: npb@nas.nasa.gov\n \n");
-			 * printf(" If email is not available, send this to:\n\n");
-			 * printf(" MS T27A-1\n");
-			 * printf(" NASA Ames Research Center\n");
-			 * printf(" Moffett Field, CA  94035-1000\n\n");
-			 * printf(" Fax: 650-604-3957\n\n");
-			 */
-			printf("\n");
-			printf("----------------------------------------------------------------------\n");
-			printf(" NPB-CPP is developed by:\n");
-			printf("            Dalvan Griebler <dalvangriebler@gmail.com>\n");
-			printf("            Gabriell Araujo <hexenoften@gmail.com>\n");
-			printf("            Jnior Lff <loffjh@gmail.com>\n");
-			printf("\n");
-			printf(" In case of problems, send an email to us\n");
-			printf("----------------------------------------------------------------------\n");
-			printf("\n");
-		}
+	/* 
+	 * printf(" Please send the results of this run to:\n\n");
+	 * printf(" NPB Development Team\n");
+	 * printf(" Internet: npb@nas.nasa.gov\n \n");
+	 * printf(" If email is not available, send this to:\n\n");
+	 * printf(" MS T27A-1\n");
+	 * printf(" NASA Ames Research Center\n");
+	 * printf(" Moffett Field, CA  94035-1000\n\n");
+	 * printf(" Fax: 650-604-3957\n\n");
+	 */
+	printf("\n\n");
+	printf("----------------------------------------------------------------------\n");
+	printf(" NPB-CPP is developed by:\n");
+	printf("            Dalvan Griebler <dalvangriebler@gmail.com>\n");
+	printf("            Jnior Lff <loffjh@gmail.com>\n");
+	printf("            Gabriell Araujo <hexenoften@gmail.com>\n");
+	printf("\n");
+	printf(" In case of problems, send an email to us\n");
+	printf("----------------------------------------------------------------------\n");
+	printf("\n");
+}
diff --git a/CUDA/common/c_timers.cpp b/CUDA/common/c_timers.cpp
index c43883d..7552ea3 100644
--- a/CUDA/common/c_timers.cpp
+++ b/CUDA/common/c_timers.cpp
@@ -47,7 +47,6 @@
 
 #include "wtime.hpp"
 #include <cstdlib>
-#include <cuda.h>
 
 /*  prototype  */
 void wtime(double*);
@@ -81,11 +80,11 @@ void timer_start(int n){
 /******            T  I  M  E  R  _  S  T  O  P             ******/
 /*****************************************************************/
 void timer_stop(int n){
-	cudaDeviceSynchronize();
 	double t, now;
 	now = elapsed_time();
 	t = now - start[n];
 	elapsed[n] += t;
+
 }
 
 /*****************************************************************/
diff --git a/CUDA/common/npb-CPP.hpp b/CUDA/common/npb-CPP.hpp
index 516d434..994e55e 100644
--- a/CUDA/common/npb-CPP.hpp
+++ b/CUDA/common/npb-CPP.hpp
@@ -104,17 +104,11 @@ static inline dcomplex dcomplex_div(dcomplex z1, dcomplex z2){
 
 extern double randlc(double*, double);
 extern void vranlc(int, double*, double, double*);
-
 extern void timer_clear(int);
 extern void timer_start(int);
 extern void timer_stop(int);
 extern double timer_read(int);
 
-extern void timer_clear();
-extern void timer_start();
-extern void timer_stop();
-extern double timer_read();
-
 extern void c_print_results(char* name,
 		char class_npb,
 		int n1,
@@ -127,11 +121,6 @@ extern void c_print_results(char* name,
 		int passed_verification,
 		char* npbversion,
 		char* compiletime,
-		char* compilerversion,
-		char* libversion,
-		char* cpu_device,
-		char* gpu_device,
-		char* gpu_config,
 		char* cc,
 		char* clink,
 		char* c_lib,
diff --git a/CUDA/common/wtime.hpp b/CUDA/common/wtime.hpp
index dcde956..ff96398 100644
--- a/CUDA/common/wtime.hpp
+++ b/CUDA/common/wtime.hpp
@@ -54,5 +54,5 @@
 #elif defined(CRAY)
 #define wtime WTIME
 #else
-#define wtime wtime_
+#define wtime wtime
 #endif
diff --git a/CUDA/config/make.def b/CUDA/config/make.def
index bdc3180..5ad6500 100644
--- a/CUDA/config/make.def
+++ b/CUDA/config/make.def
@@ -1,5 +1,12 @@
+#---------------------------------------------------------------------------
+#
+#                SITE- AND/OR PLATFORM-SPECIFIC DEFINITIONS. 
+#
+#---------------------------------------------------------------------------
+
 #---------------------------------------------------------------------------
 # Items in this file will need to be changed for each platform.
+# (Note these definitions are inconsistent with NPB2.1.)
 #---------------------------------------------------------------------------
 
 #---------------------------------------------------------------------------
@@ -17,40 +24,57 @@
 # linking is done with       $(CLINK) $(C_LIB) $(CLINKFLAGS)
 #---------------------------------------------------------------------------
 
-#---------------------------------------------------------------------------
-# This is the compiler used for CUDA programs
-#---------------------------------------------------------------------------
-COMPUTE_CAPABILITY = -gencode arch=compute_61,code=sm_61
+ifndef $(INJECT_CODE_LLVM)
+INJECT_CODE_LLVM = 0
+endif
 
-OPENMP = -Xcompiler -fopenmp -lgomp
+RANDINT = $(shell python3 -c 'from random import randint; print(randint(1000000000, 9999999999));')
 
-REGISTERS = 
+ifeq ($(INJECT_CODE_LLVM), 1)
+INST_FLAGS = -Wno-everything -g -include ${HOME}/FloatGuard/inst_pass/Inst/InstStub.h -fpass-plugin=${HOME}/FloatGuard/inst_pass/libInstPass.so -DRANDINT=${RANDINT} \
+			 -DEXP_FLAG_TOTAL=0x5F2F0
+else
+INST_FLAGS = -Wno-everything -g -include ${HOME}/FloatGuard/inst_pass/Inst/InstStub.h -DRANDINT=${RANDINT} \
+			 -DEXP_FLAG_TOTAL=0x5F2F0
+endif
 
-####-maxrregcount 50
+ifdef $(NO_BRANCH)
+INST_FLAGS += -DNO_BRANCH=1
+endif
 
-OPTIMIZATIONS = -use_fast_math -Xptxas --preserve-relocs -Xptxas -O3
+ifndef $(INJECT_CODE_CLANG)
+INJECT_CODE_CLANG = 0
+endif
 
-CPP_14 = -std=c++14
+ifeq ($(INJECT_CODE_CLANG), 1)
+INST_FLAGS += -c -emit-llvm -Xclang -load -Xclang FloatGuard-plugin.so -Xclang -plugin -Xclang inject-fp-exception 
+endif
 
-NO_NPB_CPP_CONVENTIONS = -DDO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION
 
-PROFILING = -DPROFILING
+#---------------------------------------------------------------------------
+# This is the C compiler used for OpenMP programs
+#---------------------------------------------------------------------------
+COMPUTE_CAPABILITY=
+# ${COMPUTE_CAPABILITY}
 
-EXTRA_STUFF = ${COMPUTE_CAPABILITY} ${OPENMP} ${OPTIMIZATIONS} ${REGISTERS}
+OPENMP_ENABLE=
+# ${OPENMP_ENABLE}
 
-NVCC = nvcc
+EXTRA_STUFF=-O3 -ffast-math
+# ${EXTRA_STUFF}
 
-CC = ${NVCC} ${EXTRA_STUFF}
+CC = hipcc ${COMPUTE_CAPABILITY} ${OPENMP_ENABLE} ${EXTRA_STUFF} ${INST_FLAGS}
+
+#backup old command
+#CC = nvcc -rdc=true -std=c++14 -gencode arch=compute_61,code=sm_61 -Xcompiler -fopenmp -lgomp -Xcompiler -mcmodel=medium
 
-#---------------------------------------------------------------------------
 # This links C programs; usually the same as ${CC}
-#---------------------------------------------------------------------------
-CLINK = $(CC)
+CLINK	= $(CC)
 
 #---------------------------------------------------------------------------
 # These macros are passed to the linker 
 #---------------------------------------------------------------------------
-C_LIB = -lm 
+C_LIB  = ${HOME}/FloatGuard/inst_pass/Inst/InstStub.o -lm 
 
 #---------------------------------------------------------------------------
 # These macros are passed to the compiler 
@@ -60,37 +84,42 @@ C_INC = -I../common
 #---------------------------------------------------------------------------
 # Global *compile time* flags for C programs
 #---------------------------------------------------------------------------
-CFLAGS = -O3
+CFLAGS	= -O3
+# CFLAGS = -g
 
 #---------------------------------------------------------------------------
 # Global *link time* flags. Flags for increasing maximum executable 
 # size usually go here. 
 #---------------------------------------------------------------------------
-CLINKFLAGS = -O3
+CLINKFLAGS = -O3 -fgpu-rdc --hip-link 
+
 
 #---------------------------------------------------------------------------
 # Utilities C:
 #
-# This is the C compiler used to compile C utilities. Flags required by 
+# This is the C compiler used to compile C utilities.  Flags required by 
 # this compiler go here also; typically there are few flags required; hence 
 # there are no separate macros provided for such flags.
 #---------------------------------------------------------------------------
 UCC	= cc
 
+
 #---------------------------------------------------------------------------
-# Destination of executables, relative to subdirs of the main directory. 
+# Destination of executables, relative to subdirs of the main directory. . 
 #---------------------------------------------------------------------------
 BINDIR	= ../bin
 
+
 #---------------------------------------------------------------------------
 # The variable RAND controls which random number generator 
 # is used. It is described in detail in Doc/README.install. 
 # Use "randi8" unless there is a reason to use another one. 
 # Other allowed values are "randi8_safe", "randdp" and "randdpvec"
 #---------------------------------------------------------------------------
-# RAND = randi8
+# RAND   = randi8
 # The following is highly reliable but may be slow:
-RAND = randdp
+RAND   = randdp
+
 
 #---------------------------------------------------------------------------
 # The variable WTIME is the name of the wtime source code module in the
@@ -98,7 +127,8 @@ RAND = randdp
 # For most machines,       use wtime.c
 # For SGI power challenge: use wtime_sgi64.c
 #---------------------------------------------------------------------------
-WTIME = wtime.cpp
+WTIME  = wtime.cpp
+
 
 #---------------------------------------------------------------------------
 # Enable if either Cray or IBM: 
@@ -106,5 +136,7 @@ WTIME = wtime.cpp
 # This is used by the C compiler to pass the machine name to common/wtime.h,
 # where the C/Fortran binding interface format is determined
 #---------------------------------------------------------------------------
-# MACHINE = -DCRAY
-# MACHINE = -DIBM
+# MACHINE	=	-DCRAY
+# MACHINE	=	-DIBM
+
+
diff --git a/CUDA/sys/make.common b/CUDA/sys/make.common
index 578cc41..abb3a28 100644
--- a/CUDA/sys/make.common
+++ b/CUDA/sys/make.common
@@ -36,8 +36,8 @@ ${COMMON}/c_print_results.o: ${COMMON}/c_print_results.cpp
 ${COMMON}/timers.o: ${COMMON}/timers.f
 	cd ${COMMON}; ${FCOMPILE} timers.f
 
-${COMMON}/c_timers.o: ${COMMON}/c_timers.cu
-	cd ${COMMON}; ${CCOMPILE} c_timers.cu
+${COMMON}/c_timers.o: ${COMMON}/c_timers.cpp
+	cd ${COMMON}; ${CCOMPILE} c_timers.cpp
 
 ${COMMON}/wtime.o: ${COMMON}/${WTIME}
 	cd ${COMMON}; ${CCOMPILE} ${MACHINE} ${COMMON}/${WTIME}
diff --git a/CUDA/sys/setparams.cpp b/CUDA/sys/setparams.cpp
index 7fa832c..80b8277 100644
--- a/CUDA/sys/setparams.cpp
+++ b/CUDA/sys/setparams.cpp
@@ -84,19 +84,12 @@
 #include <cstring>
 #include <ctime>
 
-/*
- * gpu.config and cpu model
- */
-#define GPU_CONFIG_PATH ("../config/gpu.config")
-#define NO_CONFIG ("-1")
-#define CPU_INFO_PATH ("/proc/cpuinfo")
-#define NO_CPU_INFO ("No info")
-
 /*
  * this is the master version number for this set of 
  * NPB benchmarks. it is in an obscure place so people
  * won't accidentally change it. 
  */
+
 #define VERSION "4.1"
 
 /* controls verbose output from setparams */
@@ -108,10 +101,6 @@
 #define FINDENT  "        "
 #define CONTINUE "     > "
 
-char* cpu_model();
-char* read_gpu_config(char* data);
-void write_gpu_config(FILE* fp);
-void profiling_flag(FILE* definitions_file);
 void get_info(char *argv[], int *typep, char *classp);
 void check_info(int type, char class_npb);
 void read_info(int type, char *classp);
@@ -128,7 +117,7 @@ void write_is_info(FILE *fp, char class_npb);
 void write_compiler_info(int type, FILE *fp);
 void write_convertdouble_info(int type, FILE *fp);
 void check_line(char *line, char *label, char *val);
-int check_include_line(char *line, char *filename);
+int  check_include_line(char *line, char *filename);
 void put_string(FILE *fp, char *name, char *val);
 void put_def_string(FILE *fp, char *name, char *val);
 void put_def_variable(FILE *fp, char *name, char *val);
@@ -195,222 +184,6 @@ main(int argc, char *argv[]){
 	exit(0);
 }
 
-void profiling_flag(FILE* definitions_file){
-	FILE* file;
-	if((file = fopen("../timer.flag", "r")) != NULL){
-		fprintf(definitions_file, "#define PROFILING\n");
-		fclose(file);
-	}
-}
-
-/*
- * read_nvcc_cuda_version(): return a string with the nvcc/cuda version
- */
-char* read_nvcc_cuda_version(){
-	FILE *file;
-	char command[64];
-	char result[64];
-	char* pointer;
-	sprintf(command, "nvcc --version |grep release |awk '{print $6}'"); 
-	file = popen(command,"r"); 
-	fgets(result, 1024 , file);
-	pointer=result;
-	strtok(pointer, "\n");
-	fclose(file);
-	return (++pointer);
-}
-
-/*
- * cpu_model(): return a string with the cpu model
- */
-char* cpu_model(){
-	FILE* file = fopen((char*)CPU_INFO_PATH, "r");
-	char* error = (char*)NO_CPU_INFO;
-	char* line = NULL;
-	char* cpu_model;
-	size_t n = 0;	
-	if(file == NULL){
-		return error;
-	}		
-	while(getline(&line, &n, file) > 0){
-		if(strstr(line, "model name")){
-			cpu_model=line;
-			while(*cpu_model != ':'){
-				cpu_model++;
-			} cpu_model++;
-			while(*cpu_model == ' '){
-				cpu_model++;
-			}
-			strtok(cpu_model, "\n");
-			fclose(file);
-			return cpu_model;
-		}
-	}
-	fclose(file);
-	return error;
-}
-
-/*
- * read_gpu_config(): read a value from gpu.config file
- */
-char* read_gpu_config(char* data){
-	FILE* file = fopen((char*)GPU_CONFIG_PATH, "r");
-	char* line = NULL;
-	char* data_read;
-	size_t n = 0;	
-	if(file == NULL){	
-		return (char*)NO_CONFIG;
-	}	
-	while(getline(&line, &n, file) > 0){
-		if(strstr(line, (char*)data)){
-			data_read=line;
-			while(*data_read != '='){
-				data_read++;
-			} data_read++;
-			while(*data_read == ' '){
-				data_read++;
-			}
-			strtok(data_read, "\n");
-			fclose(file);
-			return data_read;
-		}
-	}
-	fclose(file);
-	return (char*)NO_CONFIG;
-}
-
-/*
- * write_gpu_config(): write the values of the gpu.config file
- */
-void write_gpu_config(FILE *fp){
-	/* gpu device */
-	put_def_variable(fp, (char*)"GPU_DEVICE", (char*)read_gpu_config((char*)"GPU_DEVICE"));
-
-	/* bt */
-	/* new */
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_ADD", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_ADD"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_2"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_3", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_3"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_4", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_4"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_5", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_5"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_6", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_6"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_7", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_7"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_8", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_8"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_9", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_9"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_X_SOLVE_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_X_SOLVE_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_X_SOLVE_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_X_SOLVE_2"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_X_SOLVE_3", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_X_SOLVE_3"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_Y_SOLVE_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_Y_SOLVE_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_Y_SOLVE_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_Y_SOLVE_2"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_Y_SOLVE_3", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_Y_SOLVE_3"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_Z_SOLVE_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_Z_SOLVE_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_Z_SOLVE_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_Z_SOLVE_2"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_Z_SOLVE_3", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_Z_SOLVE_3"));
-	/* old */
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_2"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_3", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_3"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_4", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_EXACT_RHS_4"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_ERROR_NORM_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_ERROR_NORM_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_ERROR_NORM_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_ERROR_NORM_2"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_INITIALIZE", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_INITIALIZE"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_NORM_1", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_NORM_1"));
-	put_def_variable(fp, (char*)"BT_THREADS_PER_BLOCK_ON_RHS_NORM_2", (char*)read_gpu_config((char*)"BT_THREADS_PER_BLOCK_ON_RHS_NORM_2"));	
-
-	/* cg */
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_ONE", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_ONE"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_TWO", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_TWO"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_THREE", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_THREE"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_SIX", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_SIX"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_NINE", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_NINE"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_TEN", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_TEN"));
-	put_def_variable(fp, (char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN", (char*)read_gpu_config((char*)"CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN"));
-
-	/* ep */
-	put_def_variable(fp, (char*)"EP_THREADS_PER_BLOCK", (char*)read_gpu_config((char*)"EP_THREADS_PER_BLOCK"));
-
-	/* ft */
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_CHECKSUM", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_CHECKSUM"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_COMPUTE_INDEXMAP", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_COMPUTE_INDEXMAP"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_COMPUTE_INITIAL_CONDITIONS", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_COMPUTE_INITIAL_CONDITIONS"));	
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_EVOLVE", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_EVOLVE"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTX_1", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTX_1"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTX_2", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTX_2"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTX_3", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTX_3"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTY_1", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTY_1"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTY_2", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTY_2"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTY_3", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTY_3"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTZ_1", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTZ_1"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTZ_2", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTZ_2"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_FFTZ_3", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_FFTZ_3"));
-	put_def_variable(fp, (char*)"FT_THREADS_PER_BLOCK_ON_INIT_UI", (char*)read_gpu_config((char*)"FT_THREADS_PER_BLOCK_ON_INIT_UI"));
-
-	/* is */
-	put_def_variable(fp, (char*)"IS_THREADS_PER_BLOCK_ON_CREATE_SEQ", (char*)read_gpu_config((char*)"IS_THREADS_PER_BLOCK_ON_CREATE_SEQ"));	
-	put_def_variable(fp, (char*)"IS_THREADS_PER_BLOCK_ON_FULL_VERIFY", (char*)read_gpu_config((char*)"IS_THREADS_PER_BLOCK_ON_FULL_VERIFY"));
-	put_def_variable(fp, (char*)"IS_THREADS_PER_BLOCK_ON_RANK", (char*)read_gpu_config((char*)"IS_THREADS_PER_BLOCK_ON_RANK"));
-
-	/* mg */
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_COMM3", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_COMM3"));
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_INTERP", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_INTERP"));
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_NORM2U3", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_NORM2U3"));
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_PSINV", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_PSINV"));
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_RESID", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_RESID"));
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_RPRJ3", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_RPRJ3"));
-	put_def_variable(fp, (char*)"MG_THREADS_PER_BLOCK_ON_ZERO3", (char*)read_gpu_config((char*)"MG_THREADS_PER_BLOCK_ON_ZERO3"));
-
-	/* lu */
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_ERHS_1", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_ERHS_1"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_ERHS_2", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_ERHS_2"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_ERHS_3", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_ERHS_3"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_ERHS_4", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_ERHS_4"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_ERROR", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_ERROR"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_NORM", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_NORM"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_JACLD_BLTS", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_JACLD_BLTS"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_JACU_BUTS", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_JACU_BUTS"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_L2NORM", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_L2NORM"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_1", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_1"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_2", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_2"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_3", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_3"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_4", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_PINTGR_4"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_RHS_1", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_RHS_1"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_RHS_2", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_RHS_2"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_RHS_3", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_RHS_3"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_RHS_4", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_RHS_4"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_SETBV_1", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_SETBV_1"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_SETBV_2", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_SETBV_2"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_SETBV_3", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_SETBV_3"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_SETIV", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_SETIV"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_SSOR_1", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_SSOR_1"));
-	put_def_variable(fp, (char*)"LU_THREADS_PER_BLOCK_ON_SSOR_2", (char*)read_gpu_config((char*)"LU_THREADS_PER_BLOCK_ON_SSOR_2"));
-
-	/* sp */
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_ADD", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_ADD"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_1", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_1"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_2", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_COMPUTE_RHS_2"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_ERROR_NORM_1", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_ERROR_NORM_1"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_ERROR_NORM_2", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_ERROR_NORM_2"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_1", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_1"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_2", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_2"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_3", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_3"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_4", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_EXACT_RHS_4"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_INITIALIZE", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_INITIALIZE"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_RHS_NORM_1", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_RHS_NORM_1"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_RHS_NORM_2", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_RHS_NORM_2"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_TXINVR", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_TXINVR"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_X_SOLVE", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_X_SOLVE"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_Y_SOLVE", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_Y_SOLVE"));
-	put_def_variable(fp, (char*)"SP_THREADS_PER_BLOCK_ON_Z_SOLVE", (char*)read_gpu_config((char*)"SP_THREADS_PER_BLOCK_ON_Z_SOLVE"));
-
-	/* no gpu config*/
-	put_def_variable(fp, (char*)"NO_GPU_CONFIG", (char*)NO_CONFIG);
-}
-
 /*
  * get_info(): get parameters from command line 
  */
@@ -438,12 +211,12 @@ void check_info(int type, char class_npb){
 	int tmplog; 
 	/* check class_npb */
 	if(class_npb != 'S' && 
-			class_npb != 'W' && 
-			class_npb != 'A' && 
-			class_npb != 'B' && 
-			class_npb != 'C' && 
-			class_npb != 'D' &&
-			class_npb != 'E'){
+	   class_npb != 'W' && 
+	   class_npb != 'A' && 
+	   class_npb != 'B' && 
+	   class_npb != 'C' && 
+	   class_npb != 'D' &&
+	   class_npb != 'E'){
 		printf("setparams: Unknown benchmark class_npb %c\n", class_npb); 
 		printf("setparams: Allowed classes are \"S\", \"W\", \"A\", \"B\", \"C\", \"D\" and \"E\"\n");
 		exit(1);
@@ -530,12 +303,12 @@ void write_info(int type, char class_npb){
 
 	switch(type){
 		case SP:
-		case BT:
-		case FT:
-		case MG:
-		case LU:
-		case EP:
-		case CG:
+      	case BT:
+      	case FT:
+      	case MG:
+      	case LU:
+      	case EP:
+      	case CG:
 			/* write out the header */
 			fprintf(fp, DESC_LINE, class_npb);
 			/* print out a warning so bozos don't mess with the file */
@@ -565,8 +338,8 @@ void write_info(int type, char class_npb){
 	/* now do benchmark-specific stuff */
 	switch(type){   
 		case BT:	      
-			write_bt_info(fp, class_npb);
-			break;	
+   			 write_bt_info(fp, class_npb);
+    		break;	
 		case CG:	      
 			write_cg_info(fp, class_npb);
 			break;
@@ -580,22 +353,20 @@ void write_info(int type, char class_npb){
 			write_is_info(fp, class_npb);  
 			break;
 		case LU:	      
-			write_lu_info(fp, class_npb);
-			break;	 
+    		write_lu_info(fp, class_npb);
+    		break;	 
 		case MG:	      
 			write_mg_info(fp, class_npb);
 			break;	   
 		case SP:
-			write_sp_info(fp, class_npb);
-			break;		
+    		write_sp_info(fp, class_npb);
+    		break;		
 		default:
 			printf("setparams: (Internal error): Unknown benchmark type %d\n", type);
 			exit(1);
 	}
 	write_convertdouble_info(type, fp);
 	write_compiler_info(type, fp);
-	write_gpu_config(fp);
-	profiling_flag(fp);
 	fclose(fp);
 	return;
 }
@@ -612,7 +383,7 @@ void write_sp_info(FILE *fp, char class_npb){
 	else if(class_npb == 'B'){problem_size = 102; dt = "0.001"; niter = 400;}
 	else if(class_npb == 'C'){problem_size = 162; dt = "0.00067"; niter = 400;}
 	else if(class_npb == 'D'){problem_size = 408; dt = "0.00030"; niter = 500;}
-	else if(class_npb == 'E'){problem_size = 1020; dt = "0.0001"; niter = 500;}
+  	else if(class_npb == 'E'){problem_size = 1020; dt = "0.0001"; niter = 500;}
 	else{
 		printf("setparams: Internal error: invalid class_npb %c\n", class_npb);
 		exit(1);
@@ -634,7 +405,7 @@ void write_bt_info(FILE *fp, char class_npb){
 	else if(class_npb == 'B'){problem_size = 102; dt = "0.0003"; niter = 200;}
 	else if(class_npb == 'C'){problem_size = 162; dt = "0.0001"; niter = 200;}
 	else if(class_npb == 'D'){problem_size = 408; dt = "0.00002"; niter = 250;}
-	else if(class_npb == 'E'){problem_size = 1020; dt = "0.4e-5"; niter = 250;}
+  	else if(class_npb == 'E'){problem_size = 1020; dt = "0.4e-5"; niter = 250;}
 	else{
 		printf("setparams: Internal error: invalid class_npb %c\n", class_npb);
 		exit(1);
@@ -673,7 +444,7 @@ void write_lu_info(FILE *fp, char class_npb){
 	else if(class_npb == 'B'){problem_size = 102; dt_default = "2.0"; itmax = 250;}
 	else if(class_npb == 'C'){problem_size = 162; dt_default = "2.0"; itmax = 250;}
 	else if(class_npb == 'D'){problem_size = 408; dt_default = "1.0"; itmax = 300;}
-	else if(class_npb == 'E'){problem_size = 1020; dt_default = "0.5"; itmax = 300;}
+  	else if(class_npb == 'E'){problem_size = 1020; dt_default = "0.5"; itmax = 300;}
 	else{
 		printf("setparams: Internal error: invalid class_npb %c\n", class_npb);
 		exit(1);
@@ -735,11 +506,11 @@ void write_mg_info(FILE *fp, char class_npb)
  */
 void write_is_info(FILE *fp, char class_npb){
 	if(class_npb != 'S' &&
-			class_npb != 'W' &&
-			class_npb != 'A' &&
-			class_npb != 'B' &&
-			class_npb != 'C' &&
-			class_npb != 'D'){
+	   class_npb != 'W' &&
+	   class_npb != 'A' &&
+	   class_npb != 'B' &&
+	   class_npb != 'C' &&
+	   class_npb != 'D'){
 		printf("setparams: Internal error: invalid class_npb type %c\n", class_npb);
 		exit(1);
 	}
@@ -778,10 +549,10 @@ void write_cg_info(FILE *fp, char class_npb){
 		exit(1);
 	}
 	fprintf( fp, "#define NA     %d\n", na );
-	fprintf( fp, "#define NONZER %d\n", nonzer );
-	fprintf( fp, "#define NITER  %d\n", niter );
-	fprintf( fp, "#define SHIFT  %s\n", shift );
-	fprintf( fp, "#define RCOND  %s\n", rcond );
+  	fprintf( fp, "#define NONZER %d\n", nonzer );
+  	fprintf( fp, "#define NITER  %d\n", niter );
+  	fprintf( fp, "#define SHIFT  %s\n", shift );
+  	fprintf( fp, "#define RCOND  %s\n", rcond );
 }
 
 /* 
@@ -913,30 +684,25 @@ void write_compiler_info(int type, FILE *fp){
 	(void) time(&t);
 	tmp = localtime(&t);
 	(void) strftime(compiletime, (size_t)LL, "%d %b %Y", tmp);
-	char tmp2[10];
-	sprintf(tmp2, "%d.%d.%d", __GNUG__,__GNUC_MINOR__,__GNUC_PATCHLEVEL__);
 
 	switch(type){
 		case FT:
-		case SP:
-		case BT:
-		case MG:
-		case LU:
-		case EP:
-		case CG:
-		case IS:
-			put_def_string(fp, (char*)"COMPILETIME", (char*)compiletime);
+      	case SP:
+      	case BT:
+      	case MG:
+      	case LU:
+      	case EP:
+      	case CG:
+      	case IS:
+			put_def_string(fp, (char*)"COMPILETIME", compiletime);
 			put_def_string(fp, (char*)"NPBVERSION", (char*)VERSION);
-			put_def_string(fp, (char*)"LIBVERSION", (char*)read_nvcc_cuda_version());
-			put_def_string(fp, (char*)"COMPILERVERSION", (char*)read_nvcc_cuda_version());
-			put_def_string(fp, (char*)"CPU_MODEL", (char*)cpu_model());
-			put_def_string(fp, (char*)"CS1", (char*)cc);
-			put_def_string(fp, (char*)"CS2", (char*)clink);
-			put_def_string(fp, (char*)"CS3", (char*)c_lib);
-			put_def_string(fp, (char*)"CS4", (char*)c_inc);
-			put_def_string(fp, (char*)"CS5", (char*)cflags);
-			put_def_string(fp, (char*)"CS6", (char*)clinkflags);
-			put_def_string(fp, (char*)"CS7", (char*)randfile);
+			put_def_string(fp, (char*)"CS1", cc);
+			put_def_string(fp, (char*)"CS2", clink);
+			put_def_string(fp, (char*)"CS3", c_lib);
+			put_def_string(fp, (char*)"CS4", c_inc);
+			put_def_string(fp, (char*)"CS5", cflags);
+			put_def_string(fp, (char*)"CS6", clinkflags);
+			put_def_string(fp, (char*)"CS7", randfile);
 			break;
 		default:
 			printf("setparams: (Internal error): Unknown benchmark type %d\n", 
@@ -1092,12 +858,12 @@ int ilog2(int i){
 void write_convertdouble_info(int type, FILE *fp){
 	switch(type){
 		case SP:
-		case BT:
-		case LU:
-		case FT:
-		case MG:
-		case EP:
-		case CG:
+  		case BT:
+  		case LU:
+  		case FT:
+  		case MG:
+  		case EP:
+  		case CG:
 #ifdef CONVERTDOUBLE
 			fprintf(fp, "#define\tCONVERTDOUBLE\tTRUE\n");
 #else
diff --git a/README.md b/README.md
index 0f2d705..34589a3 100644
--- a/README.md
+++ b/README.md
@@ -3,18 +3,11 @@
 This is a repository aimed at providing GPU parallel codes with different parallel APIs for the NAS Parallel Benchmarks ([NPB](https://www.nas.nasa.gov/publications/npb.html)) from a C/C++ version ([NPB-CPP](https://github.com/GMAP/NPB-CPP)). You can also contribute with this project, writing issues and pull requests. :smile:
 
 
-:sound:*News:* Parametrization support for configuring number of threads per block and CUDA parallelism optimizations. :date:25/Jul/2021
-
 :sound:*News:* CUDA versions for pseudo-applications added and IS improved. :date:11/Feb/2021
 
-:sound:*News:* Paper published in the journal Software: Practice and Experience (SPE). :date:29/Nov/2021
-
-
 ## How to cite our work :+1:
 
-[DOI](https://doi.org/10.1002/spe.3056) - Araujo, G.; Griebler, D.; Rockenbach, D. A.; Danelutto, M.; Fernandes, L. G.; **NAS Parallel Benchmarks with CUDA and beyond**, Software: Practice and Experience (SPE), 2021.
-
-[DOI](https://doi.org/10.1109/PDP50117.2020.00009) - Araujo, G.; Griebler, D.; Danelutto, M.; Fernandes, L. G.; **Efficient NAS Benchmark Kernels with CUDA**. *28th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)*, Vsters, 2020. 
+[DOI](https://doi.org/10.1109/PDP50117.2020.00009) - Araujo, G. A.; Griebler, D.; Danelutto, M.; Fernandes, L. G. **Efficient NAS Benchmark Kernels with CUDA**. *28th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)*, Vsters, 2020. 
   
 ## The NPB with CUDA
 
@@ -24,9 +17,9 @@ The parallel CUDA version was implemented from the serial version of [NPB-CPP](h
 
 NAS Parallel Benchmarks code contributors with CUDA are:
 
-Dalvan Griebler: dalvan.griebler@pucrs.br
+Dalvan Griebler: dalvan.griebler@acad.pucrs.br
 
-Gabriell Araujo: gabriell.araujo@edu.pucrs.br
+Gabriell Araujo: gabriell.araujo@acad.pucrs.br
 
 ==================================================================
 
@@ -34,11 +27,11 @@ Each directory is independent and contains its own implemented version:
 
 *Five kernels*
 
-+ **IS** - Integer Sort
++ **IS** - Integer Sort, random memory access
 + **EP** - Embarrassingly Parallel
-+ **CG** - Conjugate Gradient
-+ **MG** - Multi-Grid
-+ **FT** - discrete 3D fast Fourier Transform
++ **CG** - Conjugate Gradient, irregular memory access and communication
++ **MG** - Multi-Grid on a sequence of meshes, long- and short-distance communication, memory intensive
++ **FT** - discrete 3D fast Fourier Transform, all-to-all communication
 
 *Three pseudo-application*
 
@@ -63,7 +56,7 @@ make _BENCHMARK CLASS=_VERSION
 `_BENCHMARKs` are:
 
 
-CG, EP, FT, IS, MG, BT, LU, and SP 
+CG, EP, FT, IS, MG, SP, BT, and LU 
 
 
 `_VERSIONs` are:
@@ -86,36 +79,42 @@ make ep CLASS=B
 
 ## Activating the additional timers
 
-NPB-GPU has additional timers for profiling purpose. To activate these timers, create a dummy file 'timer.flag' in the main directory of the NPB version (e.g. CUDA/timer.flag).
+NPB3.3.1 includes additional timers for profiling purpose. To activate these timers, create a dummy file 'timer.flag' in the main directory of the NPB.
 
-## Configuring the number of threads per block
+## Notes about the Fortran to CPP convertion
 
-NPB-GPU allows configuring the number of threads per block of each GPU kernel in the benchmarks. The user can specify the number of threads per block by editing the file gpu.config in the directory <API>/config/. If no file is specified, all GPU kernels are executed using the warp size of the GPU as the number of threads per block.
+The following information are also written and keep updated in [NPB-CPP](https://github.com/GMAP/NPB-CPP):
 
-Syntax of the gpu.config file: 
++ Memory conventions adopted on NPB-CPP turn better the performance of the C++ code, lowering the execution time and memory consumption (on some applications, these conventions turn the performance of the NPB-CPP even better than the original Fortran NPB3.3.1, as for example on BT pseudo-application).  
 
-```
-<benchmark-name>_THREADS_PER_BLOCK_<gpu-kernel-name> = <interger-value>
-```
+  - Any global array is allocated with dynamic memory and as one single dimension.
 
-Configuring CG benchmark as example:
+  - On kernels, a cast is made in the functions, so is possible to work using multi-dimension accesses with the arrays, so, for example a function can receive an array like `matrix_aux[NY*NY]`, and work with accesses like `matrix_aux[j][i]`, instead of one single dimension access (actually, the cast in the functions follows the original NPB3.3.1 way).
 
-```
-CG_THREADS_PER_BLOCK_ON_KERNEL_ONE = 32
-CG_THREADS_PER_BLOCK_ON_KERNEL_TWO = 128
-CG_THREADS_PER_BLOCK_ON_KERNEL_THREE = 64
-CG_THREADS_PER_BLOCK_ON_KERNEL_FOUR = 256
-CG_THREADS_PER_BLOCK_ON_KERNEL_FIVE = 32
-CG_THREADS_PER_BLOCK_ON_KERNEL_SIX = 64
-CG_THREADS_PER_BLOCK_ON_KERNEL_SEVEN = 128
-CG_THREADS_PER_BLOCK_ON_KERNEL_EIGHT = 64
-CG_THREADS_PER_BLOCK_ON_KERNEL_NINE = 512
-CG_THREADS_PER_BLOCK_ON_KERNEL_TEN = 512
-CG_THREADS_PER_BLOCK_ON_KERNEL_ELEVEN = 1024
-```
+  - On pseudo-applications, the cast is done already in the array declarations (NPB3.3.1 does not use one single dimension on pseudo-applications, so we cast the arrays directly on declarations, because this way, changes in the structure of the functions are not necessary).
 
-The NPB-GPU also allows changing the GPU device by providing the following syntax in the gpu.config file:
+  - To disable this convention (dynamic memory and on single dimension) is necessary set the flag `-DDO_NOT_ALLOCATE_ARRAYS_WITH_DYNAMIC_MEMORY_AND_AS_SINGLE_DIMENSION` on compilation.
 
-```
-GPU_DEVICE = <interger-value>
-```
\ No newline at end of file
+  - Also, we keep every single structure of the loops as the same as Fortran original, but as Fortran memory behavior is different from C/C++, we invert the dimensions of the arrays and consequently, any array access. It is like change for example from an access `array[i][j][k]` to `array[k][j][i]` (but keeping the same organization of the loops), and change array dimensions from `array[NX][NY][NZ]` to `array[NZ][NY][NX]`.   
+ 
++ The original NPB has a file for print results of IS (`c_print_results.c`) and another file for print results of the other Benchmarks (`print_results.f`), it means, one file for Fortran code and one file for C code (IS is the only Benchmark that was written using C language). As the entire NPB-CPP is in C++, we keep only a single file to print results of all Benchmarks, we merged these two files and created `c_print_results.cpp`.
+
++ Any `goto` in the applications was replaced with an equivalent code using loops, keeping the same logic.
+
++ There are some little differences on indexes and ranges of loops, that are inherent to conversions from Fortran to C/C++.
+
++ In the file `common/npb-CPP.hpp` we define additional stuff like the structure and operations for complex numbers.
+
++ FT
+
+	- Instead convert code directly from serial FT 3.3.1, we convert Fortran code from FT OpenMP version, where the format is equal to the FT serial versions before NPB 3.0.
+	In no version of NPB the OpenMP parallel code is based on the serial code presented in 3.0 and 3.3.1 versions.
+	The parallel code OpenMP in all versions of NPB is based on the sequential format before to the versions 3.0 and 3.3.1.
+	In addition, in version 3.4, the most recent, they state that the sequential code will no longer be available. The sequential code will be the OpenMP code without compiling with OpenMP.
+	Faced with these facts, we conclude that this refactored FT serial code (3.3.1) has no utility, as it was not used for parallel versions and from NPB 3.4 it will be completely forgotten.
+  - In the global.hpp, historically, the constants `FFTBLOCK_DEFAULT` and `FFTBLOCKPAD_DEFAULT` receive values that change the cache behavior of the applications and the performance can be better or worse for each processor according which values are choosed. We define these constants with the value 1 (DEFAULT_BEHAVIOR), that determines a default behavior independently of the processor where the application is running.
+  - The size of the matrixes on the original NPB is `[NZ][NY][NX+1]`, but we changed to `[NZ][NY][NX]`, because the additional positions generated by `NX+1` are not used on the application, they only spend more memory.
+  - On the original NPB, the auxiliary matrixes `y1`, `y2` and `u` have the size as `NX`. But only in cffts1 the size is NX, on the cffts2 the correct size is NY and cffts3 the size is NZ. It is a problem when NX is not the bigger dimension. To fix this, we assign the size of these matrixes as `MAXDIM` that is the size of the bigger dimension. Consequently `MAXDIM` is also used as argument in the function `fft_init` that initializes the values of the matrix `u`.
+
++ IS
+	- On the original NPB, IS has on its own source functions like `randlc`. On our version of NPB we does not need it, because we already have these functions implemented. On original NPB these functions are needed in the IS code, because IS is in C and these functions were written in Fortran for the other applications.
