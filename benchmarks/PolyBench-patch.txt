diff -ruN PolyBench-ACC-0.1/CUDA/datamining/correlation/correlation.cu PolyBench-backup/CUDA/datamining/correlation/correlation.cu
--- PolyBench-ACC-0.1/CUDA/datamining/correlation/correlation.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/datamining/correlation/correlation.cu	2024-09-26 12:55:55.232230591 -0700
@@ -13,7 +13,7 @@
 #include <math.h>
 #include <assert.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -31,7 +31,7 @@
 #define FLOAT_N 3214212.01f
 #define EPS 0.005f
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_arrays(int m, int n, DATA_TYPE POLYBENCH_2D(data, M, N, m, n))
@@ -136,10 +136,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 	
@@ -230,14 +230,14 @@
 	DATA_TYPE *mean_gpu;
 	DATA_TYPE *symmat_gpu;
 
-	cudaMalloc((void **)&data_gpu, sizeof(DATA_TYPE) * M * N);
-	cudaMalloc((void **)&symmat_gpu, sizeof(DATA_TYPE) * M * N);
-	cudaMalloc((void **)&stddev_gpu, sizeof(DATA_TYPE) * M);
-	cudaMalloc((void **)&mean_gpu, sizeof(DATA_TYPE) * M);
-	cudaMemcpy(data_gpu, data, sizeof(DATA_TYPE) * M * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(symmat_gpu, symmat, sizeof(DATA_TYPE) * M * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(stddev_gpu, stddev, sizeof(DATA_TYPE) * M, cudaMemcpyHostToDevice);
-	cudaMemcpy(mean_gpu, mean, sizeof(DATA_TYPE) * M, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&data_gpu, sizeof(DATA_TYPE) * M * N);
+	hipMalloc((void **)&symmat_gpu, sizeof(DATA_TYPE) * M * N);
+	hipMalloc((void **)&stddev_gpu, sizeof(DATA_TYPE) * M);
+	hipMalloc((void **)&mean_gpu, sizeof(DATA_TYPE) * M);
+	hipMemcpy(data_gpu, data, sizeof(DATA_TYPE) * M * N, hipMemcpyHostToDevice);
+	hipMemcpy(symmat_gpu, symmat, sizeof(DATA_TYPE) * M * N, hipMemcpyHostToDevice);
+	hipMemcpy(stddev_gpu, stddev, sizeof(DATA_TYPE) * M, hipMemcpyHostToDevice);
+	hipMemcpy(mean_gpu, mean, sizeof(DATA_TYPE) * M, hipMemcpyHostToDevice);
 		
 	dim3 block1(DIM_THREAD_BLOCK_KERNEL_1_X, DIM_THREAD_BLOCK_KERNEL_1_Y);
 	dim3 grid1((size_t)(ceil((float)(M)) / ((float)DIM_THREAD_BLOCK_KERNEL_1_X)), 1);
@@ -255,13 +255,13 @@
   	polybench_start_instruments;
 
 	mean_kernel<<< grid1, block1 >>>(m, n, mean_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	std_kernel<<< grid2, block2 >>>(m, n, mean_gpu,stddev_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	reduce_kernel<<< grid3, block3 >>>(m, n, mean_gpu,stddev_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	corr_kernel<<< grid4, block4 >>>(m, n, symmat_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
@@ -269,14 +269,14 @@
  	polybench_print_instruments;
 
 	DATA_TYPE valueAtSymmatIndexMTimesMPlus1PlusMPoint = 1.0;
-	cudaMemcpy(&(symmat_gpu[(M-1)*M + (M-1)]), &valueAtSymmatIndexMTimesMPlus1PlusMPoint, sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
+	hipMemcpy(&(symmat_gpu[(M-1)*M + (M-1)]), &valueAtSymmatIndexMTimesMPlus1PlusMPoint, sizeof(DATA_TYPE), hipMemcpyHostToDevice);
 
-	cudaMemcpy(symmat_outputFromGpu, symmat_gpu, sizeof(DATA_TYPE) * M * N, cudaMemcpyDeviceToHost);
+	hipMemcpy(symmat_outputFromGpu, symmat_gpu, sizeof(DATA_TYPE) * M * N, hipMemcpyDeviceToHost);
 	
-	cudaFree(data_gpu);
-	cudaFree(symmat_gpu);
-	cudaFree(stddev_gpu);
-	cudaFree(mean_gpu);
+	hipFree(data_gpu);
+	hipFree(symmat_gpu);
+	hipFree(stddev_gpu);
+	hipFree(mean_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/datamining/correlation/setup.ini PolyBench-backup/CUDA/datamining/correlation/setup.ini
--- PolyBench-ACC-0.1/CUDA/datamining/correlation/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/datamining/correlation/setup.ini	2024-09-26 12:55:55.232230591 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./correlation.exe_mini;./correlation.exe_small;./correlation.exe_standard;./correlation.exe_large;./correlation.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/datamining/covariance/covariance.cu PolyBench-backup/CUDA/datamining/covariance/covariance.cu
--- PolyBench-ACC-0.1/CUDA/datamining/covariance/covariance.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/datamining/covariance/covariance.cu	2024-09-26 12:55:55.232230591 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -32,7 +32,7 @@
 #define FLOAT_N 3214212.01
 #define EPS 0.005
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_arrays(int m, int n, DATA_TYPE POLYBENCH_2D(data,M,N,m,n))
@@ -110,10 +110,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 	
 	return;
 }
@@ -176,12 +176,12 @@
 	DATA_TYPE *mean_gpu;
 	DATA_TYPE *symmat_gpu;
 
-	cudaMalloc((void **)&data_gpu, sizeof(DATA_TYPE) * M * N);
-	cudaMalloc((void **)&symmat_gpu, sizeof(DATA_TYPE) * M * M);
-	cudaMalloc((void **)&mean_gpu, sizeof(DATA_TYPE) * M);
-	cudaMemcpy(data_gpu, data, sizeof(DATA_TYPE) * M * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(symmat_gpu, symmat, sizeof(DATA_TYPE) * M * M, cudaMemcpyHostToDevice);
-	cudaMemcpy(mean_gpu, mean, sizeof(DATA_TYPE) * M, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&data_gpu, sizeof(DATA_TYPE) * M * N);
+	hipMalloc((void **)&symmat_gpu, sizeof(DATA_TYPE) * M * M);
+	hipMalloc((void **)&mean_gpu, sizeof(DATA_TYPE) * M);
+	hipMemcpy(data_gpu, data, sizeof(DATA_TYPE) * M * N, hipMemcpyHostToDevice);
+	hipMemcpy(symmat_gpu, symmat, sizeof(DATA_TYPE) * M * M, hipMemcpyHostToDevice);
+	hipMemcpy(mean_gpu, mean, sizeof(DATA_TYPE) * M, hipMemcpyHostToDevice);
 	
 	dim3 block1(DIM_THREAD_BLOCK_KERNEL_1_X, DIM_THREAD_BLOCK_KERNEL_1_Y);
 	dim3 grid1((size_t)(ceil((float)M) / ((float)DIM_THREAD_BLOCK_KERNEL_1_X)), 1);
@@ -196,22 +196,22 @@
   	polybench_start_instruments;
 
 	mean_kernel<<<grid1, block1>>>(m,n,mean_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	reduce_kernel<<<grid2, block2>>>(m,n,mean_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	covar_kernel<<<grid3, block3>>>(m,n,symmat_gpu,data_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(symmat_outputFromGpu, symmat_gpu, sizeof(DATA_TYPE) * M * N, cudaMemcpyDeviceToHost);
+	hipMemcpy(symmat_outputFromGpu, symmat_gpu, sizeof(DATA_TYPE) * M * N, hipMemcpyDeviceToHost);
 	
-	cudaFree(data_gpu);
-	cudaFree(symmat_gpu);
-	cudaFree(mean_gpu);
+	hipFree(data_gpu);
+	hipFree(symmat_gpu);
+	hipFree(mean_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/datamining/covariance/setup.ini PolyBench-backup/CUDA/datamining/covariance/setup.ini
--- PolyBench-ACC-0.1/CUDA/datamining/covariance/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/datamining/covariance/setup.ini	2024-09-26 12:55:55.232230591 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./covariance.exe_mini;./covariance.exe_small;./covariance.exe_standard;./covariance.exe_large;./covariance.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/2mm/2mm.cu PolyBench-backup/CUDA/linear-algebra/kernels/2mm/2mm.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/2mm/2mm.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/2mm/2mm.cu	2024-09-26 12:55:55.232230591 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -27,7 +27,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int ni, int nj, int nk, int nl, DATA_TYPE *alpha, DATA_TYPE *beta, DATA_TYPE POLYBENCH_2D(A, NI, NK, ni, nk), 
@@ -96,10 +96,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -202,17 +202,17 @@
 	DATA_TYPE *C_gpu;
 	DATA_TYPE *D_gpu;
 
-	cudaMalloc((void **)&tmp_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NK);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NK * NJ);
-	cudaMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NL * NJ);
-	cudaMalloc((void **)&D_gpu, sizeof(DATA_TYPE) * NI * NL);
+	hipMalloc((void **)&tmp_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NK);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NK * NJ);
+	hipMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NL * NJ);
+	hipMalloc((void **)&D_gpu, sizeof(DATA_TYPE) * NI * NL);
 	
-	cudaMemcpy(tmp_gpu, tmp, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NK, cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NK * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NL * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(D_gpu, D, sizeof(DATA_TYPE) * NI * NL, cudaMemcpyHostToDevice);	
+	hipMemcpy(tmp_gpu, tmp, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NK, hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NK * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NL * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(D_gpu, D, sizeof(DATA_TYPE) * NI * NL, hipMemcpyHostToDevice);	
 		
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid1((size_t)ceil( ((float)NJ) / ((float)block.x) ), (size_t)ceil( ((float)NI) / ((float)block.y)) );
@@ -222,21 +222,21 @@
   	polybench_start_instruments;
 
 	mm2_kernel1<<<grid1,block>>>(ni, nj, nk, nl, alpha, beta, tmp_gpu, A_gpu, B_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	mm2_kernel2<<<grid2,block>>>(ni, nj, nk, nl, alpha, beta, tmp_gpu, C_gpu, D_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(D_outputFromGpu, D_gpu, sizeof(DATA_TYPE) * NI * NL, cudaMemcpyDeviceToHost);
+	hipMemcpy(D_outputFromGpu, D_gpu, sizeof(DATA_TYPE) * NI * NL, hipMemcpyDeviceToHost);
 
-	cudaFree(tmp_gpu);
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
-	cudaFree(C_gpu);
-	cudaFree(D_gpu);
+	hipFree(tmp_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
+	hipFree(C_gpu);
+	hipFree(D_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/2mm/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/2mm/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/2mm/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/2mm/setup.ini	2024-09-26 12:55:55.232230591 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./2mm.exe_mini;./2mm.exe_small;./2mm.exe_standard;./2mm.exe_large;./2mm.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/3mm/3mm.cu PolyBench-backup/CUDA/linear-algebra/kernels/3mm/3mm.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/3mm/3mm.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/3mm/3mm.cu	2024-09-26 12:55:55.232230591 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -27,7 +27,7 @@
 //define the error threshold for the results "not matching"
 #define PERCENT_DIFF_ERROR_THRESHOLD 0.05
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int ni, int nj, int nk, int nl, int nm, DATA_TYPE POLYBENCH_2D(A, NI, NK, ni, nk), DATA_TYPE POLYBENCH_2D(B, NK, NJ, nk, nj), 
@@ -92,10 +92,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 	
@@ -221,21 +221,21 @@
 	DATA_TYPE *F_gpu;
 	DATA_TYPE *G_gpu;
 	
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NK);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NK * NJ);
-	cudaMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NJ * NM);
-	cudaMalloc((void **)&D_gpu, sizeof(DATA_TYPE) * NM * NL);
-	cudaMalloc((void **)&E_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&F_gpu, sizeof(DATA_TYPE) * NJ * NL);
-	cudaMalloc((void **)&G_gpu, sizeof(DATA_TYPE) * NI * NL);
-
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NK, cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NK * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NJ * NM, cudaMemcpyHostToDevice);
-	cudaMemcpy(D_gpu, D, sizeof(DATA_TYPE) * NM * NL, cudaMemcpyHostToDevice);
-	cudaMemcpy(E_gpu, E, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(F_gpu, F, sizeof(DATA_TYPE) * NJ * NL, cudaMemcpyHostToDevice);
-	cudaMemcpy(G_gpu, G, sizeof(DATA_TYPE) * NI * NL, cudaMemcpyHostToDevice);	
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NK);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NK * NJ);
+	hipMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NJ * NM);
+	hipMalloc((void **)&D_gpu, sizeof(DATA_TYPE) * NM * NL);
+	hipMalloc((void **)&E_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&F_gpu, sizeof(DATA_TYPE) * NJ * NL);
+	hipMalloc((void **)&G_gpu, sizeof(DATA_TYPE) * NI * NL);
+
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NK, hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NK * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NJ * NM, hipMemcpyHostToDevice);
+	hipMemcpy(D_gpu, D, sizeof(DATA_TYPE) * NM * NL, hipMemcpyHostToDevice);
+	hipMemcpy(E_gpu, E, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(F_gpu, F, sizeof(DATA_TYPE) * NJ * NL, hipMemcpyHostToDevice);
+	hipMemcpy(G_gpu, G, sizeof(DATA_TYPE) * NI * NL, hipMemcpyHostToDevice);	
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid1((size_t)(ceil( ((float)NJ) / ((float)DIM_THREAD_BLOCK_X) )),(size_t)(ceil((float)NI/ ((float)DIM_THREAD_BLOCK_Y) )));
@@ -246,25 +246,25 @@
   	polybench_start_instruments;
 
 	mm3_kernel1<<<grid1,block>>>(ni, nj, nk, nl, nm, A_gpu, B_gpu, E_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	mm3_kernel2<<<grid2,block>>>(ni, nj, nk, nl, nm, C_gpu, D_gpu, F_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	mm3_kernel3<<<grid3,block>>>(ni, nj, nk, nl, nm, E_gpu, F_gpu, G_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
-	cudaMemcpy(G_outputFromGpu, G_gpu, sizeof(DATA_TYPE) * NI * NL, cudaMemcpyDeviceToHost);
+	hipMemcpy(G_outputFromGpu, G_gpu, sizeof(DATA_TYPE) * NI * NL, hipMemcpyDeviceToHost);
 	
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
-	cudaFree(C_gpu);
-	cudaFree(D_gpu);
-	cudaFree(E_gpu);
-	cudaFree(F_gpu);
-	cudaFree(G_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
+	hipFree(C_gpu);
+	hipFree(D_gpu);
+	hipFree(E_gpu);
+	hipFree(F_gpu);
+	hipFree(G_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/3mm/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/3mm/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/3mm/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/3mm/setup.ini	2024-09-26 12:55:55.232230591 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./3mm.exe_mini;./3mm.exe_small;./3mm.exe_standard;./3mm.exe_large;./3mm.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/atax/atax.cu PolyBench-backup/CUDA/linear-algebra/kernels/atax/atax.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/atax/atax.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/atax/atax.cu	2024-09-26 12:55:55.232230591 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -32,7 +32,7 @@
 #define M_PI 3.14159
 #endif
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int nx, int ny, DATA_TYPE POLYBENCH_1D(x,NX,nx), DATA_TYPE POLYBENCH_2D(A,NX,NY,nx,ny))
@@ -70,10 +70,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -143,15 +143,15 @@
 	DATA_TYPE *y_gpu;
 	DATA_TYPE *tmp_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NX * NY);
-	cudaMalloc((void **)&x_gpu, sizeof(DATA_TYPE) * NY);
-	cudaMalloc((void **)&y_gpu, sizeof(DATA_TYPE) * NY);
-	cudaMalloc((void **)&tmp_gpu, sizeof(DATA_TYPE) * NX);
-	
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NX * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(x_gpu, x, sizeof(DATA_TYPE) * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(y_gpu, y, sizeof(DATA_TYPE) * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(tmp_gpu, tmp, sizeof(DATA_TYPE) * NX, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NX * NY);
+	hipMalloc((void **)&x_gpu, sizeof(DATA_TYPE) * NY);
+	hipMalloc((void **)&y_gpu, sizeof(DATA_TYPE) * NY);
+	hipMalloc((void **)&tmp_gpu, sizeof(DATA_TYPE) * NX);
+	
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NX * NY, hipMemcpyHostToDevice);
+	hipMemcpy(x_gpu, x, sizeof(DATA_TYPE) * NY, hipMemcpyHostToDevice);
+	hipMemcpy(y_gpu, y, sizeof(DATA_TYPE) * NY, hipMemcpyHostToDevice);
+	hipMemcpy(tmp_gpu, tmp, sizeof(DATA_TYPE) * NX, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid1((size_t)(ceil( ((float)NX) / ((float)block.x) )), 1);
@@ -161,21 +161,21 @@
   	polybench_start_instruments;
 
 	atax_kernel1<<< grid1, block >>>(nx, ny, A_gpu,x_gpu,tmp_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	atax_kernel2<<< grid2, block >>>(nx, ny, A_gpu,y_gpu,tmp_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 	
-	cudaMemcpy(y_outputFromGpu, y_gpu, sizeof(DATA_TYPE) * NX, cudaMemcpyDeviceToHost);
+	hipMemcpy(y_outputFromGpu, y_gpu, sizeof(DATA_TYPE) * NX, hipMemcpyDeviceToHost);
 
-	cudaFree(A_gpu);
-	cudaFree(x_gpu);
-	cudaFree(y_gpu);
-	cudaFree(tmp_gpu);
+	hipFree(A_gpu);
+	hipFree(x_gpu);
+	hipFree(y_gpu);
+	hipFree(tmp_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/atax/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/atax/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/atax/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/atax/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./atax.exe_mini;./atax.exe_small;./atax.exe_standard;./atax.exe_large;./atax.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/bicg/bicg.cu PolyBench-backup/CUDA/linear-algebra/kernels/bicg/bicg.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/bicg/bicg.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/bicg/bicg.cu	2024-09-26 12:55:55.233230641 -0700
@@ -13,7 +13,7 @@
 #include <math.h>
 #include <assert.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -30,7 +30,7 @@
 #define M_PI 3.14159
 #endif
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int nx, int ny, DATA_TYPE POLYBENCH_2D(A,NX,NY,nx,ny), DATA_TYPE POLYBENCH_1D(p,NY,ny), DATA_TYPE POLYBENCH_1D(r,NX,nx))
@@ -84,10 +84,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -181,16 +181,16 @@
 	DATA_TYPE *r_gpu;
 	DATA_TYPE *s_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NX * NY);
-	cudaMalloc((void **)&r_gpu, sizeof(DATA_TYPE) * NX);
-	cudaMalloc((void **)&s_gpu, sizeof(DATA_TYPE) * NY);
-	cudaMalloc((void **)&p_gpu, sizeof(DATA_TYPE) * NY);
-	cudaMalloc((void **)&q_gpu, sizeof(DATA_TYPE) * NX);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NX * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(r_gpu, r, sizeof(DATA_TYPE) * NX, cudaMemcpyHostToDevice);
-	cudaMemcpy(s_gpu, s, sizeof(DATA_TYPE) * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(p_gpu, p, sizeof(DATA_TYPE) * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(q_gpu, q, sizeof(DATA_TYPE) * NX, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NX * NY);
+	hipMalloc((void **)&r_gpu, sizeof(DATA_TYPE) * NX);
+	hipMalloc((void **)&s_gpu, sizeof(DATA_TYPE) * NY);
+	hipMalloc((void **)&p_gpu, sizeof(DATA_TYPE) * NY);
+	hipMalloc((void **)&q_gpu, sizeof(DATA_TYPE) * NX);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NX * NY, hipMemcpyHostToDevice);
+	hipMemcpy(r_gpu, r, sizeof(DATA_TYPE) * NX, hipMemcpyHostToDevice);
+	hipMemcpy(s_gpu, s, sizeof(DATA_TYPE) * NY, hipMemcpyHostToDevice);
+	hipMemcpy(p_gpu, p, sizeof(DATA_TYPE) * NY, hipMemcpyHostToDevice);
+	hipMemcpy(q_gpu, q, sizeof(DATA_TYPE) * NX, hipMemcpyHostToDevice);
 
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid1((size_t)(ceil( ((float)NY) / ((float)block.x) )), 1);
@@ -200,23 +200,23 @@
   	polybench_start_instruments;
 
 	bicg_kernel1<<< grid1, block >>>(nx, ny, A_gpu, r_gpu, s_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	bicg_kernel2<<< grid2, block >>>(nx, ny, A_gpu, p_gpu, q_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 	
-	cudaMemcpy(s_outputFromGpu, s_gpu, sizeof(DATA_TYPE) * NY, cudaMemcpyDeviceToHost);
-	cudaMemcpy(q_outputFromGpu, q_gpu, sizeof(DATA_TYPE) * NX, cudaMemcpyDeviceToHost);
+	hipMemcpy(s_outputFromGpu, s_gpu, sizeof(DATA_TYPE) * NY, hipMemcpyDeviceToHost);
+	hipMemcpy(q_outputFromGpu, q_gpu, sizeof(DATA_TYPE) * NX, hipMemcpyDeviceToHost);
 
-	cudaFree(A_gpu);
-	cudaFree(r_gpu);
-	cudaFree(s_gpu);
-	cudaFree(p_gpu);
-	cudaFree(q_gpu);
+	hipFree(A_gpu);
+	hipFree(r_gpu);
+	hipFree(s_gpu);
+	hipFree(p_gpu);
+	hipFree(q_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/bicg/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/bicg/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/bicg/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/bicg/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./bicg.exe_mini;./bicg.exe_small;./bicg.exe_standard;./bicg.exe_large;./bicg.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/doitgen/doitgen.cu PolyBench-backup/CUDA/linear-algebra/kernels/doitgen/doitgen.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/doitgen/doitgen.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/doitgen/doitgen.cu	2024-09-26 12:55:55.233230641 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -28,7 +28,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 
@@ -104,10 +104,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -147,13 +147,13 @@
 	DATA_TYPE* C4Gpu;
 	DATA_TYPE* sumGpu;
 
-	cudaMalloc(&AGpu, nr * nq * np * sizeof(DATA_TYPE));
-	cudaMalloc(&C4Gpu, np * np * sizeof(DATA_TYPE));
-	cudaMalloc(&sumGpu, nr * nq * np * sizeof(DATA_TYPE));
-
-	cudaMemcpy(AGpu, A, nr * nq * np * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
-	cudaMemcpy(C4Gpu, C4, np * np * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
-	cudaMemcpy(sumGpu, sum_outputFromGpu, nr * nq * np * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
+	hipMalloc(&AGpu, nr * nq * np * sizeof(DATA_TYPE));
+	hipMalloc(&C4Gpu, np * np * sizeof(DATA_TYPE));
+	hipMalloc(&sumGpu, nr * nq * np * sizeof(DATA_TYPE));
+
+	hipMemcpy(AGpu, A, nr * nq * np * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
+	hipMemcpy(C4Gpu, C4, np * np * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
+	hipMemcpy(sumGpu, sum_outputFromGpu, nr * nq * np * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
 
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((unsigned int)ceil( ((float)np) / ((float)block.x) ), (unsigned int)ceil( ((float)nr) / ((float)block.y) ));
@@ -164,9 +164,9 @@
 	for (int r = 0; r < nr; r++)
 	{
 		doitgen_kernel1 <<<grid, block>>> (nr, nq, np, sumGpu, AGpu, C4Gpu, r);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		doitgen_kernel2 <<<grid, block>>> (nr, nq, np, sumGpu, AGpu, C4Gpu, r);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	}
 
 	/* Stop and print timer. */
@@ -174,11 +174,11 @@
   	polybench_stop_instruments;
 	polybench_print_instruments;
 		
-	cudaMemcpy(sum_outputFromGpu, sumGpu, NR * NQ * NP * sizeof(DATA_TYPE), cudaMemcpyDeviceToHost);
+	hipMemcpy(sum_outputFromGpu, sumGpu, NR * NQ * NP * sizeof(DATA_TYPE), hipMemcpyDeviceToHost);
 
-	cudaFree(AGpu);
-	cudaFree(C4Gpu);
-	cudaFree(sumGpu);
+	hipFree(AGpu);
+	hipFree(C4Gpu);
+	hipFree(sumGpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/doitgen/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/doitgen/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/doitgen/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/doitgen/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./doitgen.exe_mini;./doitgen.exe_small;./doitgen.exe_standard;./doitgen.exe_large;./doitgen.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemm/gemm.cu PolyBench-backup/CUDA/linear-algebra/kernels/gemm/gemm.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemm/gemm.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/gemm/gemm.cu	2024-09-26 12:55:55.233230641 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -28,7 +28,7 @@
 //define the error threshold for the results "not matching"
 #define PERCENT_DIFF_ERROR_THRESHOLD 0.05
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void gemm(int ni, int nj, int nk, DATA_TYPE alpha, DATA_TYPE beta, DATA_TYPE POLYBENCH_2D(A,NI,NK,ni,nk), 
@@ -109,10 +109,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -140,13 +140,13 @@
 	DATA_TYPE *B_gpu;
 	DATA_TYPE *C_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NK);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NK * NJ);
-	cudaMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NK, cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NK * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NK);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NK * NJ);
+	hipMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NK, hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NK * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((size_t)(ceil( ((float)NI)/ ((float)block.x) )),(size_t)(ceil( ((float)NJ)/ ((float)block.y) )));
@@ -155,18 +155,18 @@
   	polybench_start_instruments;
 
 	gemm_kernel<<< grid, block >>>(ni, nj, nk, alpha, beta, A_gpu, B_gpu, C_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(C_outputFromGpu, C_gpu, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyDeviceToHost);    
+	hipMemcpy(C_outputFromGpu, C_gpu, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyDeviceToHost);    
 	
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
-	cudaFree(C_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
+	hipFree(C_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemm/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/gemm/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemm/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/gemm/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./gemm.exe_mini;./gemm.exe_small;./gemm.exe_standard;./gemm.exe_large;./gemm.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemver/gemver.cu PolyBench-backup/CUDA/linear-algebra/kernels/gemver/gemver.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemver/gemver.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/gemver/gemver.cu	2024-09-26 12:55:55.233230641 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -28,7 +28,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void gemver(int n, DATA_TYPE alpha, DATA_TYPE beta, DATA_TYPE POLYBENCH_2D(A, N, N, n, n), DATA_TYPE POLYBENCH_1D(u1, N, n), DATA_TYPE POLYBENCH_1D(v1, N, n), 
@@ -124,10 +124,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -196,25 +196,25 @@
 	DATA_TYPE *u2_gpu;
 	DATA_TYPE *w_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * N * N);
-	cudaMalloc((void **)&x_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&y_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&z_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&w_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&v1_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&v2_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&u1_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&u2_gpu, sizeof(DATA_TYPE) * N);
-	
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * N * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(x_gpu, x, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(y_gpu, y, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(z_gpu, z, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(w_gpu, w, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(v1_gpu, v1, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(v2_gpu, v2, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(u1_gpu, u1, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(u2_gpu, u2, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * N * N);
+	hipMalloc((void **)&x_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&y_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&z_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&w_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&v1_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&v2_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&u1_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&u2_gpu, sizeof(DATA_TYPE) * N);
+	
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * N * N, hipMemcpyHostToDevice);
+	hipMemcpy(x_gpu, x, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(y_gpu, y, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(z_gpu, z, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(w_gpu, w, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(v1_gpu, v1, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(v2_gpu, v2, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(u1_gpu, u1, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(u2_gpu, u2, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
 
 	dim3 block1(DIM_THREAD_BLOCK_KERNEL_1_X, DIM_THREAD_BLOCK_KERNEL_1_Y);
 	dim3 grid1((size_t)(ceil((float)N) / ((float)DIM_THREAD_BLOCK_KERNEL_1_X)), (size_t)(ceil((float)N) / ((float)DIM_THREAD_BLOCK_KERNEL_1_Y)));
@@ -229,28 +229,28 @@
   	polybench_start_instruments;
 
 	gemver_kernel1<<< grid1, block1 >>>(n, alpha, beta, A_gpu,v1_gpu,v2_gpu, u1_gpu, u2_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	gemver_kernel2<<< grid2, block2 >>>(n, alpha, beta, A_gpu,x_gpu,y_gpu, z_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	gemver_kernel3<<< grid3, block3 >>>(n, alpha, beta, A_gpu,x_gpu,w_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(w_outputFromGpu, w_gpu, sizeof(DATA_TYPE) * N, cudaMemcpyDeviceToHost);
+	hipMemcpy(w_outputFromGpu, w_gpu, sizeof(DATA_TYPE) * N, hipMemcpyDeviceToHost);
 	
-	cudaFree(A_gpu);
-	cudaFree(x_gpu);
-	cudaFree(y_gpu);
-	cudaFree(z_gpu);
-	cudaFree(w_gpu);
-	cudaFree(v1_gpu);
-	cudaFree(v2_gpu);
-	cudaFree(u1_gpu);
-	cudaFree(u2_gpu);
+	hipFree(A_gpu);
+	hipFree(x_gpu);
+	hipFree(y_gpu);
+	hipFree(z_gpu);
+	hipFree(w_gpu);
+	hipFree(v1_gpu);
+	hipFree(v2_gpu);
+	hipFree(u1_gpu);
+	hipFree(u2_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemver/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/gemver/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gemver/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/gemver/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./gemver.exe_mini;./gemver.exe_small;./gemver.exe_standard;./gemver.exe_large;./gemver.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gesummv/gesummv.cu PolyBench-backup/CUDA/linear-algebra/kernels/gesummv/gesummv.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gesummv/gesummv.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/gesummv/gesummv.cu	2024-09-26 12:55:55.233230641 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -32,7 +32,7 @@
 #define ALPHA 43532.0f
 #define BETA 12313.0f
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void gesummv(int n, DATA_TYPE alpha, DATA_TYPE beta, DATA_TYPE POLYBENCH_2D(A,N,N,n,n), DATA_TYPE POLYBENCH_2D(B,N,N,n,n), DATA_TYPE POLYBENCH_1D(tmp,N,n),
@@ -96,10 +96,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -129,17 +129,17 @@
 	DATA_TYPE *y_gpu;
 	DATA_TYPE *tmp_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * N * N);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * N * N);
-	cudaMalloc((void **)&x_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&y_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&tmp_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * N * N);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * N * N);
+	hipMalloc((void **)&x_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&y_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&tmp_gpu, sizeof(DATA_TYPE) * N);
 	
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * N * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, sizeof(DATA_TYPE) * N * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(x_gpu, x, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(y_gpu, y, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(tmp_gpu, tmp, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * N * N, hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, sizeof(DATA_TYPE) * N * N, hipMemcpyHostToDevice);
+	hipMemcpy(x_gpu, x, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(y_gpu, y, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(tmp_gpu, tmp, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
 
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((unsigned int)ceil( ((float)N) / ((float)block.x) ), 1);
@@ -149,14 +149,14 @@
   	polybench_start_instruments;
 
 	gesummv_kernel<<< grid, block>>>(n, alpha, beta, A_gpu, B_gpu, tmp_gpu, x_gpu, y_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(y_outputFromGpu, y_gpu, sizeof(DATA_TYPE) * N, cudaMemcpyDeviceToHost);
+	hipMemcpy(y_outputFromGpu, y_gpu, sizeof(DATA_TYPE) * N, hipMemcpyDeviceToHost);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gesummv/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/gesummv/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/gesummv/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/gesummv/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./gesummv.exe_mini;./gesummv.exe_small;./gesummv.exe_standard;./gesummv.exe_large;./gesummv.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/mvt/mvt.cu PolyBench-backup/CUDA/linear-algebra/kernels/mvt/mvt.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/mvt/mvt.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/mvt/mvt.cu	2024-09-26 12:55:55.233230641 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -27,7 +27,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int n, DATA_TYPE POLYBENCH_2D(A, N, N, n, n), DATA_TYPE POLYBENCH_1D(x1, N, n), DATA_TYPE POLYBENCH_1D(x2, N, n), DATA_TYPE POLYBENCH_1D(y1, N, n), DATA_TYPE POLYBENCH_1D(y2, N, n))
@@ -96,10 +96,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -141,16 +141,16 @@
 	DATA_TYPE* y_1_gpu;
 	DATA_TYPE* y_2_gpu;
 
-	cudaMalloc((void **)&a_gpu, sizeof(DATA_TYPE) * N * N);
-	cudaMalloc((void **)&x1_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&x2_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&y_1_gpu, sizeof(DATA_TYPE) * N);
-	cudaMalloc((void **)&y_2_gpu, sizeof(DATA_TYPE) * N);
-	cudaMemcpy(a_gpu, a, sizeof(DATA_TYPE) * N * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(x1_gpu, x1, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(x2_gpu, x2, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(y_1_gpu, y_1, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
-	cudaMemcpy(y_2_gpu, y_2, sizeof(DATA_TYPE) * N, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&a_gpu, sizeof(DATA_TYPE) * N * N);
+	hipMalloc((void **)&x1_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&x2_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&y_1_gpu, sizeof(DATA_TYPE) * N);
+	hipMalloc((void **)&y_2_gpu, sizeof(DATA_TYPE) * N);
+	hipMemcpy(a_gpu, a, sizeof(DATA_TYPE) * N * N, hipMemcpyHostToDevice);
+	hipMemcpy(x1_gpu, x1, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(x2_gpu, x2, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(y_1_gpu, y_1, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
+	hipMemcpy(y_2_gpu, y_2, sizeof(DATA_TYPE) * N, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((size_t)ceil((float)N/ ((float)DIM_THREAD_BLOCK_X)), 1);
@@ -160,21 +160,21 @@
 	
 	mvt_kernel1<<<grid,block>>>(n, a_gpu,x1_gpu,y_1_gpu);
 	mvt_kernel2<<<grid,block>>>(n, a_gpu,x2_gpu,y_2_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 	
-	cudaMemcpy(x1_outputFromGpu, x1_gpu, sizeof(DATA_TYPE) * N, cudaMemcpyDeviceToHost);
-	cudaMemcpy(x2_outputFromGpu, x2_gpu, sizeof(DATA_TYPE) * N, cudaMemcpyDeviceToHost);    
+	hipMemcpy(x1_outputFromGpu, x1_gpu, sizeof(DATA_TYPE) * N, hipMemcpyDeviceToHost);
+	hipMemcpy(x2_outputFromGpu, x2_gpu, sizeof(DATA_TYPE) * N, hipMemcpyDeviceToHost);    
 	
-	cudaFree(a_gpu);
-	cudaFree(x1_gpu);
-	cudaFree(x2_gpu);
-	cudaFree(y_1_gpu);
-	cudaFree(y_2_gpu);
+	hipFree(a_gpu);
+	hipFree(x1_gpu);
+	hipFree(x2_gpu);
+	hipFree(y_1_gpu);
+	hipFree(y_2_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/mvt/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/mvt/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/mvt/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/mvt/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./mvt.exe_mini;./mvt.exe_small;./mvt.exe_standard;./mvt.exe_large;./mvt.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syr2k/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/syr2k/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syr2k/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/syr2k/setup.ini	2024-09-26 12:55:55.233230641 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./syr2k.exe_mini;./syr2k.exe_small;./syr2k.exe_standard;./syr2k.exe_large;./syr2k.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syr2k/syr2k.cu PolyBench-backup/CUDA/linear-algebra/kernels/syr2k/syr2k.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syr2k/syr2k.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/syr2k/syr2k.cu	2024-09-26 12:55:55.233230641 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -27,7 +27,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_arrays(int ni, int nj,
@@ -117,10 +117,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -149,12 +149,12 @@
 	DATA_TYPE *B_gpu;
 	DATA_TYPE *C_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NI * NI);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NI * NI, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NI * NI);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NI * NI, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((size_t)ceil( ((float)NI) / ((float)DIM_THREAD_BLOCK_X) ), (size_t)(ceil( ((float)NI) / ((float)DIM_THREAD_BLOCK_Y) )));
@@ -163,18 +163,18 @@
   	polybench_start_instruments;
 
 	syr2k_kernel<<<grid,block>>>(ni, nj, alpha, beta, A_gpu, B_gpu, C_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 		
-	cudaMemcpy(C_outputFromGpu, C_gpu, sizeof(DATA_TYPE) * NI * NI, cudaMemcpyDeviceToHost);
+	hipMemcpy(C_outputFromGpu, C_gpu, sizeof(DATA_TYPE) * NI * NI, hipMemcpyDeviceToHost);
 
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
-	cudaFree(C_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
+	hipFree(C_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syrk/setup.ini PolyBench-backup/CUDA/linear-algebra/kernels/syrk/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syrk/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/kernels/syrk/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./syrk.exe_mini;./syrk.exe_small;./syrk.exe_standard;./syrk.exe_large;./syrk.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syrk/syrk.cu PolyBench-backup/CUDA/linear-algebra/kernels/syrk/syrk.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/kernels/syrk/syrk.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/kernels/syrk/syrk.cu	2024-09-26 12:55:55.234230690 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -27,7 +27,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_arrays(int ni, int nj,
@@ -108,10 +108,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 	
 	return;
 }
@@ -141,10 +141,10 @@
 	DATA_TYPE* A_gpu;
 	DATA_TYPE* C_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NI * NI);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
-	cudaMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NI * NI, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&C_gpu, sizeof(DATA_TYPE) * NI * NI);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
+	hipMemcpy(C_gpu, C, sizeof(DATA_TYPE) * NI * NI, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((size_t)(ceil(((float)NI) / ((float)DIM_THREAD_BLOCK_X))), (size_t)ceil(((float)NI) / ((float)DIM_THREAD_BLOCK_Y)));
@@ -153,17 +153,17 @@
   	polybench_start_instruments;
 
 	syrk_kernel<<<grid,block>>>(ni, nj, alpha, beta, A_gpu,C_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(C_outputFromGpu, C_gpu, sizeof(DATA_TYPE) * NI * NI, cudaMemcpyDeviceToHost);
+	hipMemcpy(C_outputFromGpu, C_gpu, sizeof(DATA_TYPE) * NI * NI, hipMemcpyDeviceToHost);
 
-	cudaFree(A_gpu);
-	cudaFree(C_gpu);
+	hipFree(A_gpu);
+	hipFree(C_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/gramschmidt/gramschmidt.cu PolyBench-backup/CUDA/linear-algebra/solvers/gramschmidt/gramschmidt.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/gramschmidt/gramschmidt.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/solvers/gramschmidt/gramschmidt.cu	2024-09-26 12:55:55.234230690 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -28,7 +28,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void gramschmidt(int ni, int nj, DATA_TYPE POLYBENCH_2D(A,NI,NJ,ni,nj), DATA_TYPE POLYBENCH_2D(R,NJ,NJ,nj,nj), DATA_TYPE POLYBENCH_2D(Q,NI,NJ,ni,nj))
@@ -113,10 +113,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );	
+	hipSetDevice( GPU_DEVICE );	
 	return;
 }
 
@@ -182,10 +182,10 @@
 	DATA_TYPE *R_gpu;
 	DATA_TYPE *Q_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&R_gpu, sizeof(DATA_TYPE) * NJ * NJ);
-	cudaMalloc((void **)&Q_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&R_gpu, sizeof(DATA_TYPE) * NJ * NJ);
+	hipMalloc((void **)&Q_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
 	
 	/* Start timer. */
   	polybench_start_instruments;
@@ -193,21 +193,21 @@
 	for (k = 0; k < _PB_NJ; k++)
 	{
 		gramschmidt_kernel1<<<gridKernel1,block>>>(ni, nj, A_gpu, R_gpu, Q_gpu, k);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		gramschmidt_kernel2<<<gridKernel2,block>>>(ni, nj, A_gpu, R_gpu, Q_gpu, k);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		gramschmidt_kernel3<<<gridKernel3,block>>>(ni, nj, A_gpu, R_gpu, Q_gpu, k);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	}
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 	
-	cudaMemcpy(A_outputFromGpu, A_gpu, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyDeviceToHost);    
+	hipMemcpy(A_outputFromGpu, A_gpu, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyDeviceToHost);    
 
-	cudaFree(A_gpu);
-	cudaFree(R_gpu);
-	cudaFree(Q_gpu);
+	hipFree(A_gpu);
+	hipFree(R_gpu);
+	hipFree(Q_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/gramschmidt/setup.ini PolyBench-backup/CUDA/linear-algebra/solvers/gramschmidt/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/gramschmidt/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/solvers/gramschmidt/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./gramschmidt.exe_mini;./gramschmidt.exe_small;./gramschmidt.exe_standard;./gramschmidt.exe_large;./gramschmidt.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/lu/lu.cu PolyBench-backup/CUDA/linear-algebra/solvers/lu/lu.cu
--- PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/lu/lu.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/linear-algebra/solvers/lu/lu.cu	2024-09-26 12:55:55.234230690 -0700
@@ -8,6 +8,8 @@
  * Web address: http://www.cse.ohio-state.edu/~pouchet/software/polybench/GPU
  */
 
+
+#include <hip/hip_runtime.h>
 #include <unistd.h>
 #include <stdio.h>
 #include <time.h>
@@ -27,7 +29,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void lu(int n, DATA_TYPE POLYBENCH_2D(A,N,N,n,n))
@@ -88,10 +90,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -122,8 +124,8 @@
 {
 	DATA_TYPE* AGpu;
 
-	cudaMalloc(&AGpu, N * N * sizeof(DATA_TYPE));
-	cudaMemcpy(AGpu, A, N * N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
+	hipMalloc(&AGpu, N * N * sizeof(DATA_TYPE));
+	hipMemcpy(AGpu, A, N * N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
 
 	dim3 block1(DIM_THREAD_BLOCK_KERNEL_1_X, DIM_THREAD_BLOCK_KERNEL_1_Y);
 	dim3 block2(DIM_THREAD_BLOCK_KERNEL_2_X, DIM_THREAD_BLOCK_KERNEL_2_Y);
@@ -137,12 +139,12 @@
 	{
 		grid1.x = (unsigned int)(ceil((float)(N - (k + 1)) / ((float)block1.x)));
 		lu_kernel1<<<grid1, block1>>>(n, AGpu, k);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 
 		grid2.x = (unsigned int)(ceil((float)(N - (k + 1)) / ((float)block2.x)));
 		grid2.y = (unsigned int)(ceil((float)(N - (k + 1)) / ((float)block2.y)));
 		lu_kernel2<<<grid2, block2>>>(n, AGpu, k);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	}
 	
 	/* Stop and print timer. */
@@ -150,8 +152,8 @@
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(A_outputFromGpu, AGpu, N * N * sizeof(DATA_TYPE), cudaMemcpyDeviceToHost);
-	cudaFree(AGpu);
+	hipMemcpy(A_outputFromGpu, AGpu, N * N * sizeof(DATA_TYPE), hipMemcpyDeviceToHost);
+	hipFree(AGpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/lu/setup.ini PolyBench-backup/CUDA/linear-algebra/solvers/lu/setup.ini
--- PolyBench-ACC-0.1/CUDA/linear-algebra/solvers/lu/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/linear-algebra/solvers/lu/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./lu.exe_mini;./lu.exe_small;./lu.exe_standard;./lu.exe_large;./lu.exe_extralarge
+use_clang_plugin = true
+clang_convert = make INJECT_CODE_CLANG=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/adi/adi.cu PolyBench-backup/CUDA/stencils/adi/adi.cu
--- PolyBench-ACC-0.1/CUDA/stencils/adi/adi.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/stencils/adi/adi.cu	2024-09-26 12:55:55.234230690 -0700
@@ -8,6 +8,8 @@
  * Web address: http://www.cse.ohio-state.edu/~pouchet/software/polybench/GPU
  */
 
+
+#include <hip/hip_runtime.h>
 #include <unistd.h>
 #include <stdio.h>
 #include <time.h>
@@ -27,7 +29,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void adi(int tsteps, int n, DATA_TYPE POLYBENCH_2D(A,N,N,n,n), DATA_TYPE POLYBENCH_2D(B,N,N,n,n), DATA_TYPE POLYBENCH_2D(X,N,N,n,n))
@@ -133,10 +135,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -221,12 +223,12 @@
 	DATA_TYPE* B_gpu;
 	DATA_TYPE* X_gpu;
 
-	cudaMalloc(&A_gpu, N * N * sizeof(DATA_TYPE));
-	cudaMalloc(&B_gpu, N * N * sizeof(DATA_TYPE));
-	cudaMalloc(&X_gpu, N * N * sizeof(DATA_TYPE));
-	cudaMemcpy(A_gpu, A, N * N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, N * N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
-	cudaMemcpy(X_gpu, X, N * N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
+	hipMalloc(&A_gpu, N * N * sizeof(DATA_TYPE));
+	hipMalloc(&B_gpu, N * N * sizeof(DATA_TYPE));
+	hipMalloc(&X_gpu, N * N * sizeof(DATA_TYPE));
+	hipMemcpy(A_gpu, A, N * N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, N * N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
+	hipMemcpy(X_gpu, X, N * N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
 
 	dim3 block1(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y, 1);
 	dim3 grid1(1, 1, 1);
@@ -239,25 +241,25 @@
 	{
 		
 		adi_kernel1<<<grid1, block1>>>(n, A_gpu, B_gpu, X_gpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		adi_kernel2<<<grid1, block1>>>(n, A_gpu, B_gpu, X_gpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		adi_kernel3<<<grid1, block1>>>(n, A_gpu, B_gpu, X_gpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	
 		for (int i1 = 1; i1 < _PB_N; i1++)
 		{
 			adi_kernel4<<<grid1, block1>>>(n, A_gpu, B_gpu, X_gpu, i1);
-			cudaThreadSynchronize();
+			hipDeviceSynchronize();
 		}
 
 		adi_kernel5<<<grid1, block1>>>(n, A_gpu, B_gpu, X_gpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		
 		for (int i1 = 0; i1 < _PB_N-2; i1++)
 		{
 			adi_kernel6<<<grid1, block1>>>(n, A_gpu, B_gpu, X_gpu, i1);
-			cudaThreadSynchronize();
+			hipDeviceSynchronize();
 		}
 	}
 
@@ -266,12 +268,12 @@
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(B_outputFromGpu, B_gpu, N * N * sizeof(DATA_TYPE), cudaMemcpyDeviceToHost);
-	cudaMemcpy(X_outputFromGpu, X_gpu, N * N * sizeof(DATA_TYPE), cudaMemcpyDeviceToHost);
+	hipMemcpy(B_outputFromGpu, B_gpu, N * N * sizeof(DATA_TYPE), hipMemcpyDeviceToHost);
+	hipMemcpy(X_outputFromGpu, X_gpu, N * N * sizeof(DATA_TYPE), hipMemcpyDeviceToHost);
 
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
-	cudaFree(X_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
+	hipFree(X_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/adi/setup.ini PolyBench-backup/CUDA/stencils/adi/setup.ini
--- PolyBench-ACC-0.1/CUDA/stencils/adi/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/stencils/adi/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./adi.exe_mini;./adi.exe_small;./adi.exe_standard;./adi.exe_large;./adi.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/convolution-2d/2DConvolution.cu PolyBench-backup/CUDA/stencils/convolution-2d/2DConvolution.cu
--- PolyBench-ACC-0.1/CUDA/stencils/convolution-2d/2DConvolution.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/stencils/convolution-2d/2DConvolution.cu	2024-09-26 12:55:55.234230690 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -28,7 +28,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void conv2D(int ni, int nj, DATA_TYPE POLYBENCH_2D(A, NI, NJ, ni, nj), DATA_TYPE POLYBENCH_2D(B, NI, NJ, ni, nj))
@@ -93,10 +93,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -126,9 +126,9 @@
 	DATA_TYPE *A_gpu;
 	DATA_TYPE *B_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NI * NJ);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NI * NJ);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((size_t)ceil( ((float)NI) / ((float)block.x) ), (size_t)ceil( ((float)NJ) / ((float)block.y)) );
@@ -136,7 +136,7 @@
   	polybench_start_instruments;
 
 	convolution2D_kernel <<< grid,block >>> (ni, nj, A_gpu,B_gpu);
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	
 	/* Stop and print timer. */
 	printf("GPU Time in seconds:\n");
@@ -144,10 +144,10 @@
   	polybench_stop_instruments;
   	polybench_print_instruments;
 
-	cudaMemcpy(B_outputFromGpu, B_gpu, sizeof(DATA_TYPE) * NI * NJ, cudaMemcpyDeviceToHost);
+	hipMemcpy(B_outputFromGpu, B_gpu, sizeof(DATA_TYPE) * NI * NJ, hipMemcpyDeviceToHost);
 	
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/convolution-2d/setup.ini PolyBench-backup/CUDA/stencils/convolution-2d/setup.ini
--- PolyBench-ACC-0.1/CUDA/stencils/convolution-2d/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/stencils/convolution-2d/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./2DConvolution.exe_mini;./2DConvolution.exe_small;./2DConvolution.exe_standard;./2DConvolution.exe_large;./2DConvolution.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/convolution-3d/3DConvolution.cu PolyBench-backup/CUDA/stencils/convolution-3d/3DConvolution.cu
--- PolyBench-ACC-0.1/CUDA/stencils/convolution-3d/3DConvolution.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/stencils/convolution-3d/3DConvolution.cu	2024-09-26 12:55:55.234230690 -0700
@@ -15,7 +15,7 @@
 #include <stdlib.h>
 #include <stdarg.h>
 #include <string.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -28,7 +28,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void conv3D(int ni, int nj, int nk, DATA_TYPE POLYBENCH_3D(A, NI, NJ, NK, ni, nj, nk), DATA_TYPE POLYBENCH_3D(B, NI, NJ, NK, ni, nj, nk))
@@ -104,10 +104,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -142,10 +142,10 @@
 	DATA_TYPE *A_gpu;
 	DATA_TYPE *B_gpu;
 
-	cudaMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ * NK);
-	cudaMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NI * NJ * NK);
-	cudaMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ * NK, cudaMemcpyHostToDevice);
-	cudaMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NI * NJ * NK, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&A_gpu, sizeof(DATA_TYPE) * NI * NJ * NK);
+	hipMalloc((void **)&B_gpu, sizeof(DATA_TYPE) * NI * NJ * NK);
+	hipMemcpy(A_gpu, A, sizeof(DATA_TYPE) * NI * NJ * NK, hipMemcpyHostToDevice);
+	hipMemcpy(B_gpu, B, sizeof(DATA_TYPE) * NI * NJ * NK, hipMemcpyHostToDevice);
 	
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((size_t)(ceil( ((float)NK) / ((float)block.x) )), (size_t)(ceil( ((float)NJ) / ((float)block.y) )));
@@ -159,15 +159,15 @@
 		convolution3D_kernel<<< grid, block >>>(ni, nj, nk, A_gpu, B_gpu, i);
 	}
 
-	cudaThreadSynchronize();
+	hipDeviceSynchronize();
 	printf("GPU Time in seconds:\n");
   	polybench_stop_instruments;
  	polybench_print_instruments;
 	
-	cudaMemcpy(B_outputFromGpu, B_gpu, sizeof(DATA_TYPE) * NI * NJ * NK, cudaMemcpyDeviceToHost);
+	hipMemcpy(B_outputFromGpu, B_gpu, sizeof(DATA_TYPE) * NI * NJ * NK, hipMemcpyDeviceToHost);
 	
-	cudaFree(A_gpu);
-	cudaFree(B_gpu);
+	hipFree(A_gpu);
+	hipFree(B_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/convolution-3d/setup.ini PolyBench-backup/CUDA/stencils/convolution-3d/setup.ini
--- PolyBench-ACC-0.1/CUDA/stencils/convolution-3d/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/stencils/convolution-3d/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./3DConvolution.exe_mini;./3DConvolution.exe_small;./3DConvolution.exe_standard;./3DConvolution.exe_large;./3DConvolution.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/fdtd-2d/fdtd2d.cu PolyBench-backup/CUDA/stencils/fdtd-2d/fdtd2d.cu
--- PolyBench-ACC-0.1/CUDA/stencils/fdtd-2d/fdtd2d.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/stencils/fdtd-2d/fdtd2d.cu	2024-09-26 12:55:55.234230690 -0700
@@ -14,7 +14,7 @@
 #include <assert.h>
 #include <unistd.h>
 #include <sys/time.h>
-#include <cuda.h>
+#include <hip/hip_runtime.h>
 
 #define POLYBENCH_TIME 1
 
@@ -27,7 +27,7 @@
 
 #define GPU_DEVICE 0
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_arrays(int tmax, int nx, int ny, DATA_TYPE POLYBENCH_1D(_fict_, TMAX, TMAX), DATA_TYPE POLYBENCH_2D(ex,NX,NY,nx,ny), 
@@ -114,10 +114,10 @@
 
 void GPU_argv_init()
 {
-	cudaDeviceProp deviceProp;
-	cudaGetDeviceProperties(&deviceProp, GPU_DEVICE);
+	hipDeviceProp_t deviceProp;
+	hipGetDeviceProperties(&deviceProp, GPU_DEVICE);
 	printf("setting device %d with name %s\n",GPU_DEVICE,deviceProp.name);
-	cudaSetDevice( GPU_DEVICE );
+	hipSetDevice( GPU_DEVICE );
 }
 
 
@@ -174,15 +174,15 @@
 	DATA_TYPE *ey_gpu;
 	DATA_TYPE *hz_gpu;
 
-	cudaMalloc((void **)&_fict_gpu, sizeof(DATA_TYPE) * TMAX);
-	cudaMalloc((void **)&ex_gpu, sizeof(DATA_TYPE) * NX * NY);
-	cudaMalloc((void **)&ey_gpu, sizeof(DATA_TYPE) * NX * NY);
-	cudaMalloc((void **)&hz_gpu, sizeof(DATA_TYPE) * NX * NY);
-
-	cudaMemcpy(_fict_gpu, _fict_, sizeof(DATA_TYPE) * TMAX, cudaMemcpyHostToDevice);
-	cudaMemcpy(ex_gpu, ex, sizeof(DATA_TYPE) * NX * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(ey_gpu, ey, sizeof(DATA_TYPE) * NX * NY, cudaMemcpyHostToDevice);
-	cudaMemcpy(hz_gpu, hz, sizeof(DATA_TYPE) * NX * NY, cudaMemcpyHostToDevice);
+	hipMalloc((void **)&_fict_gpu, sizeof(DATA_TYPE) * TMAX);
+	hipMalloc((void **)&ex_gpu, sizeof(DATA_TYPE) * NX * NY);
+	hipMalloc((void **)&ey_gpu, sizeof(DATA_TYPE) * NX * NY);
+	hipMalloc((void **)&hz_gpu, sizeof(DATA_TYPE) * NX * NY);
+
+	hipMemcpy(_fict_gpu, _fict_, sizeof(DATA_TYPE) * TMAX, hipMemcpyHostToDevice);
+	hipMemcpy(ex_gpu, ex, sizeof(DATA_TYPE) * NX * NY, hipMemcpyHostToDevice);
+	hipMemcpy(ey_gpu, ey, sizeof(DATA_TYPE) * NX * NY, hipMemcpyHostToDevice);
+	hipMemcpy(hz_gpu, hz, sizeof(DATA_TYPE) * NX * NY, hipMemcpyHostToDevice);
 
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid( (size_t)ceil(((float)NY) / ((float)block.x)), (size_t)ceil(((float)NX) / ((float)block.y)));
@@ -193,11 +193,11 @@
 	for(int t = 0; t < _PB_TMAX; t++)
 	{
 		fdtd_step1_kernel<<<grid,block>>>(nx, ny, _fict_gpu, ex_gpu, ey_gpu, hz_gpu, t);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		fdtd_step2_kernel<<<grid,block>>>(nx, ny, ex_gpu, ey_gpu, hz_gpu, t);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		fdtd_step3_kernel<<<grid,block>>>(nx, ny, ex_gpu, ey_gpu, hz_gpu, t);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	}
 	
 	/* Stop and print timer. */
@@ -205,12 +205,12 @@
   	polybench_stop_instruments;
  	polybench_print_instruments;
 
-	cudaMemcpy(hz_outputFromGpu, hz_gpu, sizeof(DATA_TYPE) * NX * NY, cudaMemcpyDeviceToHost);	
+	hipMemcpy(hz_outputFromGpu, hz_gpu, sizeof(DATA_TYPE) * NX * NY, hipMemcpyDeviceToHost);	
 		
-	cudaFree(_fict_gpu);
-	cudaFree(ex_gpu);
-	cudaFree(ey_gpu);
-	cudaFree(hz_gpu);
+	hipFree(_fict_gpu);
+	hipFree(ex_gpu);
+	hipFree(ey_gpu);
+	hipFree(hz_gpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/fdtd-2d/setup.ini PolyBench-backup/CUDA/stencils/fdtd-2d/setup.ini
--- PolyBench-ACC-0.1/CUDA/stencils/fdtd-2d/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/stencils/fdtd-2d/setup.ini	2024-09-26 12:55:55.234230690 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./fdtd2d.exe_mini;./fdtd2d.exe_small;./fdtd2d.exe_standard;./fdtd2d.exe_large;./fdtd2d.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/jacobi-1d-imper/jacobi1D.cu PolyBench-backup/CUDA/stencils/jacobi-1d-imper/jacobi1D.cu
--- PolyBench-ACC-0.1/CUDA/stencils/jacobi-1d-imper/jacobi1D.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/stencils/jacobi-1d-imper/jacobi1D.cu	2024-09-26 12:55:55.235230740 -0700
@@ -8,6 +8,8 @@
  * Web address: http://www.cse.ohio-state.edu/~pouchet/software/polybench/GPU
  */
 
+
+#include <hip/hip_runtime.h>
 #include <stdio.h>
 #include <unistd.h>
 #include <time.h>
@@ -26,7 +28,7 @@
 //define the error threshold for the results "not matching"
 #define PERCENT_DIFF_ERROR_THRESHOLD 0.05
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int n, DATA_TYPE POLYBENCH_1D(A,N,n), DATA_TYPE POLYBENCH_1D(B,N,n))
@@ -113,11 +115,11 @@
 	DATA_TYPE* Agpu;
 	DATA_TYPE* Bgpu;
 
-	cudaMalloc(&Agpu, N * sizeof(DATA_TYPE));
-	cudaMalloc(&Bgpu, N * sizeof(DATA_TYPE));
+	hipMalloc(&Agpu, N * sizeof(DATA_TYPE));
+	hipMalloc(&Bgpu, N * sizeof(DATA_TYPE));
 
-	cudaMemcpy(Agpu, A, N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
-	cudaMemcpy(Bgpu, B, N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
+	hipMemcpy(Agpu, A, N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
+	hipMemcpy(Bgpu, B, N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
 	
 
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
@@ -129,9 +131,9 @@
 	for (int t = 0; t < _PB_TSTEPS ; t++)
 	{
 		runJacobiCUDA_kernel1 <<< grid, block >>> (n, Agpu, Bgpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		runJacobiCUDA_kernel2 <<< grid, block>>> (n, Agpu, Bgpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	}
 
 	/* Stop and print timer. */
@@ -139,11 +141,11 @@
   	polybench_stop_instruments;
  	polybench_print_instruments;
 	
-	cudaMemcpy(A_outputFromGpu, Agpu, sizeof(DATA_TYPE) * N, cudaMemcpyDeviceToHost);
-	cudaMemcpy(B_outputFromGpu, Bgpu, sizeof(DATA_TYPE) * N, cudaMemcpyDeviceToHost);
+	hipMemcpy(A_outputFromGpu, Agpu, sizeof(DATA_TYPE) * N, hipMemcpyDeviceToHost);
+	hipMemcpy(B_outputFromGpu, Bgpu, sizeof(DATA_TYPE) * N, hipMemcpyDeviceToHost);
 
-	cudaFree(Agpu);
-	cudaFree(Bgpu);
+	hipFree(Agpu);
+	hipFree(Bgpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/jacobi-1d-imper/setup.ini PolyBench-backup/CUDA/stencils/jacobi-1d-imper/setup.ini
--- PolyBench-ACC-0.1/CUDA/stencils/jacobi-1d-imper/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/stencils/jacobi-1d-imper/setup.ini	2024-09-26 12:55:55.235230740 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./jacobi1D.exe_mini;./jacobi1D.exe_small;./jacobi1D.exe_standard;./jacobi1D.exe_large;./jacobi1D.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/jacobi-2d-imper/jacobi2D.cu PolyBench-backup/CUDA/stencils/jacobi-2d-imper/jacobi2D.cu
--- PolyBench-ACC-0.1/CUDA/stencils/jacobi-2d-imper/jacobi2D.cu	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/stencils/jacobi-2d-imper/jacobi2D.cu	2024-09-26 12:55:55.235230740 -0700
@@ -8,6 +8,8 @@
  * Web address: http://www.cse.ohio-state.edu/~pouchet/software/polybench/GPU
  */
 
+
+#include <hip/hip_runtime.h>
 #include <stdio.h>
 #include <unistd.h>
 #include <time.h>
@@ -26,7 +28,7 @@
 //define the error threshold for the results "not matching"
 #define PERCENT_DIFF_ERROR_THRESHOLD 0.05
 
-#define RUN_ON_CPU
+//#define RUN_ON_CPU
 
 
 void init_array(int n, DATA_TYPE POLYBENCH_2D(A,N,N,n,n), DATA_TYPE POLYBENCH_2D(B,N,N,n,n))
@@ -129,10 +131,10 @@
 	DATA_TYPE* Agpu;
 	DATA_TYPE* Bgpu;
 
-	cudaMalloc(&Agpu, N * N * sizeof(DATA_TYPE));
-	cudaMalloc(&Bgpu, N * N * sizeof(DATA_TYPE));
-	cudaMemcpy(Agpu, A, N * N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
-	cudaMemcpy(Bgpu, B, N * N * sizeof(DATA_TYPE), cudaMemcpyHostToDevice);
+	hipMalloc(&Agpu, N * N * sizeof(DATA_TYPE));
+	hipMalloc(&Bgpu, N * N * sizeof(DATA_TYPE));
+	hipMemcpy(Agpu, A, N * N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
+	hipMemcpy(Bgpu, B, N * N * sizeof(DATA_TYPE), hipMemcpyHostToDevice);
 
 	dim3 block(DIM_THREAD_BLOCK_X, DIM_THREAD_BLOCK_Y);
 	dim3 grid((unsigned int)ceil( ((float)N) / ((float)block.x) ), (unsigned int)ceil( ((float)N) / ((float)block.y) ));
@@ -143,9 +145,9 @@
 	for (int t = 0; t < _PB_TSTEPS; t++)
 	{
 		runJacobiCUDA_kernel1<<<grid,block>>>(n, Agpu, Bgpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 		runJacobiCUDA_kernel2<<<grid,block>>>(n, Agpu, Bgpu);
-		cudaThreadSynchronize();
+		hipDeviceSynchronize();
 	}
 
 	/* Stop and print timer. */
@@ -153,11 +155,11 @@
   	polybench_stop_instruments;
   	polybench_print_instruments;
 	
-	cudaMemcpy(A_outputFromGpu, Agpu, sizeof(DATA_TYPE) * N * N, cudaMemcpyDeviceToHost);
-	cudaMemcpy(B_outputFromGpu, Bgpu, sizeof(DATA_TYPE) * N * N, cudaMemcpyDeviceToHost);
+	hipMemcpy(A_outputFromGpu, Agpu, sizeof(DATA_TYPE) * N * N, hipMemcpyDeviceToHost);
+	hipMemcpy(B_outputFromGpu, Bgpu, sizeof(DATA_TYPE) * N * N, hipMemcpyDeviceToHost);
 
-	cudaFree(Agpu);
-	cudaFree(Bgpu);
+	hipFree(Agpu);
+	hipFree(Bgpu);
 }
 
 
diff -ruN PolyBench-ACC-0.1/CUDA/stencils/jacobi-2d-imper/setup.ini PolyBench-backup/CUDA/stencils/jacobi-2d-imper/setup.ini
--- PolyBench-ACC-0.1/CUDA/stencils/jacobi-2d-imper/setup.ini	1969-12-31 16:00:00.000000000 -0800
+++ PolyBench-backup/CUDA/stencils/jacobi-2d-imper/setup.ini	2024-09-26 12:55:55.235230740 -0700
@@ -0,0 +1,6 @@
+[DEFAULT]
+compile = make
+run = ./jacobi2D.exe_mini;./jacobi2D.exe_small;./jacobi2D.exe_standard;./jacobi2D.exe_large;./jacobi2D.exe_extralarge
+use_clang_plugin = false
+llvm_pass = make INJECT_CODE_LLVM=1
+clean = make clean
\ No newline at end of file
diff -ruN PolyBench-ACC-0.1/CUDA/utilities/common.mk PolyBench-backup/CUDA/utilities/common.mk
--- PolyBench-ACC-0.1/CUDA/utilities/common.mk	2014-09-25 18:30:56.000000000 -0700
+++ PolyBench-backup/CUDA/utilities/common.mk	2024-09-26 12:55:55.235230740 -0700
@@ -1,4 +1,25 @@
+
+
+ifndef $(INJECT_CODE_LLVM)
+INJECT_CODE_LLVM = 0
+endif
+
+ifeq ($(INJECT_CODE_LLVM), 1)
+INST_FLAGS = -g -include ${HOME}/FloatGuard/inst_pass/Inst/InstStub.h -fpass-plugin=${HOME}/FloatGuard/inst_pass/libInstPass.so \
+			 -DEXP_FLAG_TOTAL=0x0005F2F0
+else
+INST_FLAGS = -g -include ${HOME}/FloatGuard/inst_pass/Inst/InstStub.h \
+			 -DEXP_FLAG_TOTAL=0x0005F2F0
+endif
+
+FP_FLAGS = -DDATA_TYPE=double -DDATA_PRINTF_MODIFIER=\"%0.2lf\"
+
 all:
-	nvcc -O3 ${CUFILES} -I${PATH_TO_UTILS} -o ${EXECUTABLE} 
+	${HOME}/FloatGuard/gdb_script/hipcc_wrapper.sh ${HOME}/FloatGuard/inst_pass/Inst/InstStub.o -O3 -ffast-math ${CUFILES} -I${PATH_TO_UTILS} -o ${EXECUTABLE}_mini ${INST_FLAGS} ${FP_FLAGS} -DMINI_DATASET
+	${HOME}/FloatGuard/gdb_script/hipcc_wrapper.sh ${HOME}/FloatGuard/inst_pass/Inst/InstStub.o -O3 -ffast-math ${CUFILES} -I${PATH_TO_UTILS} -o ${EXECUTABLE}_small ${INST_FLAGS} ${FP_FLAGS} -DSMALL_DATASET
+	${HOME}/FloatGuard/gdb_script/hipcc_wrapper.sh ${HOME}/FloatGuard/inst_pass/Inst/InstStub.o -O3 -ffast-math ${CUFILES} -I${PATH_TO_UTILS} -o ${EXECUTABLE}_standard ${INST_FLAGS} ${FP_FLAGS} -DSTANDARD_DATASET
+	${HOME}/FloatGuard/gdb_script/hipcc_wrapper.sh ${HOME}/FloatGuard/inst_pass/Inst/InstStub.o -O3 -ffast-math ${CUFILES} -I${PATH_TO_UTILS} -o ${EXECUTABLE}_large ${INST_FLAGS} ${FP_FLAGS} -DLARGE_DATASET
+	${HOME}/FloatGuard/gdb_script/hipcc_wrapper.sh ${HOME}/FloatGuard/inst_pass/Inst/InstStub.o -O3 -ffast-math ${CUFILES} -I${PATH_TO_UTILS} -o ${EXECUTABLE}_extralarge ${INST_FLAGS} ${FP_FLAGS} -DEXTRALARGE_DATASET
+	
 clean:
-	rm -f *~ *.exe
\ No newline at end of file
+	rm -f *~ *.exe* seq.txt
\ No newline at end of file
